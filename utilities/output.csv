year,week,Category,Function/Method/Concept,code_snippet,May Overcount
2020.0,9.0,Charts,```Bokeh```,"from 1st Place to 2nd Place
- Rename Sample Types: RV = Registered Voter, LV = Likely Voter, null = Unknown
- Output the Data
- Optional: Build the Viz

Author: Kelly Gilbert
Created: 2022-02-02
Requirements:
  - input dataset:
      - PD 2020 Wk 9 Input - Sheet1.csv
  - output dataset (for results check only):
      - PD 2020 Wk 9 Output.csv
""""""


from numpy import nan, where
import pandas as pd


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# read in the file, melt the candidates into rows, remove nulls, remove averages
df = pd.read_csv(r'.\inputs\PD 2020 Wk 9 Input - Sheet1.csv', na_values='--')\
       .melt(id_vars=['Poll', 'Date', 'Sample'], var_name='Candidate', value_name='Poll Results')\
       .dropna(subset=['Poll Results'])\
       .query(""~Poll.str.contains('Average')"", engine='python')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean up end date
df['End Date'] = pd.to_datetime(df['Date'].str.extract('.*- (\d+/\d+)', expand=False) + '/2020')
df['End Date'] = where(df['End Date'].dt.month >= 7, 
                       df['End Date'] + pd.DateOffset(years=-1), 
                       df['End Date'])


# form a Rank (modified competition) of the candidates per Poll based on their results
df['Rank'] = df.groupby(['Poll', 'End Date', 'Sample'])['Poll Results'].rank(method='max', ascending=False)\
               .astype(int)
      
        
# difference in poll results between first and second
df['Spread from 1st to 2nd Place'] = \
    df.groupby(['Poll', 'End Date', 'Sample'], as_index=False)['Poll Results']\
      .transform(lambda x: x.max() - x.nlargest(2).min())
               

# rename sample types
sample_map = {'.*RV' : 'Registered Voter', '.*LV' : 'Likely Voter', nan : 'Unknown'}
df['Sample Type'] = df['Sample'].replace(sample_map, regex=True)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

out_cols = ['Candidate', 'Poll Results', 'Spread from 1st to 2nd Place', 'Rank', 'End Date', 
            'Sample Type', 'Poll']
df.to_csv(r'.\outputs\output-2020-09.csv', index=False, columns=out_cols, date_format='%d/%m/%Y')


#---------------------------------------------------------------------------------------------------
# create chart
#---------------------------------------------------------------------------------------------------

from bokeh.io import output_file
from bokeh.layouts import row
from bokeh.models import CustomJS, Legend, DatetimeTickFormatter, Select, Title
from bokeh.plotting import figure, show

# color constants
color_selected = '#0066cc'
color_deselected = '#bab0ac'

# set the output file path
output_file('dimensions.html')

# subset of registered voters
df_rv = df.loc[df['Sample Type']=='Registered Voter']\
          .sort_values(by=['Candidate', 'End Date', 'Poll'])

# add a figure and format it
p = figure(width=900, height=500, x_axis_type='datetime')
p.add_layout(Title(text='Data from: realclearpolitics.com; Sample Type: Registered Voter', 
                   text_font_size='9pt'), 'above')
p.add_layout(Title(text='2020 Democratic Presidential Nominations', text_font_size=""24pt""), 'above')
p.xaxis.formatter=DatetimeTickFormatter(days=[""%Y-%m-%d""])
p.y_range.flipped = True
p.add_layout(Legend(), 'right')


# add a line and circles for each candidate option
candidates = sorted(df_rv['Candidate'].unique())

line_dict = {}
circle_dict = {}
for i, c in enumerate(candidates):
    line_dict[c] = p.line(df_rv.loc[df['Candidate']==c]['End Date'], 
                          df_rv.loc[df['Candidate']==c]['Rank'], 
                          legend_label=c, line_width=2,
                          color=color_selected if i==0 else color_deselected)
    
    circle_dict[c] = p.circle(df_rv.loc[df['Candidate']==c]['End Date'], 
                              df_rv.loc[df['Candidate']==c]['Rank'], 
                              legend_label=c, size=7,
                              line_width=0,
                              fill_color=color_selected if i==0 else color_deselected)
    
# create a drop-down menu
menu = Select(options=candidates, value=candidates[0], title='Select an item:')


# link the plot and the button using a callback function

# cb_obj = the model that triggered the callback (e.g. button model, dropdown model)
# args = list of name=object values you want to have accessible inside the callback function
# can't assign cb_obj.value to a model property directly; you have to store it in a variable first

callback = CustomJS(args=dict(p=p, lines=line_dict, circles=circle_dict), code=""""""
const t = cb_obj.value;

// make the selected item's marks blue and the rest gray
for (let i in lines) {
  if (i == t) {
      lines[i].glyph.line_color = '"""""" + color_selected + """"""';
      circles[i].glyph.fill_color = '"""""" + color_selected + """"""';
  } else { 
      lines[i].glyph.line_color = '"""""" + color_deselected + """"""';
      circles[i].glyph.fill_color = '"""""" + color_deselected + """"""';
  }
}
"""""")

menu.js_on_change('value', callback)    


# display the layout
chart_layout = row(p, menu)
show(chart_layout)",
2021.0,1.0,Charts,```Seaborn```,"from os import chdir
from pandas import DataFrame, read_csv
import seaborn as sns


# connect and load the csv file
chdir('C:\\projects\\preppin-data-challenge\\preppin-data-2021-01')

df = read_csv('.\\inputs\\PD 2021 Wk 1 Input - Bike Sales.csv', 
              parse_dates=['Date'], dayfirst=True)

# split the 'Store-Bike' field into 'Store' and 'Bike'
df[['Store','Bike']] = df['Store - Bike'].str.split(pat=' - ', expand=True)

# clean up the 'Bike' field to leave just three values in the 'Bike' field 
# (Mountain, Gravel, Road)
remap = { 'Graval' : 'Gravel',
          'Gravle' : 'Gravel',
          'Mountaen' : 'Mountain',
          'Rood' : 'Road',
          'Rowd' : 'Road' }
df['Bike'].replace(remap, inplace=True)

# create two different cuts of the date field: 'quarter' and 'day of month'
df['Quarter'] = df['Date'].dt.quarter
df['Day of Month'] = df['Date'].dt.day

# remove the first 10 orders as they are test values
df = df.iloc[10:]

# output the data as a csv
col_order = ['Quarter', 'Store', 'Bike', 'Order ID', 'Customer Age',
             'Bike Value', 'Existing Customer?', 'Day of Month']
df[col_order].to_csv('.\\outputs\\output-2021-01.csv', index=False)


#--------------------------------------------------------------------------------
# bonus
#--------------------------------------------------------------------------------

df = read_csv('.\\outputs\\output-2021-01.csv')

# create a list of all possible combinations
df_qtr = DataFrame({ 'Quarter' : df['Quarter'].unique() }).sort_values(by='Quarter')
df_qtr['join'] = 1
df_day = DataFrame({ 'Day of Month' : range(1,32) }).sort_values(by='Day of Month')
df_day['join'] = 1
df_bike = DataFrame({ 'Bike' : df['Bike'].unique() }).sort_values(by='Bike')
df_bike['join'] = 1

df_all = df_qtr.merge(df_day, how='outer', on='join')
df_all = df_all.merge(df_bike, how='outer', on='join')


# average bike value per order by type, quarter, and day of month
# (not sure how this is useful, but this is the calc used in the solution!)
df_sum = df.groupby(['Bike', 'Quarter', 'Day of Month'])['Bike Value'].mean()
df_sum = df_sum.reset_index()


# add the sum
df_all = df_all.merge(df_sum, how='left', on=['Quarter', 'Day of Month', 'Bike']).fillna(0)
df_all['Cuml Sales'] = df_all.groupby(['Bike','Quarter'])['Bike Value'].cumsum()


# generate charts by quarter
sns.set_style(""white"")
palette = ['#ccb22b','#9f8f12','#959c9e']

g = sns.FacetGrid(df_all, row=""Quarter"",
                  aspect=5, height=1.5, sharex=True, sharey=False,
                  legend_out=True, margin_titles=False) 

g.fig.suptitle(""Typical Running Monthly Sales in Each Quarter"", 
               x=0.3, y=1.05, fontsize='xx-large')

g.map(sns.lineplot, ""Day of Month"", ""Cuml Sales"", 'Bike', 
      palette=palette, ci=None, linewidth=2.5).add_legend(loc='upper right')",
2021.0,2.0,Charts,```Seaborn```,"from pandas import read_csv


# input the data
df = read_csv('.\\inputs\\PD 2021 Wk 2 Input - Bike Model Sales.csv', 
              parse_dates=['Order Date', 'Shipping Date'], dayfirst=True)


# clean up the Model field to leave only the letters to represent the 
# Brand of the bike
df['Brand'] = df['Model'].str.replace('[^A-Z]+', '')


# work out the Order Value using Value per Bike and Quantity
df['Order Value'] = df['Value per Bike'] * df['Quantity']


# aggregate Value per Bike, Order Value and Quantity by Brand and Bike Type 
# note, the avg value sold is the straight average and not weighted
df_brand = df.groupby(['Brand', 'Bike Type']).agg({ 'Quantity' : ['sum'],
                                                    'Order Value' : ['sum'],
                                                    'Value per Bike' : ['mean'] })
df_brand.columns = ['Quantity Sold', 'Order Value', 'Avg Bike Value Sold per Brand, Type']
df_brand.reset_index(inplace = True)

df_brand['Avg Bike Value Sold per Brand, Type'] = \
    df_brand['Avg Bike Value Sold per Brand, Type'].round(1)


# calculate Days to ship by measuring the difference between when an order was 
#  placed and when it was shipped as 'Days to Ship'
df['Days to Ship'] = (df['Shipping Date'] - df['Order Date']).dt.days
df[['Order Date', 'Shipping Date', 'Days to Ship']].head()

   
# aggregate Order Value, Quantity and Days to Ship by Brand and Store
df_store = df.groupby(['Brand', 'Store']).agg({ 'Quantity' : ['sum'],
                                                'Order Value' : ['sum'],
                                                'Days to Ship' : ['mean']})
df_store.columns = ['Total Quantity Sold', 'Total Order Value', 'Avg Days to Ship']
df_store.reset_index(inplace = True)

df_store['Avg Days to Ship'] = df_store['Avg Days to Ship'].round(1)


# output both data sets
df_brand.to_csv('.\\outputs\\output-2021-02-brand-type.csv', index = False)
df_store.to_csv('.\\outputs\\output-2021-02-brand-store.csv', index = False)


#--------------------------------------------------------------------------------
# not part of the challenge, just practice making charts
#--------------------------------------------------------------------------------

import seaborn as sns


sns.set_style('white')
sns.set_palette('Set2')

# quantity sold by brand
g = sns.catplot(data=df_brand, x='Brand', y='Quantity Sold', estimator=sum, kind='bar', ci=None,
                aspect=2, height=3)
g.fig.suptitle(""Quantity Sold by Brand"", y=1.05, fontsize='x-large')


# quantity sold by type
g = sns.catplot(data=df_brand, x='Bike Type', y='Quantity Sold', estimator=sum, kind='bar', ci=None,
                aspect=2, height=3)
g.fig.suptitle(""Quantity Sold by Bike Type"", y=1.05, fontsize='x-large')


# quantity sold by brand and type
# using the palette from week 1
palette = ['#ccb22b','#9f8f12','#959c9e']
g = sns.catplot(data=df_brand, x='Brand', y='Quantity Sold', estimator=sum, kind='bar', ci=None,
                hue='Bike Type', palette=palette, aspect=2, height=3)
g.fig.suptitle(""Quantity Sold by Brand and Bike Type"", y=1.05, fontsize='x-large')


# quantity sold by month and type
df['Order Month'] = df['Order Date'].dt.to_period('M').dt.to_timestamp()
palette = ['#ccb22b','#9f8f12','#959c9e']
g = sns.relplot(data=df, x='Order Month', y='Quantity', estimator=sum, kind='line', ci=None,
                hue='Bike Type', palette=palette, aspect=2, height=3)
g.fig.suptitle(""Quantity Sold by Month and Bike Type"", y=1.05, fontsize='x-large')",
2021.0,12.0,Charts,```Seaborn```,"from our dataset so that only the lowest level
  of granularity remains. Currently we have Total > Continents > Countries, but we don't have data
  for all countries in a continent, so it's not as simple as just filtering out the totals and
  subtotals. Plus in our Continents level of detail, we also have The Middle East and UN passport
  holders as categories. If you feel confident in your prep skills, this (plus the output) should be
  enough information to go on, but otherwise read on for a breakdown of the steps we need to take:
    - Filter out Total tourist arrivals
    - Split our workflow into 2 streams: Continents and Countries
      Hint: the hierarchy field will be useful here
    - Split out the Continent and Country names from the relevant fields
    - Aggregate our Country stream to the Continent level
    - Join the two streams together and work out how many tourists arrivals there are that we don't
      know the country of
    - Add in a Country field with the value ""Unknown""
    - Union this back to here we had our Country breakdown
- Output the data

Author: Kelly Gilbert
Created: 2021-03-24
Requirements:
  - seaborn version 0.11.0 (for axes_dict and crest palette)
  - input dataset:
      - Tourism Input.csv
  - output dataset (for results check):
      - Preppin Data 2020W9 Output.csv

""""""


from numpy import nan, where
from pandas import concat, melt, merge, read_csv, to_datetime

# for the charts
import seaborn as sns
from matplotlib import pyplot as plt


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# read in the data, then stack it
key_cols = ['Series-Measure', 'Hierarchy-Breakdown', 'Unit-Detail']

df = read_csv(r'.\inputs\Tourism Input.csv', na_values=['na'])\
         .drop(columns=['id'])\
         .melt(id_vars=key_cols)\
         .rename(columns={'variable':'Month'})

df['Month'] = to_datetime(df['Month'], format='%b-%y')


#---------------------------------------------------------------------------------------------------
# prep the data
#---------------------------------------------------------------------------------------------------

# filter for tourist arrival counts and remove null values
df = df.loc[(df['Series-Measure'].str.contains('Tourist arrivals')) & (df['value'].notna())]
df['value'] = df['value'].astype(int)

# extract the country and continent
df['Country'] = where(df['Hierarchy-Breakdown'].str.match('.*Tourist arrivals / .*'),
                      df['Series-Measure'].str.replace('Tourist arrivals from (the )?', ''), nan)

df['Breakdown'] = where(df['Hierarchy-Breakdown'].str.match('.*Tourist arrivals / .*'),
                        df['Hierarchy-Breakdown'].str.replace('.*Tourist arrivals / ', ''),
                        df['Series-Measure'].str.\
                            replace('Tourist arrivals from |Tourist arrivals - ', ''))

# sum country values by continent
cont_dtl = df[df['Country'].notna()].groupby(['Breakdown', 'Month'])['value'].sum().reset_index()

# join the continent totals to the sums to find the difference
cont_tot = df.loc[df['Country'].isna()]
cont_tot = cont_tot.merge(cont_dtl, on=['Breakdown', 'Month'], suffixes=['', '_CountryTotal'],
                          how='left')
cont_tot['value'] = cont_tot['value'] - cont_tot['value_CountryTotal'].fillna(0)

# remove the continent totals and union the new Unknown-country rows to the main dataframe
cont_tot['Country'] = 'Unknown'
cont_tot.drop(columns=['value_CountryTotal'], inplace=True)
df = concat([df[df['Country'].notna()], cont_tot])


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df.rename(columns={'value':'Number of Tourists'}, inplace=True)
df.to_csv('.\\outputs\\output-2021-12.csv', index=False, date_format='%d/%m/%Y',
          columns=['Breakdown', 'Month', 'Number of Tourists', 'Country'])


#---------------------------------------------------------------------------------------------------
# generate the charts
#---------------------------------------------------------------------------------------------------

charts_per_row = 3
sns.set_style(""white"")
sort=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

# summarize the data by region and year/month
df_region = df.groupby(['Breakdown', 'Month'])['Number of Tourists'].sum().reset_index()
df_region['Year'] = df_region['Month'].dt.year.astype(int)
df_region['Month Name'] = df_region['Month'].dt.strftime('%b')

# draw a grid of charts, one for each region, where x=month, y=# arrivals, and line per year
g = sns.relplot(kind='line', data=df_region, x='Month Name', y='Number of Tourists', units='Year', 
                hue='Year', col='Breakdown', palette='GnBu', linewidth=1, estimator=None, 
                col_wrap=charts_per_row, height=2.5, aspect=1.5, sort=sort, legend=True,
                facet_kws={'sharex':True, 'sharey':False, 'legend_out':True}).add_legend()

# place the legend
g._legend.set_bbox_to_anchor([1.1, 0])

# add spacing to the grid
g.fig.tight_layout(h_pad=4, w_pad=4)  

# add an orange line for the current year
for Breakdown, ax in g.axes_dict.items():
    # overlay the last year in orange
    sns.lineplot(data=df_region[(df_region['Year']==df_region['Year'].max()) 
                                & (df_region['Breakdown']==Breakdown)],
                 x='Month Name', y='Number of Tourists',  color='orange',
                 linewidth=3, ci=None, sort=sort, ax=ax, legend=None)
    ax.set_title(f""{Breakdown}"")
    ax.set(xlabel=None, ylabel=None)
    
# add the main title
g.fig.suptitle('Maldives Tourist Arrivals by Region and Year', x=0.58, y=1.1, fontsize='xx-large')
g.fig.text(s='Most recent year (' + str(df_region['Year'].max()) + ') is highlighted', 
          x=0.46, y=1.04, size='12')
plt.show()",
2021.0,12.0,Charts,```matplotlib```,"from our dataset so that only the lowest level
  of granularity remains. Currently we have Total > Continents > Countries, but we don't have data
  for all countries in a continent, so it's not as simple as just filtering out the totals and
  subtotals. Plus in our Continents level of detail, we also have The Middle East and UN passport
  holders as categories. If you feel confident in your prep skills, this (plus the output) should be
  enough information to go on, but otherwise read on for a breakdown of the steps we need to take:
    - Filter out Total tourist arrivals
    - Split our workflow into 2 streams: Continents and Countries
      Hint: the hierarchy field will be useful here
    - Split out the Continent and Country names from the relevant fields
    - Aggregate our Country stream to the Continent level
    - Join the two streams together and work out how many tourists arrivals there are that we don't
      know the country of
    - Add in a Country field with the value ""Unknown""
    - Union this back to here we had our Country breakdown
- Output the data

Author: Kelly Gilbert
Created: 2021-03-24
Requirements:
  - seaborn version 0.11.0 (for axes_dict and crest palette)
  - input dataset:
      - Tourism Input.csv
  - output dataset (for results check):
      - Preppin Data 2020W9 Output.csv

""""""


from numpy import nan, where
from pandas import concat, melt, merge, read_csv, to_datetime

# for the charts
import seaborn as sns
from matplotlib import pyplot as plt


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# read in the data, then stack it
key_cols = ['Series-Measure', 'Hierarchy-Breakdown', 'Unit-Detail']

df = read_csv(r'.\inputs\Tourism Input.csv', na_values=['na'])\
         .drop(columns=['id'])\
         .melt(id_vars=key_cols)\
         .rename(columns={'variable':'Month'})

df['Month'] = to_datetime(df['Month'], format='%b-%y')


#---------------------------------------------------------------------------------------------------
# prep the data
#---------------------------------------------------------------------------------------------------

# filter for tourist arrival counts and remove null values
df = df.loc[(df['Series-Measure'].str.contains('Tourist arrivals')) & (df['value'].notna())]
df['value'] = df['value'].astype(int)

# extract the country and continent
df['Country'] = where(df['Hierarchy-Breakdown'].str.match('.*Tourist arrivals / .*'),
                      df['Series-Measure'].str.replace('Tourist arrivals from (the )?', ''), nan)

df['Breakdown'] = where(df['Hierarchy-Breakdown'].str.match('.*Tourist arrivals / .*'),
                        df['Hierarchy-Breakdown'].str.replace('.*Tourist arrivals / ', ''),
                        df['Series-Measure'].str.\
                            replace('Tourist arrivals from |Tourist arrivals - ', ''))

# sum country values by continent
cont_dtl = df[df['Country'].notna()].groupby(['Breakdown', 'Month'])['value'].sum().reset_index()

# join the continent totals to the sums to find the difference
cont_tot = df.loc[df['Country'].isna()]
cont_tot = cont_tot.merge(cont_dtl, on=['Breakdown', 'Month'], suffixes=['', '_CountryTotal'],
                          how='left')
cont_tot['value'] = cont_tot['value'] - cont_tot['value_CountryTotal'].fillna(0)

# remove the continent totals and union the new Unknown-country rows to the main dataframe
cont_tot['Country'] = 'Unknown'
cont_tot.drop(columns=['value_CountryTotal'], inplace=True)
df = concat([df[df['Country'].notna()], cont_tot])


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df.rename(columns={'value':'Number of Tourists'}, inplace=True)
df.to_csv('.\\outputs\\output-2021-12.csv', index=False, date_format='%d/%m/%Y',
          columns=['Breakdown', 'Month', 'Number of Tourists', 'Country'])


#---------------------------------------------------------------------------------------------------
# generate the charts
#---------------------------------------------------------------------------------------------------

charts_per_row = 3
sns.set_style(""white"")
sort=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

# summarize the data by region and year/month
df_region = df.groupby(['Breakdown', 'Month'])['Number of Tourists'].sum().reset_index()
df_region['Year'] = df_region['Month'].dt.year.astype(int)
df_region['Month Name'] = df_region['Month'].dt.strftime('%b')

# draw a grid of charts, one for each region, where x=month, y=# arrivals, and line per year
g = sns.relplot(kind='line', data=df_region, x='Month Name', y='Number of Tourists', units='Year', 
                hue='Year', col='Breakdown', palette='GnBu', linewidth=1, estimator=None, 
                col_wrap=charts_per_row, height=2.5, aspect=1.5, sort=sort, legend=True,
                facet_kws={'sharex':True, 'sharey':False, 'legend_out':True}).add_legend()

# place the legend
g._legend.set_bbox_to_anchor([1.1, 0])

# add spacing to the grid
g.fig.tight_layout(h_pad=4, w_pad=4)  

# add an orange line for the current year
for Breakdown, ax in g.axes_dict.items():
    # overlay the last year in orange
    sns.lineplot(data=df_region[(df_region['Year']==df_region['Year'].max()) 
                                & (df_region['Breakdown']==Breakdown)],
                 x='Month Name', y='Number of Tourists',  color='orange',
                 linewidth=3, ci=None, sort=sort, ax=ax, legend=None)
    ax.set_title(f""{Breakdown}"")
    ax.set(xlabel=None, ylabel=None)
    
# add the main title
g.fig.suptitle('Maldives Tourist Arrivals by Region and Year', x=0.58, y=1.1, fontsize='xx-large')
g.fig.text(s='Most recent year (' + str(df_region['Year'].max()) + ') is highlighted', 
          x=0.46, y=1.04, size='12')
plt.show()",
2021.0,47.0,Charts,```matplotlib```,"from 1-100, with 100 representing the highest value
- Note: we're using a ranking method that averages ties, pay particular attention to 
  countries visited!
- Join the pivots together
- Output the data (4 fields)
  - name
  - metric
  - raw_value
  - scaled_value

Author: Kelly Gilbert
Created: 2021-11-25
Requirements:
  - input dataset:
      - top_female_poker_players_and_events.xlsx
  - output dataset (for results check only):
      - output.csv
""""""


import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from numpy import ceil, pi, where
from pandas import ExcelFile, melt, read_excel
from textwrap import wrap

# for results check only:
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\top_female_poker_players_and_events.xlsx') as xl:
    df_p = read_excel(xl, 'top_100')
    df_e = read_excel(xl, 'top_100_poker_events')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# replace any nulls in prize_usd with zero
df_e['prize_usd'] = df_e['prize_usd'].fillna(0)


# summarize event metrics for each player and add the player name
df_e['first_place'] = where(df_e['player_place']=='1st', 1, 0)
df_p_tot = df_e.groupby('player_id').agg(wins=('first_place', 'sum'),
                                         number_of_events=('event_date', 'count'),
                                         first_event=('event_date', 'min'),
                                         last_event=('event_date', 'max'),
                                         biggest_win=('prize_usd', 'max'),
                                         countries_visited=('event_country', 'nunique'))\
               .reset_index()\
               .merge(df_p[['player_id', 'name', 'all_time_money_usd']], on='player_id', how='left')\
               .rename(columns={'all_time_money_usd' : 'total_prize_money'})


# calculate win % and career length
df_p_tot['percent_won'] = df_p_tot['wins'] / df_p_tot['number_of_events']
df_p_tot['career_length'] = ((df_p_tot['last_event'] - df_p_tot['first_event']).dt.days) / 365.25


# pivot the metrics into rows
metrics = ['number_of_events', 'total_prize_money', 'biggest_win', 'percent_won', 
           'countries_visited', 'career_length']
df_out = df_p_tot.melt(id_vars='name', value_vars=metrics, var_name='metric', value_name='raw_value')
df_out['scaled_value'] = df_out.groupby('metric')['raw_value'].rank(method='average', ascending=True)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_out.to_csv(r'.\outputs\output-2021-47.csv', index=False)


#---------------------------------------------------------------------------------------------------
# radial bar chart
#---------------------------------------------------------------------------------------------------

# NOTE: this is not a good chart choice for this use case. I've recreated the chart here for my
# own practice (to see if I could figure it out!) but would not use this in practice.
#
#   The questions a user might be asking of this data are:
#   1 - Who are the top players across all metrics?
#   2 - Who are the top players for each individual metric (e.g. who had the highest win %?)
#   3 - I'm interested in player X. How did she perform across different metrics?
#
#   All of these questions are difficult to answer. It's possibly easiest to answer #3, but it's 
#   hard to tell the scale (is her red pie slice at 50 or 100?) This could be corrected by adding a 
#   100% circle around each chart.
#   A user could potentially answer #2 by scanning for larger areas, but it's very difficult to 
#   compare across players
#
#   Also, since the metrics are scaled by rank, there isn't a sense of scale across players (e.g. 
#   Is there a big gap between first and 2nd place?)
# 
#   Some better choices:
#   A bump chart would help us answer all three questions (keeping the scaled rank), although
#     interactivity may be needed to help a user find a specific player for #3.
#   A parallel coordinates chart would help answer 1 and 2 (adding interactivity where a user could
#     highlight a specific player would cover #3).
#   Bar charts by metric would help us answer #2, and potentially #1 if the top players in each
#     metric are similar. We could also eliminate the rank scale to show meaningful differences.
#     Highlight interaction would help answer #3.   



# I used these examples while creating the chart output:
# Circular barplot: https://www.python-graph-gallery.com/circular-barplot-basic
# Small multiples: https://jonathansoma.com/lede/data-studio/classes/small-multiples/long-explanation-of-using-plt-subplots-to-create-small-multiples/


# dimensions of the chart grid
player_count = df_out['name'].nunique()
metric_count = df_out['metric'].nunique()
CHARTS_PER_ROW = 7    # number of horizontal charts
CHARTS_PER_COL = int(ceil(player_count / CHARTS_PER_ROW))


# assign colors to each metric name
color_map = { 'biggest_win' : '#a9ded5',        # green
              'career_length' : '#ffffc5',      # yellow
              'countries_visited' : '#cecbe3',  # purple
              'number_of_events' : '#fc9f94',   # red
              'percent_won' : '#9fc4de',        # blue
              'total_prize_money' : '#fdc688'}  # orange


# calculate the width and central angle for each bar, in radians
# the original chart starts with biggest_win at 180 degrees, then moves clockwise, alphabetically
width = 2*pi / metric_count
df_out['angle'] = pi + (2*pi - (df_out['metric'].rank(method='dense') - 1) * 2*pi / metric_count) 


# initialize as polar coordinates and get all of the subplot axes
fig, axes = plt.subplots(figsize=(CHARTS_PER_ROW * 2.1, CHARTS_PER_COL * 2), 
                         subplot_kw={""projection"": ""polar""}, 
                         nrows=CHARTS_PER_COL, ncols=CHARTS_PER_ROW, sharex=True, sharey=True)

# flatten the axis list
#     axes is a list of lists (each element in axes is a row of axes, 
#     and each element of the row is an axis)
axis_list = [ax for row in axes for ax in row]


# make axis 0 blank and float a legend over it
ax = axis_list[0]
ax.axis('off')   

handles = [mpatches.Patch(color=v, label=k) for k, v in color_map.items()]
fig.legend(handles=handles, bbox_to_anchor=(0.14, 0.985), frameon=False)
    # bbox is relative to the whole size of the outer figure (where 1,1 is the upper right corner)
    # and position is the middle of the legend

# edit the subplot axes
n = 1
for player_name, player_data in df_out.groupby(['name']):

    # get the current axis
    ax = axis_list[n]
    
    # draw the radial bars
    ax.bar(
        x=player_data['angle'], 
        height=player_data['scaled_value'], 
        width=width, 
        bottom=0, 
        linewidth=1, 
        edgecolor='darkgray',
        color=player_data['metric'].map(color_map)
    )    
    
    # add title, format player chart
    ax.set_title(""\n"".join(wrap(player_name, 15, break_long_words=False)), y=-0.6) 
    ax.set_ylim(0, 100)    
    ax.axis('off')                     # remove the axis labels/grid
 
    n += 1


# remove extra axes (if CHARTS_PER_COL * CHARTS_PER_ROW > player_count)
for ax in axis_list[player_count+1:]:
    ax.remove()
    
    
# overall title
plt.suptitle('Top 100 Female Poker Players', fontsize=22, fontweight=4, y=1.02)

# overall layout adjustments
plt.tight_layout()
plt.subplots_adjust(wspace = 1.25, hspace = 1.25)  

plt.show()",
2022.0,22.0,Charts,```matplotlib```,"from one timestamp to the next. However for the last lines of 
  dialogue we'll need to know when the episode ends. To do this we'll need to union the dialogue with the episode details to find the last timestamp
- Create a rank of the timestamp for each episode, ordered by earliest timestamp
  - Think carefully about the type of rank you want to use
- Create a new column that is -1 the rank, so we can lookup the next line
- Create a duplicate dataset and remove all columns except
  - episode
  - next_line
  - time_in_secs
- Inner join these two datasets
- Calculate the dialogue durations
- Some character names are comma separated, split these names out and trim any trailing whitespace
  - It's ok to leave ""ALL"" as ""ALL""
- Reshape the data so we have a row per character
- Filter the data for just Gameplay sections
- Ensure no duplication of rows has occurred
- Output the data

Author: Kelly Gilbert
Created: 2022-MM-DD
Requirements:
  - input dataset:
      - Critical_Role_Campaign_1_Datapack.xlsx
  - output dataset (for results check only):
      - 2022W22 Output.csv
""""""


import matplotlib.pyplot as plt    # used for chart only
from numpy import where
import pandas as pd
from output_check import output_check    # custom function for comparing my results to the solution


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\Critical_Role_Campaign_1_Datapack.xlsx') as xl:
    df_eps = pd.read_excel(xl, sheet_name='episode_details')
    df_dialogue = pd.read_excel(xl, sheet_name='dialogue')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# get the next timestamp and filter to Gameplay sections
df_out = ( pd.merge_asof(df_dialogue.sort_values(by=['time_in_secs'])
                             .rename(columns={'time_in_secs' : 'start_time'}), 
                         df_dialogue[['Episode', 'time_in_secs']].sort_values(by=['time_in_secs'])
                             .rename(columns={'time_in_secs' : 'end_time'}), 
                         left_on='start_time', right_on='end_time', by=['Episode'], 
                         direction='forward', allow_exact_matches=False)
             .query(""section == 'Gameplay'"")
         )


# set the end time and calculate the duration
df_out['Duration'] = ( where(df_out['end_time'].isna(), 
                             df_out['Episode'].replace(dict(zip(df_eps['Episode'], 
                                                                 df_eps['runtime_in_secs']))), 
                             df_out['end_time'])
                       - df_out['start_time']
                     ).astype(int)


# split (duplicate) if there are multiple characters in name
df_out = ( df_out.assign(name=df_out['name'].str.replace(' ', '').str.split(','),
                         dialogue=df_out['dialogue'].astype(str).str.strip())
                 .explode('name')
                 .drop(columns=['end_time'])
                 .drop_duplicates()
         )


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_out.to_csv(r'.\outputs\output-2022-22.csv', index=False, 
              columns=['Episode', 'name', 'start_time', 'Duration', 'youtube_timestamp', 
                       'dialogue', 'section'])",
2020.0,4.0,Other,Function,"delta, to_datetime


def age(begin, end):
    """"""
    calculates the number of full years between begin and end
    """"""
    return end.year - begin.year # - ((end.month, end.day) < (begin.month, begin.day))


def split_dict(in_dict):
    """"""
    add the list of misspelled names as keys, with the correct spelling as the value
    """"""
    
    new_dict = { i:k for k, v in in_dict.items() for i in v}    
    new_dict.update({ k.lower() : k.title() for k in in_dict.keys() })
    return new_dict


CURRENT_DATE = dt.datetime(2020, 1, 22)


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = read_csv(r'.\inputs\PD 2020 Wk 4 Input.csv', parse_dates=['DoB'], dayfirst=True)
df_q = read_csv(r'.\inputs\Store Survey Results - Question Sheet.csv')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean the Country and Store names
#df['Country'].unique()
#df['Store'].unique()

country_dict = { 'England' : ['3ngland', 'egland', 'eggland', 'ingland'],
                 'Scotland' : ['sc0tland', 'scottish'],
                 'United States' : ['united state'],
                 'Netherlands' : ['the netherlands'] }

store_dict = { 'Amsterdam' : ['amstelveen']}

df['Country'] = df['Country'].str.lower().replace(split_dict(country_dict)).str.title()
df['Store'] = df['Store'].str.lower().replace(split_dict(store_dict)).str.title()


# summarize response attributes by Response #
# pivot_table excludes records if any of these cols are null, such as the DoB
df_r = df.drop_duplicates(subset='Response')[[c for c in df.columns if c not in ['Question Number', 'Answer']]]


# pivot questions by Response # and merge back to the attributes
df_p = df.pivot_table(index=['Response'], values='Answer', columns='Question Number', aggfunc='first')\
         .reset_index()\
       .merge(df_r, how='inner', on='Response')


# rename the columns
#     I chose to pivot using the numeric column names vs. merging the question list, because it would 
#     save memory for a large dataset vs. repeating the question for every row.
question_dict = {n : q for n, q in zip(df_q['Number'], df_q['Question'])}
df_p.columns = [question_dict.get(c, c) for c in df_p.columns]


# clean date and time, create Completion Date column
df_p['Date'] = to_datetime(df_p['What day did you fill the survey in?'],
                           dayfirst=True, errors='coerce')

df_p['Date'] = where(df_p['Date'].isna(), 
                     to_datetime(df_p['What day did you fill the survey in?']
                               .str.replace('.* - (.*)', '\\1 ' + str(CURRENT_DATE.year), regex=True)),
                     df_p['Date'])
    
df_p[['Hour', 'Minute', 'AMPM']] = df_p['What time did you fill the survey in?']\
                                       .str.replace('.', ':', regex=False)\
                                       .str.extract('(\d{1,2}):?(\d{2})\s?(.*)')
    
df_p['Hour'] = df_p['Hour'].astype(int) + where(df_p['AMPM'].str.lower().str.contains('pm'), 12, 0)
df_p['Minute'] = df_p['Minute'].astype(int)

df_p['Completion Date'] = df_p['Date']\
                          + to_timedelta(df_p['Hour'], unit='h')\
                          + to_timedelta(df_p['Minute'], unit='m')


# understand the age of the customer based on their Date of Birth (DoB)
df_p['Age of Customer'] = df_p['DoB'].apply(lambda x: age(x, CURRENT_DATE))


# remove any answers that are not a customer's first or latest
group_cols = ['Name', 'Store', 'Country']
df_p['Result'] = where(df_p['Completion Date'] == df_p.groupby(group_cols)['Completion Date'].transform('min'),
                       'First',
                       where(df_p['Completion Date'] == df_p.groupby(group_cols)['Completion Date'].transform('max'),
                             'Latest', 'Other'))
df_p = df_p.loc[df_p['Result'] != 'Other']


# classify NPS recommendation
score_field = 'Would you recommend C&BSco to your friends and family? (Score 0-10)'
df_p['Detractor'] = where(df_p[score_field].astype(int) <= 6, 1, nan)
df_p['Passive'] = where((df_p[score_field].astype(int) >= 7) 
                        & (df_p[score_field].astype(int) <= 8), 1, nan)
df_p['Promoter'] = where(df_p[score_field].astype(int) > 8, 1, nan)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

cols = ['Country', 'Store', 'Name', 'Completion Date', 'Result',
       'Would you recommend C&BSco to your friends and family? (Score 0-10)',
       'Promoter', 'Detractor', 'Passive', 'Age of Customer',
       'If you wouldn\'t, why?', 'If you would, why?']
df_p.to_csv(r'.\outputs\output-2020-04.csv', columns=cols, index=False, date_format='%d/%m/%Y %H:%M:%S')",
2020.0,11.0,Other,Function,"-------------------

def get_box_sizes(order_sizes, size_list):
    """"""
    Returns a dataframe with one column per box size, containing the number of boxes of that size
    
    order_sizes: a series containing the order sizes
    size_list: a list of box sizes
    
    returns: a dataframe with columns for each box size
    """"""    

    df = pd.DataFrame({'Order Size' : order_sizes})\
           .assign(Remainder=lambda df_x: df_x['Order Size'])
    size_list.sort(reverse=True)
    
    for i, s in enumerate(size_list):
        if i < len(size_list) - 1:
            df[f'Boxes of {s}'] = df['Remainder'] // s
            df['Remainder'] = df['Remainder'] % s
        else:
            df[f'Boxes of {s}'] = (np.ceil(df['Remainder'] / s)).astype(int)
            
    return df.drop(columns=['Order Size', 'Remainder'])


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\PD 2020 Week 11 Input.xlsx') as xl:
    df_orders = pd.read_excel(xl, sheet_name='Orders') 
    df_sizes = pd.read_excel(xl, sheet_name='Box Sizes')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# for each order, figure out how many boxes of each size will be required.
df_boxes = pd.concat([df_orders, 
                      get_box_sizes(df_orders['Order Size'], list(df_sizes['Box Size']))],
                      axis=1)

# create one row per box
df_soaps = df_boxes.melt(id_vars=['Order Number', 'Order Size'], var_name='Box_Size', 
                         value_name='Last Box Per Box Size')\
                   .assign(Box_Size=lambda df_x: df_x['Box_Size'].str.replace('Boxes of ', '').astype(int),
                           Box_Number=lambda df_x: [range(1, c+1) for c in df_x['Last Box Per Box Size']])\
                   .explode('Box_Number')\
                   .dropna(subset=['Box_Number'])\
                   .rename(columns=lambda c: c.replace('_', ' '))

# add the number of soaps per box
df_soaps['Soaps in Box'] = np.where((df_soaps['Box Size']==df_sizes['Box Size'].min()) 
                                     & (df_soaps['Box Number']==df_soaps['Last Box Per Box Size']),
                                    (df_soaps['Order Size'] - 1) % df_sizes['Box Size'].min() + 1,
                                    df_soaps['Box Size'])

# renumber the boxes 1 to total number of boxes
df_soaps['Box Number'] = df_soaps.groupby('Order Number')['Box Number'].transform('cumcount') + 1


#---------------------------------------------------------------------------------------------------
# output the files
#---------------------------------------------------------------------------------------------------

df_boxes.to_csv(r'.\outputs\output-2020-11-boxes-per-order.csv', index=False)
df_soaps.to_csv(r'.\outputs\output-2020-11-soaps-per-box.csv', index=False)",
2020.0,12.0,Other,Function,"ort datetime as dt


def year_week_nbr(date_in):
    """"""
    mimic tableau week number
    if the year starts on sunday, use week number %U as-is, otherwise add one
    """"""
    if dt.date(date_in.year,1,1).weekday == 6:    # sunday
        adder=0
    else:
        adder=1       

    return date_in.year * 100 + int(dt.datetime.strftime(date_in, '%U')) + adder


#------------------------------------------------------------------------------
# import and prep the data
#------------------------------------------------------------------------------

in_file = ExcelFile(r'.\inputs\PD week 12 input(1).xlsx')


# Total Sales: convert the scent name to all caps, no spaces
df_tot = read_excel(in_file, sheet_name='Total Sales')
#df_tot.dtypes    # make sure numbers read in properly
df_tot.rename(columns={'Scent' : 'Scent_orig'}, inplace=True)
df_tot['Scent_join'] = df_tot['Scent_orig'].str.replace(' ', '')


# Percentage of Sales:
# get the year/week
# concatenate the Product ID and Size for joining to the lookup table
df_pct = read_excel(in_file, sheet_name='Percentage of Sales')
#df_pct.dtypes    # make sure numbers read in properly
df_pct = df_pct[df_pct['Percentage of Sales'] != 0]
df_pct['Product_join'] = df_pct['Product ID'] + df_pct['Size']
df_pct['Year Week Number'] = [year_week_nbr(d) for d in df_pct['Week Commencing']] 

# read in the Lookup Table sheet and convert the scent to all caps, no spaces
df_lookup = read_excel(in_file, sheet_name='Lookup Table')
#df_lookup.dtypes    # make sure numbers read in properly
df_lookup['Scent_join'] = df_lookup['Scent'].str.upper().replace(' +', '', regex=True)


#------------------------------------------------------------------------------
# merge (join) the data
#------------------------------------------------------------------------------

# join the lookup data to the pct sales data
df = df_pct.merge(df_lookup, how='left', left_on='Product_join', right_on='Product')

# join the total sales data to the pct/lookup data
df = df.merge(df_tot, how='left', on=['Year Week Number', 'Scent_join'])


# check for unjoined data (should be none)
# unjoined = df[df['Total Scent Sales'].isna()]
# unjoined.groupby(['Scent_join', 'Year Week Number'])['Year Week Number'].count()


# calculate sales and clean up the final table
df['Sales'] = round(df['Total Scent Sales'] * 100 * df['Percentage of Sales'], 0)/100
df = df[['Year Week Number', 'Scent', 'Size', 'Product Type', 'Sales']][df['Sales'].notna()]


# output to csv
df.to_csv('.\\outputs\\output-2020-12.csv', index=False, 
          columns=['Year Week Number', 'Scent', 'Size', 'Product Type', 'Sales'])",
2021.0,8.0,Other,Function,"as import read_csv


def convert_id(customer_id):
    """"""convert long, numeric customer IDs to scientific notation to match the solution""""""
    if len(customer_id) > 6 and customer_id.isnumeric():
        customer_id = ""{:.2E}"".format(float(customer_id)) 
    return customer_id


#--------------------------------------------------------------------------------
# input the data
#--------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Copy of Karaoke Dataset.xlsx') as xl:
    choices = read_excel(xl, 'Karaoke Choices').sort_values(by='Date')
    customers = read_excel(xl, 'Customers', converters={'Customer ID':str}).sort_values(by='Entry Time')


#--------------------------------------------------------------------------------
# process the data
#--------------------------------------------------------------------------------

# fix long customer IDs 
customers['Customer ID'] = [convert_id(c) for c in customers['Customer ID']]


# determine the session (if the time between songs is >= 59 minutes, increment the session)
choices['Session #'] = choices['Date'].diff(1).dt.total_seconds().ge(59*60).cumsum() + 1


# number the songs in order for each session
choices['Song Order'] = choices.groupby('Session #')['Date'].rank('dense', ascending=True).astype(int)


# join the customer to the closest session after their entry time, 10 min tolerance
session_start = choices[choices['Song Order']==1][['Date','Session #']]

customer_sessions = merge_asof(customers, session_start, left_on='Entry Time', right_on='Date', 
                               tolerance=Timedelta(minutes=10), direction='forward').dropna()


# join customer sessions to song list
final = merge(choices, customer_sessions, how='left', on=['Session #'], suffixes=['','_y'])
final.drop(columns=['Date_y'], inplace=True)


#--------------------------------------------------------------------------------
# output the file
#--------------------------------------------------------------------------------

final['Date'] = final['Date'].dt.round('1s')
final.to_csv('.\\outputs\\output-2021-08.csv', index=False, date_format='%d/%m/%Y %H:%M:%S',
             columns=['Session #', 'Customer ID', 'Song Order', 'Date', 'Artist', 'Song'])",
2021.0,9.0,Other,Function,"as import read_csv


def round_half_up(n, decimals=0):
    """""" 
    use round half up method vs. Python default rounding
    """"""
    multiplier = 10 ** decimals
    # Replace math.floor with np.floor
    return floor(n*multiplier + 0.5) / multiplier

round_half_up_v = vectorize(round_half_up)


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

customer_info = read_excel('.\\inputs\\Customer Information.xlsx')['IDs'].str.split(' ').explode()
areas = read_excel('.\\inputs\\Area Code Lookup.xlsx')
products = read_excel('.\\inputs\\Product Lookup.xlsx')

# parse the customer info
extract_regex = r'\D*?(?P<phone>\d{6}).*?\,(?P<join>\d{2}\w)(?P<ordered>\d+)-(?P<product_id>.+)'
customers = DataFrame(customer_info)['IDs'].str.extract(extract_regex)
customers['ordered'] = customers['ordered'].astype(int)

# clean the price
products['Price'] = products['Price'].str.strip('').astype(float)

# remove areas with duplicate first letter/last 2 digits
areas['join'] = areas['Code'].astype(str).str.slice(-2) + areas['Area'].str.slice(stop=1)
areas['count'] = areas.groupby('join')['join'].transform('count')

# remove unnecessary areas
remove_areas = ['Clevedon', 'Fakenham', 'Stornoway']
areas = areas[(areas['count'] == 1) & (~areas['Area'].isin(remove_areas))]

# join area and product info to customers
customers = customers.merge(areas, on='join', how='inner')


#---------------------------------------------------------------------------------------------------
# calculate sales and ranks
#---------------------------------------------------------------------------------------------------

# group by area/item
area_item = customers.groupby(['Area', 'product_id'])[['ordered']].sum().reset_index()

# calculate the revenue per item and % of area
area_item = area_item.merge(products, left_on='product_id', right_on='Product ID', how='inner')
area_item['Revenue'] = round_half_up_v(area_item['ordered'] * area_item['Price'], 0).astype(int)

area_item['% of Total - Product'] = \
    round_half_up_v((area_item['Revenue'] / area_item.groupby('Area')['Revenue'].transform('sum')) * 100, 2)

# add the rank
area_item['Rank'] = area_item.groupby('Area').rank(ascending=False)[['Revenue']].astype(int)


#---------------------------------------------------------------------------------------------------
# output the data
#---------------------------------------------------------------------------------------------------

area_item.to_csv('.\\outputs\\output-2021-09.csv', index=False,
                 columns=['Rank', 'Area', 'Product Name', 'Revenue', '% of Total - Product'])",
2021.0,10.0,Other,Function,"as import read_csv


def get_evolution_group(p_name):
    """"""
    given a Pokemon name, returns the first Pokemon in the evolution hierarchy
    
    e_dict = a dictionary of evolution steps (keys: evolving to, values: evolving from)
    p_name = the Pokemon name to look up
    """"""
    if p_name not in evolution_dict.keys() or p_name == evolution_dict[p_name]: 
        return p_name
    else:
        return get_evolution_group(evolution_dict[p_name])
    

#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Pokemon Input.xlsx') as xl:
    pokemon = read_excel(xl, 'Pokemon')
    evolution = read_excel(xl, 'Evolution')


#---------------------------------------------------------------------------------------------------
# filter the pokemon and evolution lists
#---------------------------------------------------------------------------------------------------

# keep up to Generation III (up to and including #386) and not Mega evolutions
pokemon = pokemon[(pokemon['#'].astype(float) <= 386) & (pokemon['Name'].str.slice(0,5) != 'Mega ')]

# remove multiple rows for different types 
pokemon.drop(columns=['Type'], inplace=True)
pokemon.drop_duplicates(inplace=True)

# remove non-gen III from the evolution dataset
valid_names = list(pokemon['Name'])
evolution = evolution[(evolution['Evolving from'].isin(valid_names)) &
                      (evolution['Evolving to'].isin(valid_names))]


#---------------------------------------------------------------------------------------------------
# get evolution info
#---------------------------------------------------------------------------------------------------

evolution_dict = dict(zip(evolution['Evolving to'], evolution['Evolving from']))

# bring in information about what our Pokmon evolve TO (keep nulls)
df = pokemon.merge(evolution, left_on='Name', right_on='Evolving from', how='left')

# bring in information about what a Pokmon evolves FROM (keep nulls)
df['Evolving from'] = [evolution_dict[k] if k in evolution_dict.keys() else nan for k in df['Name']]

# get the first Pokemon in the evolution hierarchy
df['Evolution Group'] = df['Name'].apply(get_evolution_group)

# if it is a 3rd+ evolution, add the first evolution
df['First Evolution'] = [nan if (n==g) | (g==f) else g 
                         for n,g,f in zip(df['Name'], df['Evolution Group'], df['Evolving from'])]

# ensure we still have all of the Pokemon
if not pokemon['Name'].unique().sort() == df['Name'].unique().sort():
    print('The list of Pokemon does not match after joins')
    raise SystemExit
    
# ensure all rows are unique
df.drop_duplicates(inplace=True)


# --------------------------------------------------------------------------------------------------
# output the data
# --------------------------------------------------------------------------------------------------

output_cols = ['Evolution Group'] + list(pokemon.columns) + list(evolution.columns) + \
              ['First Evolution']
df.to_csv('.\\outputs\\output-2021-10.csv', index=False, columns=output_cols)",
2021.0,34.0,Other,Function," merge, read_excel


def stack_dict(in_dict):
    """"""
    stack all of the name options as keys, with the correct spelling as value
    """"""
    
    new_dict = { i:k for k, v in in_dict.items() for i in v}    
    new_dict.update({ k:k for k in in_dict.keys() })
    return new_dict


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

store_map = { 'Bristol' : ['Bristal', 'Bristole', 'Bristoll'],
              'Stratford' : ['Statford' , 'Stratfod', 'Straford', 'Stratfodd'],
              'Wimbledon' : ['Wimbledan', 'Vimbledon', 'Wimbledone'],
              'York' : ['Yor', 'Yorkk', 'Yark']
            }


with ExcelFile(r'.\inputs\2021 Week 34 Input.xlsx') as xl:
    sales = read_excel(xl, 'Employee Sales')\
            .melt(id_vars=['Store', 'Employee'], var_name='Month', value_name='Sales')
    targets = df = read_excel(xl, 'Employee Targets')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean store name in targets
store_map_stack = stack_dict(store_map)
targets['Store'] = targets['Store'].replace(store_map_stack)

missing_stores = [s for s in targets['Store'].unique() if s not in store_map_stack.keys()]
if len(missing_stores) > 0:
    print('The following stores names are not present in the lookup:\n' + '\n'.join(missing_stores))


# join sales to targets
sales = sales.merge(targets, on=['Store', 'Employee'], how='left')
sales['met_target_flag'] = where(sales['Sales'] >= sales['Monthly Target'], 1, 0)


# summarize
summary = sales.groupby(['Store', 'Employee']).agg(Avg_monthly_Sales=('Sales', 'mean'),
                                                   pct_of_months_target_met=('met_target_flag', 'mean'),
                                                   Monthly_Target=('Monthly Target', 'mean'))\
                                              .reset_index()
summary.columns = [c.replace('pct_', '%_').replace('_', ' ') for c in summary.columns]


# keep employees below 90% of target
summary = summary.loc[summary['Avg monthly Sales'] < summary['Monthly Target'] * 0.9]


# % of months meeting target
summary['% of months target met'] = round(summary['% of months target met'] * 100, 0)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

out_cols = ['Store', 'Employee', 'Avg monthly Sales', '% of months target met', 'Monthly Target']
summary.to_csv(r'.\outputs\output-2021-34.csv', index=False, columns=out_cols)",
2021.0,35.0,Other,Function," merge, read_excel


def parse_sizes(s):
    """"""
    parses the a x b size strings, converts to centimeters, and calculates the area
    input: a series of strings
    output: a dataframe containing three columns: Max Side, Min Side, and Area
    """"""
    df = s.str.extract(r'(\d+)(\S+)(?: x )?(\d+)?(\S+)?')
    
    # convert to centimeters
    df[0] = df[0].astype(float) * where(df[1] == '""', 2.54, 1)
    df[2] = where(df[2].isna(), df[0], df[2]).astype(float) * where(df[1] == '""', 2.54, 1)
    
    # find largest size
    df['Max Side'] = where(df[0] >= df[2], df[0], df[2])
    df['Min Side'] = where(df[0] < df[2], df[0], df[2])
    df['Area'] = df['Max Side'] * df['Min Side']
    
    return df[['Max Side', 'Min Side', 'Area']]


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\inputs\Pictures Input.xlsx') as xl:
    p = read_excel(xl, 'Pictures')
    f = read_excel(xl, 'Frames').drop_duplicates().rename(columns={'Size' : 'Frame'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split up the sizes of the pictures and the frames into lengths and widths
p[['Max Side', 'Min Side', 'Area']] = parse_sizes(p['Size'])
f[['Max Side', 'Min Side', 'Area']] = parse_sizes(f['Frame'])


# all combinations of pictures and frames
df = p.assign(key=1).merge(f.assign(key=1), how='inner', on='key', suffixes=['', '_f'])\
      .drop('key', 1)


# find valid picture/frame combinations with the smallest area difference
df = df.loc[(df['Max Side'] <= df['Max Side_f']) & (df['Min Side'] <= df['Min Side_f'])]
df['Area_diff'] = df['Area_f'] - df['Area']
df = df[df['Area_diff'] == df.groupby(['Picture'])['Area_diff'].transform('min')]


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df.to_csv(r'.\outputs\output-2021-35.csv', index=False, 
          columns=['Picture', 'Frame', 'Max Side', 'Min Side'])",
2021.0,42.0,Other,Function,"necessary (up to 9)

def apply_format(s):
    """"""
    formats a series of numbers as strings
        if nan --> empty string
        if whole number --> no decimal places
        otherwise --> 9 decimal places
    """"""
    
    return where(s.isna(), '', 
                 s.map(lambda x: f'{x:.0f}' if x == round(x, 0) else str(x)))


df['Value raised per day'] = apply_format(df['Value raised per day'])
df['Avg raised per weekday'] = apply_format(df['Avg raised per weekday'])


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df.to_csv(r'.\outputs\output-2021-42.csv', index=False,
          columns=['Avg raised per weekday', 'Value raised per day', 'Days into fund raising', 
                   'Date', 'Total Raised to date'])",
2021.0,51.0,Other,Function,"as import read_csv


def print_errors(df_in, message):
    if len(df_in) > 0:
        print('-' * (len(message) + 4))
        print(f'{message}\n')
        print('-' * (len(message) + 4))
        print(df_in)


def sort_ignorecase(x):
    if x.dtype == 'O':
        return x.str.lower()
    else:
        return x


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = read_csv(r'.\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expand=True)

print_errors(df[(df['Store'].isna()) | (df['OrderID'].isna())][['OrderID']], 
             'The following OrderIDs could not be parsed:')

    
# turn the Return State field into a binary Returned field
df['Returned'] = where(df['Return State'].notna(), 1, 0)


# create a Sales field
df['Unit Price'] = df['Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df['Sales'] = df['Unit Price'] * df['Quantity']

print_errors(df[df['Sales'].isna()][['OrderID', 'Product Name', 'Unit Price', 'Quantity']], 
                'Sales could not be calculated:')


# create the Store dimension table
df_store = df.groupby('Store')['Order Date'].min().reset_index()\
             .sort_values(by=['Order Date', 'Store'], key=sort_ignorecase)\
             .rename(columns={'Order Date' : 'First Order'})
df_store['StoreID'] = range(1, len(df_store) + 1)


# create the Customer dimension table
df_cust = df.groupby('Customer').agg(Returned=('Returned', 'sum'),
                                     Order_Lines=('OrderID', 'count'),
                                     Number_of_Orders=('OrderID', 'nunique'),
                                     First_Order=('Order Date', 'min')).reset_index()\
            .sort_values(by=['First_Order', 'Customer'], key=sort_ignorecase)
df_cust.columns = [c.replace('_', ' ') for c in df_cust.columns]
df_cust['Return %'] = (df_cust['Returned'] / df_cust['Order Lines']).round(2)
df_cust['CustomerID'] = range(1, len(df_cust) + 1)


# create the Product dimension table
df_prod = df.groupby(['Category', 'Sub-Category', 'Product Name'])\
            .agg(Unit_Price=('Unit Price', 'mean'),
                 First_Sold=('Order Date', 'min')).reset_index()\
            .sort_values(by=['First_Sold', 'Product Name'], key=sort_ignorecase)
df_prod.columns = [c.replace('_', ' ') for c in df_prod.columns]
df_prod['ProductID'] = range(1, len(df_prod) + 1)


# replace the dimensions with their IDs in the original dataset to create the fact table
df = df.merge(df_store[['StoreID', 'Store']], on='Store', how='left')\
       .merge(df_cust[['CustomerID', 'Customer']], on='Customer', how='left')\
       .merge(df_prod[['ProductID', 'Product Name']], on='Product Name', how='left')


#---------------------------------------------------------------------------------------------------
# output the files
#---------------------------------------------------------------------------------------------------

df_store.to_csv(r'.\outputs\output-2021-51-store.csv', index=False, date_format='%d/%m/%Y',
                columns=['StoreID', 'Store', 'First Order'])
df_cust.to_csv(r'.\outputs\output-2021-51-customer.csv', index=False, date_format='%d/%m/%Y',
               columns=['CustomerID', 'Customer', 'Return %', 'Number of Orders', 'First Order'])
df_prod.to_csv(r'.\outputs\output-2021-51-product.csv', index=False, date_format='%d/%m/%Y',
               columns=['ProductID', 'Category', 'Sub-Category', 'Product Name', 'Unit Price', 
                        'First Sold'])
df.to_csv(r'.\outputs\output-2021-51-order.csv', index=False, date_format='%d/%m/%Y',
          columns=['StoreID', 'CustomerID', 'OrderID', 'Order Date', 'ProductID', 'Returned',
                   'Quantity', 'Sales'])",
2022.0,27.0,Other,Function,"-------------------

def output_group(df_in, group_cols): 
    product_type = df_in['Product Type'].max()

    ( df_in.groupby(group_cols, as_index=False)
           .agg(Sale_Value=('Sale Value', 'sum'),
                Present_in_N_orders=('Order ID', 'nunique'))
           .rename(columns=lambda x: x.replace('_', ' '))
           .to_csv(f'.\\outputs\\output-2022-27-{product_type}.csv', index=False)
    )


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"", 
                 parse_dates=['Sale Date'], dayfirst=True)
    

#---------------------------------------------------------------------------------------------------
# process the data (split path)
#---------------------------------------------------------------------------------------------------

# separate out the Product Name field to form Product Type and Quantity
df[['Product Type', 'Original Quantity', 'Unit']] = df['Product Name'].str.extract('(.+?) - (\d+)(.*)')


# split the data based on product type
df_bar = df.loc[df['Product Type']=='Bar', df.columns]
df_liquid = df.loc[df['Product Type']=='Liquid', df.columns]


# for liquid, ensure every value is in milliliters
df_liquid['Quantity'] = ( df_liquid['Original Quantity'].astype(int) 
                          * where(df_liquid['Unit'] == 'L', 1000, 1) )


#---------------------------------------------------------------------------------------------------
# output the file (split path)
#---------------------------------------------------------------------------------------------------

output_group(df_bar, ['Store Name', 'Region', 'Quantity'])
output_group(df_liquid, ['Store Name', 'Region', 'Quantity'])",
2022.0,31.0,Other,Function,"-------------------

def output_data(df, store_name):
    """"""
    Prep data and output the file for the selected store.
    """"""
    
    # keep liquid and selected store
    df_store = df.loc[(df['Store Name']==store_name) & (df['Product Name'].str.contains('Liquid')),
                      df.columns]
    
    # split the Product Name field into Product Type and Size
    df_store[['Product Type', 'Size']] = df_store['Product Name'].str.extract('(.*) - (.*)')
    
    # rank based on sales and keep the top 10
    df_store['Rank of Product & Scent by Store'] = \
        df_store['Sale Value'].rank(method='first', ascending=False)
    df_out = df_store.loc[df_store['Rank of Product & Scent by Store'] <= 10, df_store.columns]
    
    # round the Sales Values to the nearest 10 value (ie 1913 becomes 1910)
    df_out['Sale Value'] = df_out['Sale Value'].round(-1)

    # output the file    
    df_out.to_csv(f'.\\outputs\\output-2022-31-{store_name}.csv', index=False,
                  columns=['Store Name', 'Rank of Product & Scent by Store', 'Scent Name', 
                           'Size', 'Sale Value'])
    
    print(f'\n*** SUCCESS: the file for {store_name} has been created.\n')


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# input the file and sum sales by product, scent, and store
df = ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"")
         .groupby(['Product Name', 'Scent Name', 'Store Name'], as_index=False)['Sale Value'].sum() 
     )


#---------------------------------------------------------------------------------------------------
# get user input and output the file
#---------------------------------------------------------------------------------------------------

store_list = sorted(df['Store Name'].unique())
store_list_str = '\n  '.join([f'{i+1} - {n}' for i,n in enumerate(store_list)])

while True:
    input_num = input('\nStore list:\n  ' + store_list_str + '\n\n' 
                      + 'Please enter a number (or press Enter to quit): ')
    
    if input_num.isnumeric() and int(input_num) in range(1, len(store_list)+2):
        output_data(df, store_list[int(input_num)-1])
        
    elif input_num == '':
        break
    
    else:    
        print(f'\n*** ERROR: {input_num} is not a valid number.\n')",
2022.0,34.0,Other,Function,"-------------------

def get_user_input(value_name, value_list):
    """"""
    present a menu of options to the user and return the user's choice
    """"""
    
    value_list = sorted(value_list)
    options_str = '\n'.join([f'  {i+1} - {c}' for i,c in enumerate(value_list)])
    
    while True:
        user_input = input(f'\n{value_name.title()} list:\n{options_str}\n\n'
                           + f'Select an option (1 - {len(value_list)}) or press Enter to cancel:')
        
        if user_input.isnumeric() and int(user_input) in range(1, len(value_list)+1):
            return value_list[int(user_input)-1]
        elif user_input == '':
            return user_input
        else:
            print(f'\n*** ERROR: {user_input} is not a valid choice. '
                  + f'Please enter a number between 1 and {len(value_list)}.')


def input_and_prep_data(input_path):
    """"""
    input and preps the file
    """"""
    
    # input the data
    df = ( pd.read_csv(input_path, parse_dates=['Date'], dayfirst=True)
             .rename(columns={'Value' : 'Mins'})
             .rename(columns=lambda c: 'Unnamed' if 'Unnamed' in c else c)
         )


    # split the unnamed column and change music type to title case
    df[['Coach', 'Calories', 'Music Type']] = df['Unnamed'].str.extract('(.*)\s+-\s+(\d+)\s+-\s+(.*)')
    df['Music Type'] = df['Music Type'].str.strip().str.title()
    df['Calories'] = df['Calories'].astype(int)
    
    return df[['Coach', 'Calories', 'Music Type', 'Date', 'Mins']]


def output_file(df, coach, music_type, n):
    """"""
    filter the data and output the file
    """"""
    
    # top in by calories burned
    df_out = df.loc[(df['Coach'] == coach) 
                    & (df['Music Type'] == music_type), df.columns]
    
    # rank by calories burned
    df_out['Rank'] = df_out.groupby(['Coach', 'Music Type'])['Calories'].rank(ascending=False, 
                                                                              method='dense')
    
    
    # output the file
    filepath = f'.\\outputs\\output-2022-34 - Top {n} for rides with {coach} powered by {music_type}.csv'
    ( df_out[df_out['Rank'] <= n]
          .sort_values('Rank', ascending=True)
          .to_csv(filepath, index=False, date_format='%d/%m/%Y')
    )
    
    print(f""\n*** File created: {filepath}"")


#---------------------------------------------------------------------------------------------------
# main loop - get user input and output the file
#---------------------------------------------------------------------------------------------------

df = input_and_prep_data(r"".\inputs\Preppin' Summer 2022 - CEO Cycling.csv"")


while True:    
    coach = get_user_input('Coach', df['Coach'].unique())
    if coach == '':
        break
        
    music_type = get_user_input('Music type', df['Music Type'].unique())
    if music_type == '':
        break
    
    n = input('\nReturn the top n sessions by calories burned (or press Enter to cancel):\n')
    if n == '':
        break
    elif not n.isnumeric() or int(n) <= 0:
        print(f'\n*** ERROR: {n} is not a valid number. Please enter a number greater than zero.')
        break
    
    output_file(df, coach, music_type, int(n))",
2022.0,35.0,Other,Function,"-------------------

def to_float(in_str):
    """"""
    if in_str is a float, return the float value. otherwise, return None
    """"""
    
    try:
        return float(in_str)
    except:
        return None


def input_and_prep(kph):
    """"""
    input the file and perform initial data prep
    """"""
    
    # input and prep data
    df =  ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - CEO Cycling.csv"", 
                        parse_dates=['Date'], dayfirst=True)
              .rename(columns={'Value' : 'Mins'})
              .rename(columns=lambda c: 'Unnamed' if 'Unnamed' in c else c)
              .drop(columns=['Units', 'Type'])
          )

    df[['Coach', 'Calories', 'Music Type']] = df['Unnamed'].str.extract('(.*)\s+-\s+(\d+)\s+-\s+(.*)')
    df['Music Type'] = df['Music Type'].str.strip().str.title()
    df['Year'] = -df['Date'].dt.year
    df['Calories'] = df['Calories'].astype(int)
    df['Distance'] = kph * df['Mins'] / 60
    
    return df


def aggregate_and_output(df):
    """"""
    perform aggregations and generate the output file
    """"""
    
    # aggregate by year
    df_agg = ( df.groupby(['Year'])
                 .agg(Total_Mins=('Mins', 'sum'),
                      Total_Calories=('Calories', 'sum'),
                      Total_Rides=('Mins', 'count'),
                      Total_Distance=('Distance', 'sum'))
             )
    df_agg['Avg. Calories per Ride'] = (df_agg['Total_Calories'] / df_agg['Total_Rides']).round(1)
    df_agg['Avg. Calories per Minute'] = (df_agg['Total_Calories'] / df_agg['Total_Mins']).round(1)


    # aggregate by year and coach
    df_coach = ( df.groupby(['Year', 'Coach'], as_index=False)
                   .agg(Mins=('Mins', 'sum'),
                        Calories=('Calories', 'sum'))
               )
    
    df_coach['Calories per Min'] = df_coach['Calories'] / df_coach['Mins']
    df_coach['Calories per Minute per Coach'] = ( df_coach['Coach'] + ' (' 
                                       + (df_coach['Calories per Min']).round(1).astype(str) + ')' )
    df_coach['Total Mins per Coach'] = df_coach['Coach'] + ' (' + df_coach['Mins'].astype(str) + ')'


    # add the coach aggregations to the total df
    df_all = ( pd.concat([df_agg.drop(columns=['Total_Calories']), 
                          df_coach.iloc[df_coach.groupby('Year')['Mins'].idxmax()]
                                                .set_index('Year')
                                                ['Total Mins per Coach'],
                          df_coach.iloc[df_coach.groupby('Year')['Calories per Min'].idxmax()]
                                                .set_index('Year')
                                                ['Calories per Minute per Coach']],
                          axis=1)
                .rename(columns=lambda c: c.replace('_', ' '))
             )    
    
 
    # reshape the df so measures are in rows and years are in cols, then output the data  
    ( df_all.melt(ignore_index=False, var_name='Measure')
            .pivot_table(index='Measure', values='value', columns='Year', aggfunc='first')
            .reset_index()
            .rename(columns=lambda c: str(c).replace('-', ''))
            .to_csv(r'.\outputs\output-2022-35.csv', index=False)
    )
    
    print('*** File output complete.')


#---------------------------------------------------------------------------------------------------
# get user input for speed and generate the file
#---------------------------------------------------------------------------------------------------

while True:
    input_kph = input('Enter the average pace (kph):')
    kph = to_float(input_kph)
    
    if input_kph == '' :
        break
    elif kph == None or kph <= 0:
        print(f'*** ERROR: {input_kph} is not a vailid number. Please enter a number > 0.')
    else:
        df = input_and_prep(kph)
        aggregate_and_output(df)",
,,Other,Function,"as import read_csv


def sort_ignorecase(x):
    if x.dtype == 'O':
        return x.str.lower()
    else:
        return x


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = read_csv(r'..\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expand=True)

    
# turn the Return State field into a binary Returned field
df['Returned'] = where(df['Return State'].notna(), 1, 0)


# create a Sales field
df['Unit Price'] = df['Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df['Sales'] = df['Unit Price'] * df['Quantity']


# add IDs
for c in ['Store', 'Customer', 'Product Name']:
    df.sort_values(by=['Order Date', c], key=sort_ignorecase, inplace=True)
    df[f""{c.replace(' Name', '')}ID""] = df[c].factorize()[0] + 1


# create the Store dimension table
df_store = df.groupby(['StoreID', 'Store'])['Order Date'].min().reset_index()\
             .rename(columns={'Order Date' : 'First Order'})
             

# create the Customer dimension table
df_cust = df.groupby(['CustomerID', 'Customer'])\
            .agg(Returned=('Returned', 'sum'),
                 Order_Lines=('OrderID', 'count'),
                 Number_of_Orders=('OrderID', 'nunique'),
                 First_Order=('Order Date', 'min')).reset_index()
df_cust.columns = [c.replace('_', ' ') for c in df_cust.columns]
df_cust['Return %'] = (df_cust['Returned'] / df_cust['Order Lines']).round(2)


# create the Product dimension table
df_prod = df.groupby(['ProductID', 'Category', 'Sub-Category', 'Product Name'])\
            .agg(Unit_Price=('Unit Price', 'mean'),
                 First_Sold=('Order Date', 'min')).reset_index()
df_prod.columns = [c.replace('_', ' ') for c in df_prod.columns]",
,,Other,Function,"as import read_csv


def sort_ignorecase(x):
    if x.dtype == 'O':
        return x.str.lower()
    else:
        return x

#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = read_csv(r'..\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expand=True)

    
# turn the Return State field into a binary Returned field
df['Returned'] = where(df['Return State'].notna(), 1, 0)


# create a Sales field
df['Unit Price'] = df['Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df['Sales'] = df['Unit Price'] * df['Quantity']


# create the Store dimension table
df_store = df.groupby('Store')['Order Date'].min().reset_index()\
             .sort_values(by=['Order Date', 'Store'], key=sort_ignorecase)\
             .rename(columns={'Order Date' : 'First Order'})
df_store['StoreID'] = range(1, len(df_store) + 1)


# create the Customer dimension table
df_cust = df.groupby('Customer').agg(Returned=('Returned', 'sum'),
                                     Order_Lines=('OrderID', 'count'),
                                     Number_of_Orders=('OrderID', 'nunique'),
                                     First_Order=('Order Date', 'min')).reset_index()\
            .sort_values(by=['First_Order', 'Customer'], key=sort_ignorecase)
df_cust.columns = [c.replace('_', ' ') for c in df_cust.columns]
df_cust['Return %'] = (df_cust['Returned'] / df_cust['Order Lines']).round(2)
df_cust['CustomerID'] = range(1, len(df_cust) + 1)


# create the Product dimension table
df_prod = df.groupby(['Category', 'Sub-Category', 'Product Name'])\
            .agg(Unit_Price=('Unit Price', 'mean'),
                 First_Sold=('Order Date', 'min')).reset_index()\
            .sort_values(by=['First_Sold', 'Product Name'], key=sort_ignorecase)
df_prod.columns = [c.replace('_', ' ') for c in df_prod.columns]
df_prod['ProductID'] = range(1, len(df_prod) + 1)


# replace the dimensions with their IDs in the original dataset to create the fact table
df = df.merge(df_store[['StoreID', 'Store']], on='Store', how='left')\
       .merge(df_cust[['CustomerID', 'Customer']], on='Customer', how='left')\
       .merge(df_prod[['ProductID', 'Product Name']], on='Product Name', how='left')",
,,Other,Function,"-------------------

def create_data(recs, card='low'):
    # generate sample data
    rnd.seed(0)
    
    # similar cardinality to the sample dataset (380 choices)
    if card == 'sample':
        return DataFrame({'Customer' : [''.join(rnd.choices('ABCDEFGHIJKLMNOPQRST', k=2))*10 
                                        for n in range(0, recs)],
                          'Order Date' : rnd.choices(date_range('2000-01-01', '2021-12-31'), k=recs)})

    # low cardinality customer (52 possible choices)
    elif card == 'low':
        return DataFrame({'Customer' : [rnd.choice(string.ascii_letters)*20 for n in range(0, recs)],
                          'Order Date' : rnd.choices(date_range('2000-01-01', '2021-12-31'), k=recs)})
                          
    # medium cardinality customer (2,652 possible choices)
    elif card == 'med':
        return DataFrame({'Customer' : [''.join(rnd.choices(string.ascii_letters, k=2))*10 
                                        for n in range(0, recs)],
                          'Order Date' : rnd.choices(date_range('2000-01-01', '2021-12-31'), k=recs)})

    # high cardinality customer (5.74 x 10^16 possible choices)
    else:
        return DataFrame({'Customer' : [''.join(rnd.choices(string.ascii_letters, k=10))*2 
                                        for n in range(0, recs)],
                          'Order Date' : rnd.choices(date_range('2000-01-01', '2021-12-31'), k=recs)})


def use_factorize(df):
    # add customer ID using factorize
    df.sort_values(by=['Order Date', 'Customer'], inplace=True)
    df['CustomerID'] = df['Customer'].factorize()[0] + 1
                  
    # create the Customer dimension table
    df_cust = df.groupby(['CustomerID', 'Customer'], as_index=False)\
                .agg(First_Order=('Order Date', 'min'))


def use_groupby_and_merge(df):
    # create the Customer dimension table
    df_cust = df.groupby(['Customer'], as_index=False)\
                .agg(First_Order=('Order Date', 'min'))\
                .sort_values(by=['First_Order', 'Customer'])\
                .assign(CustomerID=range(1, df['Customer'].nunique()+1))
                
    # add the customer ID to the main table
    df = df.merge(df_cust[['CustomerID', 'Customer']], on='Customer')


def run_factorize():
    df = create_data(1000000, card='high')
    use_factorize(df)
    
def run_groupby_and_merge():
    df = create_data(1000000, card='high')
    use_groupby_and_merge(df)


# --------------------------------------------------------------------------------------------------
# line profiling
# --------------------------------------------------------------------------------------------------

%load_ext line_profiler

%lprun -u 0.001 -f use_factorize run_factorize()
%lprun -u 0.001 -f use_groupby_and_merge run_groupby_and_merge()",
2021.0,10.0,Other,Function (recursive),"s import read_csv


def get_evolution_group(p_name):
    """"""
    given a Pokemon name, returns the first Pokemon in the evolution hierarchy
    
    e_dict = a dictionary of evolution steps (keys: evolving to, values: evolving from)
    p_name = the Pokemon name to look up
    """"""
    if p_name not in evolution_dict.keys() or p_name == evolution_dict[p_name]: 
        return p_name
    else:
        return get_evolution_group(evolution_dict[p_na",1.0
2019.0,4.0,Other,Loops,"File, read_excel

# for results check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\PD - ESPN stats.xlsx') as xl:
    df = read_excel(",1.0
2020.0,1.0,Other,Loops,"l
dfSubtotal = None
for i in range(maxLevel, 0, -1):
    # remove a layer",1.0
2020.0,2.0,Other,Loops,"m/%y'), '%m/%d/%Y') for d in df['Date']]


# clean time field:

# keep numeric char",1.0
2020.0,3.0,Other,Loops,"
result_sheets = [s for s in xl.sheet_names if 'Results' in s]
df = None
for sheet in result_sheets:
    df = concat([df,",1.0
2020.0,9.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2020 Wk 9",1.0
2020.0,11.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2020 Wk 1",1.0
2021.0,3.0,Other,Loops," - output datasets (for results check):
    - Product Quarte",1.0
2021.0,4.0,Other,Loops,"  - output dataset (for results check):
    - PD 2021 Wk 4 O",1.0
2021.0,5.0,Other,Loops,"  - output dataset (for results check):
    - Current AM Dat",1.0
2021.0,6.0,Other,Loops,"  - output dataset (for results check):
    - 2021W06 Output",1.0
2021.0,7.0,Other,Loops," read_excel

# used for answer check only
from pandas import read_csv


# import the data
with ExcelFile(r'.\\inputs\\Shopping List and Ingredients.xlsx') as xl:
    dfItems = read_e",1.0
2021.0,8.0,Other,Loops,", Timedelta

# used for answer check only
from pandas import read_csv


def convert_id(customer_id):
    """"""convert long,",1.0
2021.0,9.0,Other,Loops,"s 0.25.0 or higher (for explode)
  - input datasets:
      - Area Code Lo",1.0
2021.0,10.0,Other,Loops,"  - output dataset (for results check):
      - Pokemon Outp",1.0
2021.0,11.0,Other,Loops," read_excel

# used for answer check only
from pandas import read_csv


# --------------------------------------------------------------------------------------------------
# input the data
# --------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Cocktails Dataset.xlsx') as xl:
    cocktails = read",1.0
2021.0,12.0,Other,Loops,"e don't have data
  for all countries in a continent, so it's not as simple as just filtering out the totals and
  subtotals. Plus in our Continents level of detail, we also have The Middle East and UN passport
  holders as categories. If you feel confident in your prep skills, this (plus the output) should be
  enough information to go on, but otherwise read on for a breakdown of the steps we need to take:
    - Filter out Tot",1.0
2021.0,13.0,Other,Loops,"(pl_xx-yy.csv files for each season)
  - output datasets (for results check):
    - Rank by Positi",1.0
2021.0,15.0,Other,Loops,"able, read_excel

# for results check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Menu and Orders.xlsx') as xl:
    menu = read_exce",1.0
2021.0,17.0,Other,Loops,"melt, read_excel

# for solution check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Preppin Data Challenge.xlsx') as xl:
    df = read_excel(",1.0
2021.0,18.0,Other,Loops,"  - output dataset (for results check):
      - PD 2021 Wk 1",1.0
2021.0,19.0,Other,Loops,"  - output dataset (for results check):
      - PD 2021 Wk 1",1.0
2021.0,20.0,Other,Loops,"ting a separate tab for each
    for n in df_outliers['n'].unique():
        df_out = df_",1.0
2021.0,21.0,Other,Loops,"cel, to_datetime

# for solution check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = None
with ExcelFile(r'.\\inputs\\PD 2021 Wk 21 Input.xlsx') as xl:
    for s in xl.shee",1.0
2021.0,22.0,Other,Loops,"File, read_excel

# for solution check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Answer Smash Input.xlsx') as xl:
    smash = read_exc",1.0
2021.0,23.0,Other,Loops,"File, read_excel

# for results check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\NPS Input.xlsx') as xl:
    df = concat([rea",1.0
2021.0,24.0,Other,Loops,"File, read_excel

# for solution check only
from pandas import read_csv


start_date = '2021-04-01'
end_date = '2021-05-31'


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Absenteeism Scaffold.xlsx') as xl:
    df = read_excel(",1.0
2021.0,25.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2021 Wk 2",1.0
2021.0,27.0,Other,Loops,"m import choices

# for solution check only
from pandas import read_csv


# setup variables
lottery_count = 4    # the number to pick via lottery


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\inputs\PD 2021 Wk 27 Input.xlsx') as xl:
    prob = read_exce",1.0
2021.0,28.0,Other,Loops,"lties.xlsx') as xl:
    for s in xl.sheet_names:
        df_temp = re",1.0
2021.0,29.0,Other,Loops,"lit'] = [[e.strip() for e in es] for es in events['Events'].str.split(',')]
events = events.explode('Events Split')


# calculate victory/medal events
events['Medal Ceremony?'] = events['Events Split'].str.contains('Gold Medal|Victory Ceremony', 
                                                                flags=IGNORECASE)


# combine the Venue table
events['Venue_lower'] = events['Venue'].str.lower()
venues['Venue_lower'] = venues['Venue'].str.lower()
venues[['Latitude', 'Longitude']] = venues['Location'].str.extract('(.*), (.*)')
final = events.merge(venues[['Venue_lower', 'Latitude', 'Longitude']].drop_duplicates(), 
                     on='Venue_lower')


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

out_cols = ['Latitude', 'Longitude', 'Medal Ceremony?', 'Sport Group', 'Events Split', 
            'UK Date Time', 'Date', 'Sport', 'Venue']

with ExcelWriter(r'.\outputs\output-2021-29.xlsx') as xl:
    final[out_cols].",1.0
2021.0,33.0,Other,Loops,"lta, to_datetime

# for results check only:
from pandas import r",1.0
2021.0,34.0,Other,Loops,"missing_stores = [s for s in targets['Store'].unique() if s not in store_map_stack.keys()]
if len(missing_stores) > 0:
    print('The follo",1.0
2021.0,38.0,Other,Loops,"  - output dataset (for results check only):
      - Trilogies Ou",1.0
2021.0,41.0,Other,Loops,"  - output dataset (for results check only):
      - 2021W41 Outp",1.0
2021.0,42.0,Other,Loops," day of fundraising for each row
df['Total Raised to date'].fillna(method='ffill', inplace=True)
df['Value raised per day'] = (df['Total Raised to date'] / df['Days into fund raising']).round(9)


# workout the weekday for each date
df['Date'] = df['Date'].dt.day_name()


# average the amount raised per day of fundraising for each weekday
df['Avg raised per weekday'] = df.groupby('Date')['Value raised per day'].transform('mean').round(9)


#---------------------------------------------------------------------------------------------------
# formatting to match the solution file
#---------------------------------------------------------------------------------------------------

# this part isn't absolutely necessary; it was just an extra little challenge to match the 
# provided output file, which represents whole numbers without decimals (e.g. 50) and floats using
# smallest number of decimal places necessary (up to 9)

def apply_format(s):
    """"""
    formats ",1.0
2021.0,43.0,Other,Loops,"mport read_csv    # for answer check only


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\inputs\2021W43 Input.xlsx') as xl:
    df_risk = read_e",1.0
2021.0,44.0,Other,Loops,"able, read_excel

# for results check only
from pandas import read_csv

#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r"".\inputs\Carl's 2021 cycling.xlsx"") as xl:
    df = read_excel(",1.0
2021.0,45.0,Other,Loops,"  - output dataset (for results check only):
      - TC Output.cs",1.0
2021.0,46.0,Other,Loops,"erge, read_excel

# for results check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Bookshop.xlsx') as xl:
    # read the dimen",1.0
2021.0,47.0,Other,Loops,"  - output dataset (for results check only):
      - output.csv
""",1.0
2021.0,48.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2021 Wk 4",1.0
2021.0,50.0,Other,Loops,"  - output dataset (for results check only):
      - Sales Depart",1.0
2021.0,51.0,Other,Loops," - output datasets (for results check only):
      - Customer Dim",1.0
2021.0,52.0,Other,Loops,"ile, read_excel


# for results check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\inputs\PD 2021 Wk 52 Input.xlsx') as xl:
    df_comp = read_e",1.0
2022.0,4.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2022 Wk 4",1.0
2022.0,7.0,Other,Loops,"             else x)
                           for s in xl.sheet_names])


with pd.ExcelFile(r'.\inputs\PeopleData.xlsx') as xl:
    # read in the pe",1.0
2022.0,8.0,Other,Loops,"  - output dataset (for results check only):
      - output_prepp",1.0
2022.0,9.0,Other,Loops,"  - output dataset (for results check only):
      - Customer Cla",1.0
2022.0,10.0,Other,Loops,"  - output dataset (for results check only):
      - Bechdel Test",1.0
2022.0,12.0,Other,Loops,"mes over the years. For each EmployerId, find the most recent report
  they submitted and apply this EmployerName across all reports they've submitted
- Create a Pay Gap field to explain the pay gap in plain English
    - You may encounter floating point inaccuracies. Find out more about how to resolve them here
    - In this dataset, a positive DiffMedianHourlyPercent means the women's pay is lower than the 
      men's pay, whilst a negative value indicates the other way around
    - The phrasing should be as follows:
        - In this or",1.0
2022.0,16.0,Other,Loops,"e treated the same, for example
- Fill down the course name for each Guest (hint)
- It may help to bring in the Recipe ID from the Lookup Table 
- Where the Dish contains the Course name, it may be helpful to replace the Recipe ID in the following way:
    Starters = 1
   ",1.0
2022.0,19.0,Other,Loops,"ck  # custom module for comparing output to the solution file


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\PD 2022 Wk 19 Input.xlsx') as xl:
    df_sales = pd.re",1.0
2022.0,20.0,Other,Loops,"ck  # custom module for comparing output to the solution file


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\TC22 Input.xlsx') as xl:
    df_reg = pd.read",1.0
2022.0,21.0,Other,Loops,"  - output dataset (for results check only):
      - 2022W21 Outp",1.0
2022.0,22.0,Other,Loops,"  - output dataset (for results check only):
      - 2022W22 Outp",1.0
2022.0,23.0,Other,Loops,"  - output dataset (for results check only):
      - Cleaned Sale",1.0
2022.0,24.0,Other,Loops,"  - output dataset (for results check only):
      - 2022W24 Outp",1.0
2022.0,27.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2022 Wk 2",1.0
2022.0,28.0,Other,Loops,"ime).strftime('%A') for d in missing]


# get count by day of week
weekday_counts = Counter(weekdays)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

with open(r'.\outputs\output-2022-28.csv', 'w') as f:
    # write the head",1.0
2022.0,29.0,Other,Loops," Type
- Total Sales for each Store and Product Type
- Change the Targets data set into three columns
  - Product
  - Store
  - Sales Target (k's)
- Multiple the Sales Target (k's) by 1000 to create the full sales target number (i.e. 75 becomes 75000)
- Prepare your data sets for joining together by choosing your next step:
  - Easy - make your",1.0
2022.0,30.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2022 Wk 3",1.0
2022.0,31.0,Other,Loops,"  # custom function for checking my output vs. the solution


#---------------------------------------------------------------------------------------------------
# function
#---------------------------------------------------------------------------------------------------

def output_data(df, store_name):
    """"""
    Prep dat",1.0
2022.0,33.0,Other,Loops,"  - output dataset (for results check only):
      - PD 2022 Wk 3",1.0
2022.0,34.0,Other,Loops,"  # custom function for checking my output vs. the solution


#---------------------------------------------------------------------------------------------------
# functions
#---------------------------------------------------------------------------------------------------

def get_user_input(value_name, value_list):
    """"""
    present ",1.0
2022.0,35.0,Other,Loops,"  # custom function for checking my output vs. the solution


#---------------------------------------------------------------------------------------------------
# define functions
#---------------------------------------------------------------------------------------------------

def to_float(in_str):
    """"""
    if in_st",1.0
,,Other,Loops," - output datasets (for results check only):
      - Customer Dim",1.0
,,Other,Loops," - output datasets (for results check only):
      - Customer Dim",1.0
,,Other,Loops,"  - output dataset (for results check only):
      - PD 2022 Wk 4",1.0
2020.0,4.0,Other,Mapping values with dictionary,ountry'].str.lower().replace(split_dict(country_dict)),1.0
2020.0,7.0,Other,Mapping values with dictionary,"f_curr['Salary'].str.replace(r""[\D]"",'')  
df_curr",1.0
2020.0,9.0,Other,Mapping values with dictionary,"ype'] = df['Sample'].replace(sample_map, regex=True)


",1.0
2021.0,1.0,Other,Mapping values with dictionary," 'Road' }
df['Bike'].replace(remap, inplace=True)

# c",1.0
2021.0,29.0,Other,Mapping values with dictionary,"\.', '').str.title().replace(sports_map)
events['Sport ",1.0
2021.0,30.0,Other,Mapping values with dictionary,"'From'] = df['From'].replace(floor_map).astype(int)
df",1.0
2021.0,34.0,Other,Mapping values with dictionary,"] = targets['Store'].replace(store_map_stack)

missing",1.0
2021.0,43.0,Other,Mapping values with dictionary,"g'] = df_a['Rating'].replace(risk_dict)


# bring Bus",1.0
2021.0,48.0,Other,Mapping values with dictionary,"'] * df_melt['unit'].replace(multiplier).fillna(1)

# drop ",1.0
2022.0,2.0,Other,Mapping values with dictionary,'].apply(lambda x: x.replace(year=datetime.now().year,1.0
2022.0,4.0,Other,Mapping values with dictionary,"['Method of Travel'].replace(travel_method_renames)
df[",1.0
2022.0,5.0,Other,Mapping values with dictionary,"ints'] = df['Grade'].replace(grade_map)


# determine ",1.0
2022.0,10.0,Other,Mapping values with dictionary,"ovie'] = df['Movie'].replace(char_dict, regex=True)\
",1.0
2022.0,16.0,Other,Mapping values with dictionary,"        df_x['Dish'].replace(COURSES), NaN )).ffill())\
",1.0
2022.0,17.0,Other,Mapping values with dictionary,"_x: df_x['location'].replace(LOCATION_RENAMES))\
        ",1.0
2022.0,19.0,Other,Mapping values with dictionary,] = df_sales['Size'].replace(dict(zip(df_sizes['Size ,1.0
2022.0,20.0,Other,Mapping values with dictionary,df_reg['Session ID'].replace(dict(zip(df_ses['Session,1.0
2022.0,22.0,Other,Mapping values with dictionary,   df_out['Episode'].replace(dict(zip(df_eps['Episode,1.0
2022.0,30.0,Other,Mapping values with dictionary,"] = df_top3['Store'].replace({k:v for k,v in zip(d",1.0
2022.0,33.0,Other,Mapping values with dictionary," df_sales['Product'].replace({k:v for k,v in zip(d",1.0
,,Other,Mapping values with dictionary,"['Method_of_Travel'].replace(travel_method_renames),
  ",1.0
2022.0,6.0,Other,```decimal``` module,"import decimal as d
from numpy import where
import pandas as pd


# precision of Decimal operations
d.getcontext().prec = 14


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\7 letter words.xlsx') as xl:
    df_words = pd.read_excel(xl, sheet_name='7 letter words')
    df_scores = pd.read_excel(xl, sheet_name='Scrabble Scores')
    
    
#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split the point and tile info, split tiles into rows, split tile letter and frequency
df_scores = df_scores['Scrabble'].str.extract('(?P<Points>\d+) points?:\s+(?P<Tile>.*)')\
            .assign(Tile=lambda df_x: df_x['Tile'].str.split(','))\
            .explode('Tile')\
            .assign(Frequency=lambda df_x: df_x['Tile'].str.extract(r'.*(\d+)').astype(int),
                    Tile=lambda df_x: df_x['Tile'].str.lower().str.extract(r'\s*(.*?)\s+.*'),
                    Points=lambda df_x: df_x['Points'].astype(int))


# calculate the % Chance of drawing a particular tile and round to 2 decimal places
df_scores['Tile Chance'] = (df_scores['Frequency'] / df_scores['Frequency'].sum()).round(2).apply(d.Decimal)


# count of letters in each word
df_letters = df_words.assign(Tile=lambda df_x: df_x['7 letter word'].str.lower().str.findall('(.)'))\
                     .explode('Tile')\
                     .groupby(['7 letter word', 'Tile'], as_index=False).agg(Count=('Tile', 'size'))


# join the words and letters, calculate the % chance
df_letters = df_letters.merge(df_scores, on='Tile')\
                       .assign(Total_Chance=lambda df_x: where(df_x['Count'] > df_x['Frequency'], 0,
                                                               df_x['Tile Chance'] ** df_x['Count']),
                               Total_Points=lambda df_x: where(df_x['Count'] > df_x['Frequency'], 0,
                                                               df_x['Points'] * df_x['Count']))


# group by word, filtering out impossible words
df_out = df_letters.groupby('7 letter word')\
                   .filter(lambda df_x: df_x['Total_Chance'].min() > 0.0)\
                   .groupby('7 letter word', as_index=False)\
                   .agg(Total_Chance=('Total_Chance', 'prod'),
                        Total_Points=('Total_Points', 'sum'))\
                   .rename(columns={'Total_Chance' : '% Chance', 'Total_Points' : 'Total Points'})

df_out['Points Rank'] = df_out['Total Points'].rank(method='dense', ascending=False).astype(int)
df_out['Likelihood Rank'] = df_out['% Chance'].rank(method='dense', ascending=False).astype(int)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_out.to_csv(r'.\outputs\output-2022-06.csv', index=False)",
2020.0,2.0,Other,```zip```,"        for (t,p) in zip(df['time_part'],df['",
2020.0,4.0,Other,```zip```,"= {n : q for n, q in zip(df_q['Number'], df_q",
2021.0,7.0,Other,```zip```,"          for i,k in zip(dfAll['Ingredients/A",
2021.0,10.0,Other,```zip```,"        for n,g,f in zip(df['Name'], df['Evol",
2021.0,11.0,Other,```zip```,"v_dict[c] for c,p in zip(src['Currency'], src",
2021.0,24.0,Other,```zip```,"req='D') for d, p in zip(df['Start Date'], df",
2021.0,37.0,Other,```zip```," p 
              in zip(df['Start Date'], df",
2022.0,15.0,Other,```zip```,"         for s, e in zip(df_x['Contract Start",
2022.0,30.0,Other,```zip```,"lace({k:v for k,v in zip(df_stores['StoreID']",
2022.0,33.0,Other,```zip```,"lace({k:v for k,v in zip(df_lookup['Product I",
2019.0,4.0,Other,f strings / format,"th('HI ')]:
    df[[f'{c} - Player', f'{c} - Value']] =",
2020.0,11.0,Other,f strings / format,"- 1:
            df[f'Boxes of {s}'] = df['Remainder'] ",
2021.0,12.0,Other,f strings / format,"e)
    ax.set_title(f""{Breakdown}"")
    ax.set(xlabel=",
2021.0,14.0,Other,f strings / format,"s = {'A':'Window', 'F':'Window', 'B':'Middle', 'E':'Middle', 'C':'Aisle', 'D':'Aisle'}
seats['Seat Position'] = seats['Seat Position'].map(seat_types)

# parse the flight details
flights = flight_list.iloc[:, 0].str.replace('[\[\]]', '').str.split('|', expand=True)
flights.columns = flight_list.columns[0].replace('[', '').replace(']', '').split('|')
flights['FlightID'] = flights['FlightID'].astype(int)

# calculate the time of day for each flight
flights['Depart Time of Day'] = where(flights['DepTime'] < '12:00:00', 'Morning',
                                  where(flights['DepTime'] <= '18:00:00', 'Afternoon', 'Evening'))

# parse the row types
planes[['min_bc_row', 'max_bc_row']] = planes['Business Class'].str.split('-', expand=True).astype(int)
planes.drop(columns=['Business Class'], inplace=True)

# combine the seat list and passenger list tables
combined = passengers.merge(seats, on='passenger_number', how='left')\
            .merge(flights.merge(planes, left_on='FlightID', right_on='FlightNo.', how='left'),
                   left_on='flight_number', right_on='FlightID', how='left')


# identify the row type (Business Class or Economy)
combined['Business Class'] = where((combined['Row'] >= combined['min_bc_row'])
                                   & (combined['Row'] <= combined['max_bc_row']),
                                   'Business Class', 'Economy')

# checks
len(passengers) == len(combined)    # check for duplication
combined[combined['Business Class'].isna()]        # check for missing row class
combined[combined['Depart Time of Day'].isna()]    # check for missing time of day
combined[combined['Seat Position'].isna()]         # check for missing seat position


# --------------------------------------------------------------------------------------------------
# output the data
# --------------------------------------------------------------------------------------------------

# Q1: what time of day were the most purchases made? (avg spend/flight)
q1 = combined[combined['Business Class'] != 'Business Class']\
              .groupby(['flight_number', 'Depart Time of Day'])['purchase_amount'].sum()\
              .groupby('Depart Time of Day').mean().reset_index()\
              .sort_values(by='purchase_amount', ascending=False)
q1.rename(columns={'purchase_amount' : 'Avg per Flight'}, inplace=True)
q1['Avg per Flight'] = q",
2021.0,36.0,Other,f strings / format,"umns={ 'CY_index' : f'{max_year-1}/{str(max_year)[-2:]} avg index', 
                 ",
2021.0,41.0,Other,f strings / format,aFrame({'Season' : [f'{y}-{(y+1) % 100}' for y in missing_ye,
2021.0,42.0,Other,f strings / format,"    s.map(lambda x: f'{x:.0f}' if x == round(x, 0)",
2021.0,44.0,Other,f strings / format,"no activities
print(f'Days with no activities: {df_p[df_p[""Activities per day""] == 0][""Date""].count()}')


df_p.to_csv(r'.\",
2021.0,46.0,Other,f strings / format," record count
print(f'Number of records after merge: {len(df)}')


#---------------",
2021.0,51.0,Other,f strings / format,"+ 4))
        print(f'{message}\n')
        print('-' ",
2022.0,5.0,Other,f strings / format," 'D' : 4, 'E' : 2, 'F' : 1}


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = pd.read_csv(r'.\inputs\PD 2022 Wk 5 Input.csv')\
       .melt(id_vars='Student ID', var_name='Subject', value_name='Score')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# divide the grades into 6 evenly-distributed groups, add the grade letter and points
df['Grade'] = df.groupby('Subject')['Score']\
                .transform(lambda x: pd.qcut(x, q=6, labels=['F', 'E', 'D', 'C', 'B', 'A']))

df['Points'] = df['Grade'].replace(grade_map)


# determine how many high school application points each Student has received across all subjects
df['Total Points per Student'] = df.groupby('Student ID')['Points'].transform('sum')


# work out the average total points per student by grade 
# [KLG: I assumed this is weighted by number of grades, i.e. if a student had Bs in three subjects,
# then that student counts 3x in the B average. This produced results similar to the solution.]
df['Avg student total points per grade'] = df.groupby('Grade')['Total Points per Student']\
                                             .transform('mean')


# take the average total score you get for students who have received at least one A
# remove anyone who scored less than this. 
# remove results where a student received an A grade
avg_with_a = df[df['Grade']=='A']['Avg student total points per grade'].min()

df = df.loc[(df['Total Points per Student'] >= avg_with_a) & (df['Grade'] != 'A')]


# How many students scored more than the average if you ignore their As?
df['Points per Student without As'] = df.groupby('Student ID')['Points'].transform('sum')

print(f""Students > A avg: {df[df['Points per Student without As'] > avg_with_a]['Student ID'].nunique()}"")


#---------------",
2022.0,13.0,Other,f strings / format,"].count()
outcome = f'{filter_rows / len(df.index):.0%} of Customers account for {FILTER_PCT}% of Sales'


#----------------",
2022.0,14.0,Other,f strings / format,"'Total', 'Week.1', 'F', 'F.1']
renames = {'Ser.' : 'Series', 'Wk.' : 'Week', 'Total' : 'Score', 'Week' : 'Points', 
           'Week.1' : 'Original_Rank', 'F.1' : 'F_rank'}

df = pd.read_csv(r"".\inputs\Richard Osm",
2022.0,16.0,Other,f strings / format,f_orders.columns = [f'{df_orders.columns[i-1]}_Selection' if 'Unnamed' in c e,
2022.0,21.0,Other,f strings / format,"             .query(f""Breakdown == {str(METRIC_LIST)}"")
         )    


#",
2022.0,27.0,Other,f strings / format,")
          .to_csv(f'.\\outputs\\output-2022-27-{t}.csv', index=False)
    )",
2022.0,28.0,Other,f strings / format,"e data
    [f.write(f'{k},{v}\n') for k,v in weekday",
2022.0,31.0,Other,f strings / format," 
    df_out.to_csv(f'.\\outputs\\output-2022-31-{store_name}.csv', index=False,
     ",
2022.0,34.0,Other,f strings / format,"ns_str = '\n'.join([f'  {i+1} - {c}' for i,c in enumerat",
2022.0,35.0,Other,f strings / format,"<= 0:
        print(f'*** ERROR: {input_kph} is not a vailid number. Please enter a number > 0.')
    else:
        ",
,,Other,f strings / format,"nplace=True)
    df[f""{c.replace(' Name', '')}ID""] = df[c].factorize(",
2020.0,11.0,Other,"list methods (```sort```, ```reverse```, ```append```, ```union```, etc.)","from 1 each in each order.
- The box ID should be ascending from the box with the most soap to the box with the least.
- Output the above as well.

Author: Kelly Gilbert
Created: 2022-01-21
Requirements:
  - input dataset:
      - PD 2020 Week 11 Input.xlsx
  - output dataset (for results check only):
      - PD 2020 Wk 11 Output - Soaps per Box.csv
""""""

import numpy as np
import pandas as pd


#---------------------------------------------------------------------------------------------------
# functions
#---------------------------------------------------------------------------------------------------

def get_box_sizes(order_sizes, size_list):
    """"""
    Returns a dataframe with one column per box size, containing the number of boxes of that size
    
    order_sizes: a series containing the order sizes
    size_list: a list of box sizes
    
    returns: a dataframe with columns for each box size
    """"""    

    df = pd.DataFrame({'Order Size' : order_sizes})\
           .assign(Remainder=lambda df_x: df_x['Order Size'])
    size_list.sort(reverse=True)
    
    for i, s in enumerate(size_list):
        if i < len(size_list) - 1:
            df[f'Boxes of {s}'] = df['Remainder'] // s
            df['Remainder'] = df['Remainder'] % s
        else:
            df[f'Boxes of {s}'] = (np.ceil(df['Remainder'] / s)).astype(int)
            
    return df.drop(columns=['Order Size', 'Remainder'])


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\PD 2020 Week 11 Input.xlsx') as xl:
    df_orders = pd.read_excel(xl, sheet_name='Orders') 
    df_sizes = pd.read_excel(xl, sheet_name='Box Sizes')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# for each order, figure out how many boxes of each size will be required.
df_boxes = pd.concat([df_orders, 
                      get_box_sizes(df_orders['Order Size'], list(df_sizes['Box Size']))],
                      axis=1)

# create one row per box
df_soaps = df_boxes.melt(id_vars=['Order Number', 'Order Size'], var_name='Box_Size', 
                         value_name='Last Box Per Box Size')\
                   .assign(Box_Size=lambda df_x: df_x['Box_Size'].str.replace('Boxes of ', '').astype(int),
                           Box_Number=lambda df_x: [range(1, c+1) for c in df_x['Last Box Per Box Size']])\
                   .explode('Box_Number')\
                   .dropna(subset=['Box_Number'])\
                   .rename(columns=lambda c: c.replace('_', ' '))

# add the number of soaps per box
df_soaps['Soaps in Box'] = np.where((df_soaps['Box Size']==df_sizes['Box Size'].min()) 
                                     & (df_soaps['Box Number']==df_soaps['Last Box Per Box Size']),
                                    (df_soaps['Order Size'] - 1) % df_sizes['Box Size'].min() + 1,
                                    df_soaps['Box Size'])

# renumber the boxes 1 to total number of boxes
df_soaps['Box Number'] = df_soaps.groupby('Order Number')['Box Number'].transform('cumcount') + 1


#---------------------------------------------------------------------------------------------------
# output the files
#---------------------------------------------------------------------------------------------------

df_boxes.to_csv(r'.\outputs\output-2020-11-boxes-per-order.csv', index=False)
df_soaps.to_csv(r'.\outputs\output-2020-11-soaps-per-box.csv', index=False)",
2021.0,10.0,Other,"list methods (```sort```, ```reverse```, ```append```, ```union```, etc.)","from both the evolved from and evolved to fields
- Bring in information about what a Pokmon evolves from
- Ensure that we have all 386 of our Pokmon, with nulls if they don't have a pre-evolved form or 
  if they don't evolve
- Finally, for Pokmon that have 3 evolutions, we want to know what the First Evolution is in their 
  Evolution Group
- Some duplication may have occurred with all our joins, ensure no 2 rows are exactly the same
- Create a calculation for our Evolution Group
- The Evolution Group will be named after the First Evolution e.g. in the above example, Bulbasaur 
  is the name of the Evolution Group
- Output the data

Author: Kelly Gilbert
Created: 2021-03-19
Requirements:
  - input dataset:
      - Pokemon Input.xlsx
  - output dataset (for results check):
      - Pokemon Output.csv

""""""

from pandas import ExcelFile, read_excel
from numpy import nan

# used for answer check only
from pandas import read_csv


def get_evolution_group(p_name):
    """"""
    given a Pokemon name, returns the first Pokemon in the evolution hierarchy
    
    e_dict = a dictionary of evolution steps (keys: evolving to, values: evolving from)
    p_name = the Pokemon name to look up
    """"""
    if p_name not in evolution_dict.keys() or p_name == evolution_dict[p_name]: 
        return p_name
    else:
        return get_evolution_group(evolution_dict[p_name])
    

#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Pokemon Input.xlsx') as xl:
    pokemon = read_excel(xl, 'Pokemon')
    evolution = read_excel(xl, 'Evolution')


#---------------------------------------------------------------------------------------------------
# filter the pokemon and evolution lists
#---------------------------------------------------------------------------------------------------

# keep up to Generation III (up to and including #386) and not Mega evolutions
pokemon = pokemon[(pokemon['#'].astype(float) <= 386) & (pokemon['Name'].str.slice(0,5) != 'Mega ')]

# remove multiple rows for different types 
pokemon.drop(columns=['Type'], inplace=True)
pokemon.drop_duplicates(inplace=True)

# remove non-gen III from the evolution dataset
valid_names = list(pokemon['Name'])
evolution = evolution[(evolution['Evolving from'].isin(valid_names)) &
                      (evolution['Evolving to'].isin(valid_names))]


#---------------------------------------------------------------------------------------------------
# get evolution info
#---------------------------------------------------------------------------------------------------

evolution_dict = dict(zip(evolution['Evolving to'], evolution['Evolving from']))

# bring in information about what our Pokmon evolve TO (keep nulls)
df = pokemon.merge(evolution, left_on='Name', right_on='Evolving from', how='left')

# bring in information about what a Pokmon evolves FROM (keep nulls)
df['Evolving from'] = [evolution_dict[k] if k in evolution_dict.keys() else nan for k in df['Name']]

# get the first Pokemon in the evolution hierarchy
df['Evolution Group'] = df['Name'].apply(get_evolution_group)

# if it is a 3rd+ evolution, add the first evolution
df['First Evolution'] = [nan if (n==g) | (g==f) else g 
                         for n,g,f in zip(df['Name'], df['Evolution Group'], df['Evolving from'])]

# ensure we still have all of the Pokemon
if not pokemon['Name'].unique().sort() == df['Name'].unique().sort():
    print('The list of Pokemon does not match after joins')
    raise SystemExit
    
# ensure all rows are unique
df.drop_duplicates(inplace=True)


# --------------------------------------------------------------------------------------------------
# output the data
# --------------------------------------------------------------------------------------------------

output_cols = ['Evolution Group'] + list(pokemon.columns) + list(evolution.columns) + \
              ['First Evolution']
df.to_csv('.\\outputs\\output-2021-10.csv', index=False, columns=output_cols)",
2021.0,31.0,Other,"list methods (```sort```, ```reverse```, ```append```, ```union```, etc.)","from pandas import pivot_table, read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = read_csv(r'.\\inputs\\PD 2021 Wk 31 Input.csv', parse_dates=['Date'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# remove the 'Return to Manufacturer' records
df = df.loc[df['Status'] != 'Return to Manufacturer']

# pivot the data by store and item
total_name = 'Items sold per store'
pivot = pivot_table(df, values='Number of Items', index='Store', columns='Item', aggfunc='sum', 
                    fill_value=None, margins=True, margins_name=total_name)\
                   .reset_index()


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

cols = list(pivot.columns)
cols.reverse()

pivot.iloc[0:len(pivot)-1].to_csv(r'.\outputs\output-2021-31.csv', index=False, columns=cols)",
2019.0,4.0,Other,list/dict comprehension,"Sheet1', converters={'W-L' : str})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# fix date: year for Oct-Dec dates should be 2018 instead of 2019
df['True Date'] = where(df['DATE'].dt.month > 6, df['DATE'] - DateOffset(years=1), df['DATE'])


# split the ""Hi-"" categories up so player and value is separate
for c in [c for c in df.column",
2020.0,1.0,Other,list/dict comprehension,"nulls with zeroes
df['Profit'].fillna(0, inplace=True)

# extract the hierarchy from the Item and copy it
df['Hierarchy'] = df['Item'].str.extract('([\d\.]+?)\.? .*')
df['Hierarchy2'] = df['Hierarchy']
df['Level'] = df['Hierarchy'].str.count('\.')
maxLevel = df['Level'].max()

# iterate through the hierarchy levels, creating subtotals at each level
dfSubtotal = None
for i in range(maxLevel, 0, -1):
    # remove a layer of hierarchy
    df['Hierarchy2'] = df['Hierarchy2'].str.extract('(.*?)\.\d+$')
   
    # using only the detail records, sum by current level of hierarchy
    # and add to the Subtotal df
    dfSubtotal = pd.concat([dfSubtotal, 
      df[df['Level']==maxLevel].groupby(df['Hierarchy2'], as_index=True)['Profit'].sum()])
  
# join subtotals back to the main dataframe and update the Profit
df = pd.merge(df, dfSubtotal, how='left', left_on='Hierarchy', right_index=True)
df['Profit'] = df['Profit_x'] + df['Profit_y'].fillna(0)

# add the spacing to the Item
df['Item'] = [' '* 5*x for x in df['Level']] + df['Item",
2020.0,2.0,Other,list/dict comprehension,"om d/m/y to m/d/y
df['Date_out'] = [dt.strftime(dt.strptime(d, '%d/%m/%y'), '%m/%d/%Y') for d in df['Date']]


# clean t",
2020.0,3.0,Other,list/dict comprehension,"am.rename(columns = {'Divison':'Division'}, inplace = True)


# concatenate the results sheets
result_sheets = [s for s in xl.sheet_names if 'Results' in s]
df = None
for sheet in result_sheets:
    df = concat([df, xl.parse(sheet)]",
2020.0,4.0,Other,list/dict comprehension,"    
    new_dict = { i:k for k, v in in_dict.items() for i in v}    
    new_dict.update({ k.lower() : k.title",
2020.0,6.0,Other,list/dict comprehension,"e year/week
df_rates['Rate'] =    \
    df_rates['British Pound to US Dollar'].str.extract('\= ([\d\.]+)', expand=False).astype(float)
df_rates['Week'] = [int(d.strftime('%U')) + 1 for d in df_rates['Date']]
df_rates['Y",
2020.0,7.0,Other,list/dict comprehension," leave month
df_curr['Salary'] = df_curr['Salary'].str.replace(r""[\D]"",'')  
df_curr['Salary'] = df_curr['Salary'].astype(int)
df_curr['Leave Date'] = '3/1/2020'


# concatenate (union) the current and left employees
df_all = concat([df_curr, df_left], sort=False)


# convert the join/leave dates to the first of the month for joining
df_all['Join Month'] = [parse(d) for d in df_all['Join Date'].str.rep",
2020.0,8.0,Other,list/dict comprehension," week
    renames = {'Volume' : 'Sales Volume', 'Value' : 'Sales Value'}
    df_weekly = pd.concat([pd.read_excel(xl, s)\
                             .assign(Week=int(s.replace('Week ', '')))\
                             .rename(columns=renames)
                           for s in xl.sheet_names if 'Week' in s])\
                  .assign(Type=lambda df_x: df_x['Type'].str.lower())",
2020.0,9.0,Other,list/dict comprehension,"      .melt(id_vars=['Poll', 'Date', 'Sample'], var_name='Candidate', value_name='Poll Results')\
       .dropna(subset=['Poll Results'])\
       .query(""~Poll.str.contains('Average')"", engine='python')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean up end date
df['End Date'] = pd.to_datetime(df['Date'].str.extract('.*- (\d+/\d+)', expand=False) + '/2020')
df['End Date'] = where(df['End Date'].dt.month >= 7, 
                       df['End Date'] + pd.DateOffset(years=-1), 
                       df['End Date'])


# form a Rank (modified competition) of the candidates per Poll based on their results
df['Rank'] = df.groupby(['Poll', 'End Date', 'Sample'])['Poll Results'].rank(method='max', ascending=False)\
               .astype(int)
      
        
# difference in poll results between first and second
df['Spread from 1st to 2nd Place'] = \
    df.groupby(['Poll', 'End Date', 'Sample'], as_index=False)['Poll Results']\
      .transform(lambda x: x.max() - x.nlargest(2).min())
               

# rename sample types
sample_map = {'.*RV' : 'Registered Voter', '.*LV' : 'Likely Voter', nan : 'Unknown'}
df['Sample Type'] = df['Sample'].replace(sample_map, regex=True)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

out_cols = ['Candidate', 'Poll Results', 'Spread from 1st to 2nd Place', 'Rank', 'End Date', 
            'Sample Type', 'Poll']
df.to_csv(r'.\outputs\output-2020-09.csv', index=False, columns=out_cols, date_format='%d/%m/%Y')


#---------------------------------------------------------------------------------------------------
# create chart
#---------------------------------------------------------------------------------------------------

from bokeh.io import output_file
from bokeh.layouts import row
from bokeh.models import CustomJS, Legend, DatetimeTickFormatter, Select, Title
from bokeh.plotting import figure, show

# color constants
color_selected = '#0066cc'
color_deselected = '#bab0ac'

# set the output file path
output_file('dimensions.html')

# subset of registered voters
df_rv = df.loc[df['Sample Type']=='Registered Voter']\
          .sort_values(by=['Candidate', 'End Date', 'Poll'])

# add a figure and format it
p = figure(width=900, height=500, x_axis_type='datetime')
p.add_layout(Title(text='Data from: realclearpolitics.com; Sample Type: Registered Voter', 
                   text_font_size='9pt'), 'above')
p.add_layout(Title(text='2020 Democratic Presidential Nominations', text_font_size=""24pt""), 'above')
p.xaxis.formatter=DatetimeTickFormatter(days=[""%Y-%m-%d""])
p.y_range.flipped = True
p.add_layout(Legend(), 'right')


# add a line and circles for each candidate option
candidates = sorted(df_rv['Candidate'].unique())

line_dict = {}
circle_dict = {}
for i, c in enumerate(candidates):
    line_dict[c] = p.line(df_rv.lo",
2020.0,11.0,Other,list/dict comprehension,"  df = pd.DataFrame({'Order Size' : order_sizes})\
           .assign(Remainder=lambda df_x: df_x['Order Size'])
    size_list.sort(reverse=True)
    
    for i, s in enumerate(size_list):
        if i < len(size_list) - 1:
            df[f'Boxes of {s}'] = d",
2020.0,12.0,Other,list/dict comprehension,"_tot.rename(columns={'Scent' : 'Scent_orig'}, inplace=True)
df_tot['Scent_join'] = df_tot['Scent_orig'].str.replace(' ', '')


# Percentage of Sales:
# get the year/week
# concatenate the Product ID and Size for joining to the lookup table
df_pct = read_excel(in_file, sheet_name='Percentage of Sales')
#df_pct.dtypes    # make sure numbers read in properly
df_pct = df_pct[df_pct['Percentage o",
2021.0,4.0,Other,list/dict comprehension,"= None
for sheet in [s for s in xl.sheet_names if s != 'Targets']:
    dfNew = xl.parse(sheet)
    dfNew['Store'] = sheet
   ",
2021.0,7.0,Other,list/dict comprehension,"mbers
keywordList = ['E' + k if k.isnumeric() else k 
               for k in dfKeywordsIn.stack().str.split(', ').explode().reset_index(drop=True)]

    
    
#--------------------------------------------------------------------------------
# method 1: list comprehension
#    The challenge specifically said to append the keywords (i.e. a cartesian
#    product, method #2 below). In practice, I would accomplish this using a
#    list comprehension.
#--------------------------------------------------------------------------------

# parse the keywords and numbers, stack the names and numbers, add E to the numbers
dfKeywords = ['E' + k if k.isnumer",
2021.0,8.0,Other,list/dict comprehension,"     customer_id = ""{:.2E}"".format(float(customer_id)) 
    return customer_id


#--------------------------------------------------------------------------------
# input the data
#--------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Copy of Karaoke Dataset.xlsx') as xl:
    choices = read_excel(xl, 'Karaoke Choices').sort_values(by='Date')
    customers = read_excel(xl, 'Customers', converters={'Customer ID':str}).sort_values(by='Entry Time')


#--------------------------------------------------------------------------------
# process the data
#--------------------------------------------------------------------------------

# fix long customer IDs 
customers['Customer ID'] = [convert_id(c) for c in customers['Customer ID']]


# ",
2021.0,10.0,Other,list/dict comprehension,"me == evolution_dict[p_name]: 
        return p_name
    else:
        return get_evolution_group(evolution_dict[p_name])
    

#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\Pokemon Input.xlsx') as xl:
    pokemon = read_excel(xl, 'Pokemon')
    evolution = read_excel(xl, 'Evolution')


#---------------------------------------------------------------------------------------------------
# filter the pokemon and evolution lists
#---------------------------------------------------------------------------------------------------

# keep up to Generation III (up to and including #386) and not Mega evolutions
pokemon = pokemon[(pokemon['#'].astype(float) <= 386) & (pokemon['Name'].str.slice(0,5) != 'Mega ')]

# remove multiple rows for different types 
pokemon.drop(columns=['Type'], inplace=True)
pokemon.drop_duplicates(inplace=True)

# remove non-gen III from the evolution dataset
valid_names = list(pokemon['Name'])
evolution = evolution[(evolution['Evolving from'].isin(valid_names)) &
                      (evolution['Evolving to'].isin(valid_names))]


#---------------------------------------------------------------------------------------------------
# get evolution info
#---------------------------------------------------------------------------------------------------

evolution_dict = dict(zip(evolution['Evolving to'], evolution['Evolving from']))

# bring in information about what our Pokmon evolve TO (keep nulls)
df = pokemon.merge(evolution, left_on='Name', right_on='Evolving from', how='left')

# bring in information about what a Pokmon evolves FROM (keep nulls)
df['Evolving from'] = [",
2021.0,11.0,Other,list/dict comprehension,"
recipes = cocktails['Recipe (ml)'].str.split('; ').explode().str.extract(regex_str, expand=True)

# convert ingredient prices to pounds
src['price_pounds'] = [p / conv_dict[c] for c,p in zip(src['Currency'], src['Pr",
2021.0,12.0,Other,list/dict comprehension,"stack it
key_cols = ['Series-Measure', 'Hierarchy-Breakdown', 'Unit-Detail']

df = read_csv(r'.\inputs\Tourism Input.csv', na_values=['na'])\
         .drop(columns=['id'])\
         .melt(id_vars=key_cols)\
         .rename(columns={'variable':'Month'})

df['Month'] = to_datetime(df['Month'], format='%b-%y')


#---------------------------------------------------------------------------------------------------
# prep the data
#---------------------------------------------------------------------------------------------------

# filter for tourist arrival counts and remove null values
df = df.loc[(df['Series-Measure'].str.contains('Tourist arrivals')) & (df['value'].notna())]
df['value'] = df['value'].astype(int)

# extract the country and continent
df['Country'] = where(df['Hierarchy-Breakdown'].str.match('.*Tourist arrivals / .*'),
                      df['Series-Measure'].str.replace('Tourist arrivals from (the )?', ''), nan)

df['Breakdown'] = where(df['Hierarchy-Breakdown'].str.match('.*Tourist arrivals / .*'),
                        df['Hierarchy-Breakdown'].str.replace('.*Tourist arrivals / ', ''),
                        df['Series-Measure'].str.\
                            replace('Tourist arrivals from |Tourist arrivals - ', ''))

# sum country values by continent
cont_dtl = df[df['Country'].notna()].groupby(['Breakdown', 'Month'])['value'].sum().reset_index()

# join the continent totals to the sums to find the difference
cont_tot = df.loc[df['Country'].isna()]
cont_tot = cont_tot.merge(cont_dtl, on=['Breakdown', 'Month'], suffixes=['', '_CountryTotal'],
                          how='left')
cont_tot['value'] = cont_tot['value'] - cont_tot['value_CountryTotal'].fillna(0)

# remove the continent totals and union the new Unknown-country rows to the main dataframe
cont_tot['Country'] = 'Unknown'
cont_tot.drop(columns=['value_CountryTotal'], inplace=True)
df = concat([df[df['Country'].notna()], cont_tot])


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df.rename(columns={'value':'Number of Tourists'}, inplace=True)
df.to_csv('.\\outputs\\output-2021-12.csv', index=False, date_format='%d/%m/%Y',
          columns=['Breakdown', 'Month', 'Number of Tourists', 'Country'])


#---------------------------------------------------------------------------------------------------
# generate the charts
#---------------------------------------------------------------------------------------------------

charts_per_row = 3
sns.set_style(""white"")
sort=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']

# summarize the data by region and year/month
df_region = df.groupby(['Breakdown', 'Month'])['Number of Tourists'].sum().reset_index()
df_region['Year'] = df_region['Month'].dt.year.astype(int)
df_region['Month Name'] = df_region['Month'].dt.strftime('%b')

# draw a grid of charts, one for each region, where x=month, y=# arrivals, and line per year
g = sns.relplot(kind='line', data=df_region, x='Month Name', y='Number of Tourists', units='Year', 
                hue='Year', col='Breakdown', palette='GnBu', linewidth=1, estimator=None, 
                col_wrap=charts_per_row, height=2.5, aspect=1.5, sort=sort, legend=True,
                facet_kws={'sharex':True, 'sharey':False, 'legend_out':True}).add_legend()

# place the legend
g._legend.set_bbox_to_anchor([1.1, 0])

# add spacing to the grid
g.fig.tight_layout(h_pad=4, w_pad=4)  

# add an orange line for the current year
for Breakdown, ax in g.axes_dict.items():
    # overlay the last year in orange
    sns.lineplot(data=df_region[(df_region['Year']==",
2021.0,13.0,Other,list/dict comprehension,"------

df = concat([read_csv(r'.\inputs\\' + f) for f in listdir(r'.\inputs')])
#df.info(verbose=True)    # investigate the list of fields


# --------------------------------------------------------------------------------------------------
# prep data
# --------------------------------------------------------------------------------------------------

# remove the space after the player name
df['Name'] = df['Name']",
2021.0,14.0,Other,list/dict comprehension,"sengers = passengers[[c for c in passengers.columns if 'Unnamed' not in c]]


# --------------------------------------------------------------------------------------------------
# prep / calculations
# --------------------------------------------------------------------------------------------------

# transpose the seat list
seats = seats_mtx.melt(id_vars='Row', value_vars=seats_mtx.columns[1:])
seats.rename(co",
2021.0,16.0,Other,list/dict comprehension,"----------

big_6 = ['Arsenal', 'Chelsea', 'Liverpool', 'Man Utd', 'Man City', 'Spurs']

df = read_csv(r'.\inputs\PL Fixtures.csv', parse_dates=['Date'], dayfirst=True)\
     .dropna(subset=['Result'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# identify games involving the big 6
# sorted(concat([df['Home Team'], df['Away Team']]).unique())
df['big_6'] = where(df['Home Team'].isin(big_6) | df['Away Team'].isin(big_6), 1, 0)

# split the score into away and home
df[['Home Score', 'Away Score']] = df['Result'].str.split(' - ', expand=True).astype(int)
df['Winner'] = where(df['Away Score'] == df['Home Score'], 'Draw',
                 where(df['Away Score'] > df['Home Score'], 'Away', 'Home'))

# stack the teams
value_vars = ['Home Team', 'Away Team']
df_m = df.melt(id_vars=[c for c in df.columns if c not in value_vars], value_name='Team')


# calculate the goals scored/conceded and total points for ech team
df_m['Goals Scored'] = wh",
2021.0,20.0,Other,list/dict comprehension,"        parse_dates=['Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create the mean and standard deviation for each Week
df_wk = df.groupby('Week').agg(Mean=('Complaints', 'mean'),
                               stdev=('Complaints', 'std')).reset_index()

# duplicate the weekly dataframe for each number of standard deviations
stdev_values = [1, 2, 3]

df_wk['join'] = 1
df_wk = df_wk.merge(DataFrame({'n' : stdev_values, 'join' : [1]*len(stdev_values)}), on='join')

# calculate the control limits for 1, 2, and 3 standard deviations
df_wk['ucl'] = df_wk['Mean'] + df_wk['n'] * df_wk['stdev']
df_wk['lcl'] = df_wk['Mean'] - df_wk['n'] * df_wk['stdev']
df_wk['Variation'] = df_wk['ucl'] - df_wk['lcl']

# join back to the original dataset
df_all = df.merge(df_wk, on='Week')

# assess whether each of the complaint values is within or outside of the control limits
df_all['Outlier?'] = where((df_all['Complaints'] > df_all['ucl'])
                           | (df_all['Complaints'] < df_all['lcl']), 'Outlier', 0)

# keep only outliers
df_outliers = df_all.loc[df_all['Outlier?']=='Outlier']


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_outliers.rename(columns={'ucl':'Upper Control Limit', 'lcl':'Lower Control Limit',
                            'stdev':'Standard Deviation'}, inplace=True)

out_cols = ['Variation', 'Outlier?', 'Lower Control Limit', 'Upper Control Limit',
            'Standard Deviation', 'Mean', 'Date', 'Week', 'Complaints', 'Department']

with ExcelWriter(r'.\outputs\output-2021-20.xlsx') as w:

    # cycle through the number of std deviations, outputting a separate tab for each
    for n in df_outliers['n'].unique():
     ",
2021.0,22.0,Other,list/dict comprehension,"egory data
cat = cat['Category: Answer'].str.strip().str.extract('(?P<Category>.*?)\: (?P<Answer>.*)', expand=True)

# find the name for each answer smash
smash['Name'] = [[n for n in names['Name'] if s.lower()",
2021.0,23.0,Other,list/dict comprehension,"xl:
    df = concat([read_excel(xl, s) for s in xl.sheet_names])   
        

#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# response count by airline
df = df.groupby('Airline').filter(lambda x: len(x) >= 50) 

# classify customer responses to the question in the following way
#     0-6 = Detractors, 7-8 = Passive, 9-10 = Promoters
df['nps_type'] = cut(df",
2021.0,24.0,Other,list/dict comprehension," absences by date
df['Date'] = [date_range(d, periods=p, freq='D') for d, p in zip(df['Start Date'], df['D",
2021.0,25.0,Other,list/dict comprehension,"   exclude = concat([read_excel(xl, s) for s in ['Mega Evolutions', '",
2021.0,26.0,Other,list/dict comprehension,"1.csv', parse_dates=['Date'], dayfirst=True)\
             .sort_values(by='Date')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create 7 rows per original date
df_dates = DataFrame( {'Date' : df['Date'].unique()} )
df_dates['join_date'] = [[d + Timedelta(str(n-3) + ' day') for n in range(7)] 
                         for d in df_dates['Date']]
df_dates = ",
2021.0,27.0,Other,list/dict comprehension," melt(prob, id_vars=['Seed'], var_name='pick', value_name='prob').dropna()
prob['prob'] = prob['prob'].astype(str).str.replace('>', '').astype(float)


# select the first four picks by lottery
lottery = []
#pick = 0
for pick in range(lottery_count):
    
    # subset the probability table and randomly select a seed based on the weights
    prob_sub = prob.loc[(prob['pick']==pick+1) & ~prob['Seed'].isin(lottery)]
    lottery.append(choices(list(prob_sub['Seed']), weights=list(prob_sub['prob']))[0])


# add the remaining teams in seed order and join to the Teams list for final output
output = DataFrame({'Actual Pick' : range(1,len(teams)+1),
                    'Seed' : lottery + [t for t in teams['Seed'].sort_values(",
2021.0,28.0,Other,list/dict comprehension,"  df_temp.columns = [c.lower().strip() for c in df_temp.columns]
        df_temp['sheet'] = s
       ",
2021.0,29.0,Other,list/dict comprehension,"ECASE

sports_map = { '3X3 Basketball' : '3x3 Basketball',
               'Artistic Gymnastic' : 'Artistic Gymnastics',
               'Baseball' : 'Baseball/Softball',
               'Beach Volleybal' : 'Beach Volleyball', 
               'Beach Volley' : 'Beach Volleyball',
               'Cycling Bmx Racing' : 'Cycling BMX Racing',
               'Cycling Bmx Freestyle' : 'Cycling BMX Freestyle',
               'Softball' : 'Baseball/Softball',
               'Softball/Baseball' : 'Baseball/Softball'}

group_map = { '3X3 Basketball' : 'Basketball',
              'Artistic Gymnastic' : 'Gymnastics',
              'Artistic Gymnastics' : 'Gymnastics',
              'Artistic Swimming' : 'Swimming',
              'Baseball/Softball' : 'Baseball',
              'Beach Volleyball' : 'Volleyball',
              'Canoe Slalom' : 'Canoeing',
              'Canoe Sprint' : 'Canoeing',
              'Closing Ceremony' : 'Ceremony',
              'Cycling Bmx Freestyle' : 'Cycling',
              'Cycling Bmx Racing' : 'Cycling',
              'Cycling Mountain Bike' : 'Cycling',
              'Cycling Road' : 'Cycling',
              'Cycling Track' : 'Cycling',
              'Judo' : 'Martial Arts',
              'Karate' : 'Martial Arts',
              'Marathon Swimming' : 'Swimming',
              'Opening Ceremony' : 'Ceremony',
              'Table Tennis' : 'Tennis',
              'Taekwondo' : 'Martial Arts',
              'Trampoline Gymnastics' : 'Gymnastics'
            }


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\inputs\Olympic Events.xlsx') as xl:
    events = read_excel(xl, 'Olympics Events')
    venues = read_excel(xl, 'Venues')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create a correctly formatted DateTime field
events['UK Date Time'] = to_datetime(events['Date'].str.replace('(?<=\d)[a-z]+', '') + ' ' 
                                         + events['Time'].str.replace('xx', '0:00'), 
                                     format='%d_%B_%Y %H:%M')
events['Date'] = events['UK Date Time'].dt.date


# clean the sport name and group similar sports into a Sport Type field
events['Sport'] = events['Sport'].str.replace('\.', '').str.title().replace(sports_map)
events['Sport Group'] = events['Sport'].str.title().replace(group_map)


# parse the event list so each event is on a separate row
events['Events Split'] = [[e.strip() for e in es] for es in events['Events'].str.split(",
2021.0,30.0,Other,list/dict comprehension,"------

floor_map = { 'B' : -1, 'G' : 0}

# create a TripID field based on the time of day; assume all trips took place on 12th July 2021
# if there are multiple records per timestamp, the original record order is maintained
df['trip_dtt'] = to_datetime('2021-07-12 ' + df['Hour'].astype(str) + ':' + df['Minute'].astype(str), 
                            format='%Y-%m-%d %H:%M')
df = df.reset_index().sort_values(by=['trip_dtt', 'index']).rename(columns={'index' : 'trip_id'})


# calculate how many floors the lift has to travel between trips
df['From'] = df['From'].replace(floor_map).astype(int)
df['To'] = df['To'].replace(floor_map).astype(int)
df['floors'] = abs(df['From'].shift(-1) - df['To'])


# calculate which floor the majority of trips begin at - call this the Default Position
default_position = df.groupby('From')['To'].size().idxmax()


# if every trip began from the same floor, how many floors would the lift need to travel to begin 
# each journey?
df['floors_from_dp'] = abs(df['From'].shift(-1) - default_position)


# summarize output
if default_position in floor_map.values():
    default_position = {v:k for k, v in floor_map.items()}[default_position]
  ",
2021.0,34.0,Other,list/dict comprehension,"    
    new_dict = { i:k for k, v in in_dict.items() for i in v}    
    new_dict.update({ k:k for k in in_dic",
2021.0,37.0,Other,list/dict comprehension,"hold the contract
df['Payment Date'] = [date_range(d, periods=p, freq=DateOffset(months=1)) for d, p 
              in zip(df['Start Date'], df['C",
2021.0,38.0,Other,list/dict comprehension,"Films in Series
df_f[['Film Order', 'Total Films in Series']] = df_f['Number in Series'].str.extract('(.*)/(.*)')


# avg, max for each trilogy
# rank the trilogies based on the average rating and use the highest ranking metric to break ties
df_sum = df_f.groupby('Trilogy Grouping').agg(Trilogy_Average=('Rating', 'mean'),
                                              Trilogy_Max=('Rating', 'max'))\
             .sort_values(['Trilogy_Average', 'Trilogy_Max'], ascending=False)\
             .reset_index()
df_sum.columns = [c.replace('_', ' ') for c in df_sum.columns]
df_sum['Trilogy Ranking'] =",
2021.0,41.0,Other,list/dict comprehension,"s+').rename(columns={'P.1' : 'Pts'})
df.columns = [c if c == 'POS' else c.title() for c in df.columns]


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create a Special Circumstances field with the following categories
df['Special Circumstanc",
2021.0,42.0,Other,list/dict comprehension,"        parse_dates=['Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create new rows for any date missing between the first and last date in the data set provided
# calculate how many days of fundraising there has been by the date in each row (1st Jan would be 0)
df = DataFrame({'Date' : date_range(",
2021.0,43.0,Other,list/dict comprehension,"it A ', parse_dates={'Date lodged' : ['Month ', 'Date', 'Year']})
    df_b = read_excel(xl, 'Business Unit B ', skiprows=5, parse_dates=['Date lodged'], dayfirst=True)\
           .rename(columns={'Unit' : 'Business Unit '})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# use the lookup table to update the risk rating for business unit A
risk_dict = dict(zip(df_risk['Risk level'], df_risk['Risk rating']))
df_a['Rating'] = df_a['Rating'].replace(risk_dict)


# bring Business Unit A & B together
df = concat([df_a, df_b])
df.columns = [c.strip() for c in df.columns]


# classify each case in relation to the beginning of the quarter
df['Quarter Status'] = ",
2021.0,45.0,Other,list/dict comprehension,"xl:
    df = concat([read_excel(xl, s).assign(date=s) for s in xl.sheet_names if s != 'Attendees'])\
        .rename(columns={'Attendee IDs' : 'At",
2021.0,46.0,Other,list/dict comprehension," DataFrames
    d = {}
    for s in [s for s in xl.sheet_",
2021.0,47.0,Other,list/dict comprehension,"e_usd with zero
df_e['prize_usd'] = df_e['prize_usd'].fillna(0)


# summarize event metrics for each player and add the player name
df_e['first_place'] = where(df_e['player_place']=='1st', 1, 0)
df_p_tot = df_e.groupby('player_id').agg(wins=('first_place', 'sum'),
                                         number_of_events=('event_date', 'count'),
                                         first_event=('event_date', 'min'),
                                         last_event=('event_date', 'max'),
                                         biggest_win=('prize_usd', 'max'),
                                         countries_visited=('event_country', 'nunique'))\
               .reset_index()\
               .merge(df_p[['player_id', 'name', 'all_time_money_usd']], on='player_id', how='left')\
               .rename(columns={'all_time_money_usd' : 'total_prize_money'})


# calculate win % and career length
df_p_tot['percent_won'] = df_p_tot['wins'] / df_p_tot['number_of_events']
df_p_tot['career_length'] = ((df_p_tot['last_event'] - df_p_tot['first_event']).dt.days) / 365.25


# pivot the metrics into rows
metrics = ['number_of_events', 'total_prize_money', 'biggest_win', 'percent_won', 
           'countries_visited', 'career_length']
df_out = df_p_tot.melt(id_vars='name', value_vars=metrics, var_name='metric', value_name='raw_value')
df_out['scaled_value'] = df_out.groupby('metric')['raw_value'].rank(method='average', ascending=True)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_out.to_csv(r'.\outputs\output-2021-47.csv', index=False)


#---------------------------------------------------------------------------------------------------
# radial bar chart
#---------------------------------------------------------------------------------------------------

# NOTE: this is not a good chart choice for this use case. I've recreated the chart here for my
# own practice (to see if I could figure it out!) but would not use this in practice.
#
#   The questions a user might be asking of this data are:
#   1 - Who are the top players across all metrics?
#   2 - Who are the top players for each individual metric (e.g. who had the highest win %?)
#   3 - I'm interested in player X. How did she perform across different metrics?
#
#   All of these questions are difficult to answer. It's possibly easiest to answer #3, but it's 
#   hard to tell the scale (is her red pie slice at 50 or 100?) This could be corrected by adding a 
#   100% circle around each chart.
#   A user could potentially answer #2 by scanning for larger areas, but it's very difficult to 
#   compare across players
#
#   Also, since the metrics are scaled by rank, there isn't a sense of scale across players (e.g. 
#   Is there a big gap between first and 2nd place?)
# 
#   Some better choices:
#   A bump chart would help us answer all three questions (keeping the scaled rank), although
#     interactivity may be needed to help a user find a specific player for #3.
#   A parallel coordinates chart would help answer 1 and 2 (adding interactivity where a user could
#     highlight a specific player would cover #3).
#   Bar charts by metric would help us answer #2, and potentially #1 if the top players in each
#     metric are similar. We could also eliminate the rank scale to show meaningful differences.
#     Highlight interaction would help answer #3.   



# I used these examples while creating the chart output:
# Circular barplot: https://www.python-graph-gallery.com/circular-barplot-basic
# Small multiples: https://jonathansoma.com/lede/data-studio/classes/small-multiples/long-explanation-of-using-plt-subplots-to-create-small-multiples/


# dimensions of the chart grid
player_count = df_out['name'].nunique()
me",
2021.0,48.0,Other,list/dict comprehension,"t = df.melt(id_vars=['Unnamed: 1'],
                  value_vars=[c for c in df.columns if c != 'Unnamed: 1'],
                  value_name='True Value')\
            .dropna(subset=['True Value'])\
    ",
2021.0,50.0,Other,list/dict comprehension,"xl:
    df = concat([read_excel(xl, s).assign(sheet=s) for s in xl.sheet_names])\
         .rename(columns={'Unnamed: 7' : 'YTD ",
2021.0,51.0,Other,list/dict comprehension,"4))
        print(f'{message}\n')
        print('-' * (len(message) + 4))
        print(df_in)


def sort_ignorecase(x):
    if x.dtype == 'O':
        return x.str.lower()
    else:
        return x


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = read_csv(r'.\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expand=True)

print_errors(df[(df['Store'].isna()) | (df['OrderID'].isna())][['OrderID']], 
             'The following OrderIDs could not be parsed:')

    
# turn the Return State field into a binary Returned field
df['Returned'] = where(df['Return State'].notna(), 1, 0)


# create a Sales field
df['Unit Price'] = df['Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df['Sales'] = df['Unit Price'] * df['Quantity']

print_errors(df[df['Sales'].isna()][['OrderID', 'Product Name', 'Unit Price', 'Quantity']], 
                'Sales could not be calculated:')


# create the Store dimension table
df_store = df.groupby('Store')['Order Date'].min().reset_index()\
             .sort_values(by=['Order Date', 'Store'], key=sort_ignorecase)\
             .rename(columns={'Order Date' : 'First Order'})
df_store['StoreID'] = range(1, len(df_store) + 1)


# create the Customer dimension table
df_cust = df.groupby('Customer').agg(Returned=('Returned', 'sum'),
                                     Order_Lines=('OrderID', 'count'),
                                     Number_of_Orders=('OrderID', 'nunique'),
                                     First_Order=('Order Date', 'min')).reset_index()\
            .sort_values(by=['First_Order', 'Customer'], key=sort_ignorecase)
df_cust.columns = [c.replace('_', ' ') for c in df_cust.columns]
df_cust['Return %'] = (df_cu",
2022.0,1.0,Other,list/dict comprehension,"t.csv', parse_dates=['Date of Birth'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# form the pupil's name correctly for the records in the format Last Name, First Name
df[""Pupil's Name""] = df",
2022.0,2.0,Other,list/dict comprehension,"t.csv', parse_dates=['Date of Birth'],
                 usecols=['id', 'pupil first name', 'pupil last name', 'Date of Birth'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# format the pupil's name in First Name Last Name format (ie Carl Allchin)
df['Pupil Name'] = df['pupil first name'] + ' ' + df['pupil last name']


# create the date for the pupil's birthday in calendar year 2022 (not academic year)
df['This Year\'s Birthd",
2022.0,5.0,Other,list/dict comprehension,"------

grade_map = {'A' : 10, 'B' : 8, 'C' : 6, 'D' : 4, 'E' : 2, 'F' : 1}


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = pd.read_csv(r'.\inputs\PD 2022 Wk 5 Input.csv')\
       .melt(id_vars='Student ID', var_name='Subject', value_name='Score')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# divide the grades into 6 evenly-distributed groups, add the grade letter and points
df['Grade'] = df.groupby('Subject')['Score']\
                .transform(lambda x: pd.qcut(x, q=6, labels=['F', 'E', 'D', 'C', 'B', 'A']))

df['Points'] = df['Grade'].replace(grade_map)


# determine how many high school application points each Student has received across all subjects
df['Total Points per Student'] = df.groupby('Student ID')['Points'].transform('sum')


# work out the average total points per student by grade 
# [KLG: I assumed this is weighted by number of grades, i.e. if a student had Bs in three subjects,
# then that student counts 3x in the B average. This produced results similar to the solution.]
df['Avg student total points per grade'] = df.groupby('Grade')['Total Points per Student']\
                                             .transform('mean')


# take the average total score you get for students who have received at least one A
# remove anyone who scored less than this. 
# remove results where a student received an A grade
avg_with_a = df[df['Grade']=='A']['Avg student total points per grade'].min()

df = df.loc[(df['Total Points per Student'] >= avg_with_a) & (df['Grade'] != 'A')]


# How many students scored more than the average if you ignore their As?
df['Points per Student without As'] = df.groupby('Student ID')['Points'].transform('sum')

print(f""Students > A avg: {df[df['Points per Student without As'] > avg_with_a]['Student ID'].nunique()}"")


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df.to_csv(r'.\outputs\output-2022-05.csv', index=False, 
          columns=['Avg student total points per grade', 'Total Points per Student', 'Grade', 
                   'Points', 'Subject', 'Score', 'Student ID'])


#---------------------------------------------------------------------------------------------------
# alternative grade cut
#---------------------------------------------------------------------------------------------------

# the method I used above gives the same grade to all students who received a score, so the number
#   of students with each grade may not be exactly equal
# the provided solution cuts strictly into evenly-sized groups, so sometimes the breakpoint can 
#   occur with in a score (i.e. students with the same score could receive different letter grades)
# the method below will allow breaks within a score, like the solution.

df['Grade'] = df.groupb",
2022.0,7.0,Other,list/dict comprehension,"_metric = pd.concat([pd.read_excel(xl, s)\
                               .assign(Month_Start_Date=pd.to_datetime(s + '1, 2021'))\
                               .rename(columns=lambda x: 'Calls ' + x 
                                                         if x in ['Offered', 'Not Answered', 'Answered']
                                                         else x)
                           for s in xl.sheet_names])


with pd.ExcelFile(r'.\inputs\PeopleData.xlsx') as xl:
    # read in the people data (join People, Leader, and Location tabs)
    df_people = pd.read_excel(xl, 'People')\
                  .merge(pd.read_excel(xl, 'Leaders'), left_on='Leader 1', right_on='id', 
                         how='left', suffixes=['', ' L'])\
        ",
2022.0,12.0,Other,list/dict comprehension,"--------

usecols = ['EmployerName', 'EmployerId', 'EmployerSize', 'DiffMedianHourlyPercent', 'DateSubmitted']

df = pd.concat([pd.read_csv(path.join(IN_DIR, f), encoding='utf-8', usecols=usecols)\
                  .assign(Report=f[-16:-4],
                          Year=int(f[-16:-12])) 
                for f in listdir(IN_DIR)], ignore_index=True)
    

#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# fill in missing submission dates
df['DateSubmitted'] = w",
2022.0,13.0,Other,list/dict comprehension,"= df_orders.groupby(['Customer ID', 'First Name', 'Surname'], as_index=False)['Sales'].sum()\
              .sort_values(by='Sales', ascending=False)\
              .reset_index(drop=True)\
              .assign(Pct_of_Total=lambda df_x: round(df_x['Sales'] / df_x['Sales'].sum() * 100, 9),
                      Total_Customers=lambda df_x: len(df_x.index))\
              .assign(Running_Pct_Total_Sales=lambda df_x: df_x['Pct_of_Total'].cumsum().round(2))    


# describe the result in plain English
filter_rows = df[df['Running_Pct_Total_Sales'] <= FILTER_PCT]['Customer ID'].count()
outcome = f'{filter_rows / len(df.index):.0%} of Customers account for {FILTER_PCT}% of Sales'


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

out_cols = ['Total Customers', 'Running % Total Sales', '% of Total', 'Customer ID', 'First Name', 
            'Surname', 'Sales']

# write out the table
df.iloc[0:filter_rows].rename(columns=lambda x: x.replace('_', ' ').replace('Pct', '%'))\
  .to_csv(f'.\\outputs\\output-2022-13-Pareto Output {FILTER_PCT}%.csv', index=False, 
          columns=out_cols, encoding='utf-8')


# write out the words
with open(f'.\\outputs\\output-2022-13-Pareto in words {FILTER_PCT}%.csv', m",
2022.0,15.0,Other,list/dict comprehension,"--------------

df_c['Current Date'] = CURRENT_DATE

# work out the length of each contract in months 
df_c['Contract Length'] = df_c['Contract End'].dt.to_period('M').view('int64') \
                          - df_c['Contract Start'].dt.to_period('M').view('int64')


# work out the number of months until each contract expires (imagine today is 13th April 2022)
df_c['Months Until Expiry'] = df_c['Contract End'].dt.to_period('M').view('int64') \
                              - df_c['Current Date'].dt.to_period('M').view('int64') 


# join to the pricing table and create one row per month begin date                              
df = df_c.merge(df_p, on=['City', 'Office Size'], how='left')\
         .assign(Month_Divider=\
                 lambda df_x: [pd.date_range(s, e, freq=pd.DateOffset(months=1)).union([e]) 
                               for s, e in zip(df_x['Contract Start'], d",
2022.0,16.0,Other,list/dict comprehension,"s as pd


COURSES = {'Main' : 'Mains', 'Starter' : 'Starters', 'Desserts' : 'Dessert'}


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\Menu Input.xlsx') as xl:
    # drop totally blank columns
    df_orders = pd.read_excel(xl, 'Orders')\
                  .dropna(how='all', axis=1)
    
    df_lookup = pd.read_excel(xl, 'Lookup Table')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# rename the columns name_Dish and name_Selection
df_orders.columns = [f'{df_orders.columns[i-1]}_Selection' if 'Unnamed' in c else f'{c}_Dish'
                     for i, c in enumerate(df_orders.columns)]


# melt original columns into rows, split the column names into guest name and dish or selection,
# pivot dish/selection into columns
# fill the course name down
# join to lookup to get the recipe id
df_out = \
    df_orders.reset_index()\
             .melt(id_vars='index')\
             .assign(Guest=lambda df_x: df_x['variable'].str.extr",
2022.0,17.0,Other,list/dict comprehension,"
LOCATION_RENAMES = {'Edinurgh' : 'Edinburgh'}
CONTENT_TYPES = {'Primary' : ['Cardiff', 'Edinburgh', 'London']}


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\2022W17 Input.xlsx') as xl:
    
    # input the streaming data, fix location spelling, sum the duration by session
    df_s = pd.read_excel(xl, sheet_name='Streaming', parse_dates=['t'])\
             .assign(location=lambda df_x: df_x['location'].replace(LOCATION_RENAMES))\
             .rename(columns={'t' : 'timestamp'})\
             .groupby(['userID', 'timestamp', 'location', 'content_type'], 
                      as_index=False, dropna=False)['duration'].sum()
    
    # input the pricing data
    df_p = pd.read_excel(xl, 'Avg Pricing', parse_dates=['Month'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# adjust the content type
df_s['content_type'] = where(df_s['location'].isin(CONTENT_TYPES['Primary']), 'Primary',
                          where(df_s['content_type'] == 'Preserved', df_s['content_type'], 
                                'Secondary'))

# add the pricing month
# for Primary content --> overall minimum month
# for all other cotent --> minimum month by user/location/content type
df_s['Month'] = where(df_s['content_type']=='Primary', 
                      df_s.groupby(['userID'])['timestamp']\
                          .transform('min').dt.tz_localize(None).astype('datetime64[M]'), 
                      df_s.groupby(['userID', 'location', 'content_type'])['timestamp']\
                          .transform('min').dt.tz_localize(None).astype('datetime64[M]'))

    
# join to pricing table and fill in avg price for preserved content
df_out = df_s.merge(df_p.rename(columns={'Content_Type' : 'co",
2022.0,21.0,Other,list/dict comprehension,"eturn
METRIC_LIST = ['% Shipped in 3 days', '% Shipped in 5 days', 
               '% Processed in 3 days', '% Processed in 5 days', 
               '# Received']


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# read in all of the sheets, copy the depts and targets down, remove unnecessary cols/rows
with pd.ExcelFile(r'.\inputs\2022W21 Input.xlsx') as xl:
    
    df = ( pd.concat([pd.read_excel(xl, sheet_name=s, skiprows=3, 
                                    usecols=lambda c: 'FY' not in str(c) and c != 'Comment')
                        .assign(Shop=s) 
                      for s in xl.sheet_names]) 
             .assign(Department=lambda df_x: df_x['Department'].ffill(",
2022.0,24.0,Other,list/dict comprehension,"ights', parse_dates=['First flight'], 
                               dtype={'Scheduled duration' : str})
    df_cities = pd.read_excel(xl, sheet_name='World Cities')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# remove the airport names from the From and To fields
df_flights[['From', 'To']] = df_flights[['From', 'To']].replace('[-/].*', '', regex=True)


# create a Route field which concatenates the From and To fields with a hyphen
df_flights['Route'] = df_flights['From'] + ' - ' + df_flights['To'] 


# split out the Distance field so that we have one field for the Distance in km 
# and one field for the Distance in miles
df_flights[['Distance - km', 'D",
2022.0,27.0,Other,list/dict comprehension,"        parse_dates=['Sale Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data (no split)
#---------------------------------------------------------------------------------------------------

# separate out the Product Name field to form Product Type and Quantity
df[['Product Type', 'Original Quantity', 'Unit']] = df['Product Name'].str.extract('(.+?) - (\d+)(.*)')


# for liquid, ensure every value is in milliliters
df['Quantity'] =  df['O",
2022.0,28.0,Other,list/dict comprehension,"        parse_dates=['Sale Date'], dayfirst=True, usecols=['Sale Date'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# get a list of all possible dates
df_all = pd.DataFrame({ 'date' : pd.date_range(start=df_sales['Sale Date'].min(), 
                                               end=df_sales['Sale Date'].max(), freq='1D') })


# get dates that are in all dates, but not in sales
mask = ~df_all['date'].isin(df_sales['Sale Date'].unique())
df_missing = df_all.loc[mask, ['date']]


# get weekday name
df_missing['Day of the Week'] = df_missing['date'].dt.day_name()


# get counts by day of the week
df_out = ( df_missing['Day of the Week'].value_counts()
                     .reset_index()
                     .rename(columns={ 'Day of the Week' : 'Number of Days',
                                      'index' : 'Day of the Week'})
         )


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_out.to_csv(r'.\outputs\output-2022-28.csv', index=False)





# ---------- non-pandas method ---------------------------------------------------------------------

from collections import Counter
from datetime import datetime
from numpy import datetime64


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df_sales = pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"", 
                       parse_dates=['Sale Date'], dayfirst=True, usecols=['Sale Date'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# days without sales
missing = list(set(pd.date_range(start=df_sales['Sale Date'].min(), 
                                 end=df_sales['Sale Date'].max(), freq='1D').values)
               -
               set(df_sales['Sale Date'].unique())
              )


# get days of the week
weekdays = [datetime64(d, 'D').astype(datetime).strftime('%A') for d in missing]


# get count by day of week
weekday_counts = Counter(weekdays)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

with open(r'.\outputs\output-2022-28.csv', 'w') as f:
    # write the headings
    f.write('Day of the Week,Number of Days\n')  

    # write out the data
    [f.write(f'{k},{v}\n'",
2022.0,30.0,Other,list/dict comprehension,"ge
top3_filepaths = [r'.\inputs\Preppin_ Summer 2022 - Top 3 Sales People per Store (East).csv',
                  r'.\inputs\Preppin_ Summer 2022 - Top 3 Sales People per Store (West).csv']

df_top3 = pd.concat([pd.read_csv(f)
                       .assign(Region=f[f.find('(') + 1 : f.find(')')])
                     for f in top3_filepaths])

# read in the store lookup file
df_stores = pd.read_csv(r'.\inputs\Preppin_ Summer 2022 - Store Lookup.csv')

# read in the sales file, summarize by store
df_sales = ( pd.read_csv(r'.\inputs\Preppin_ Summer 2022 - PD 2022 Wk 27 Input.csv')
               .groupby('Store Name', as_index=False)['Sale Value'].sum()
",
2022.0,31.0,Other,list/dict comprehension,"   df_store = df.loc[(df['Store Name']==store_name) & (df['Product Name'].str.contains('Liquid')),
                      df.columns]
    
    # split the Product Name field into Product Type and Size
    df_store[['Product Type', 'Size']] = df_store['Product Name'].str.extract('(.*) - (.*)')
    
    # rank based on sales and keep the top 10
    df_store['Rank of Product & Scent by Store'] = \
        df_store['Sale Value'].rank(method='first', ascending=False)
    df_out = df_store.loc[df_store['Rank of Product & Scent by Store'] <= 10, df_store.columns]
    
    # round the Sales Values to the nearest 10 value (ie 1913 becomes 1910)
    df_out['Sale Value'] = df_out['Sale Value'].round(-1)

    # output the file    
    df_out.to_csv(f'.\\outputs\\output-2022-31-{store_name}.csv', index=False,
                  columns=['Store Name', 'Rank of Product & Scent by Store', 'Scent Name', 
                           'Size', 'Sale Value'])
    
    print(f'\n*** SUCCESS: the file for {store_name} has been created.\n')


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# input the file and sum sales by product, scent, and store
df = ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"")
         .groupby(['Product Name', 'Scent Name', 'Store Name'], as_index=False)['Sale Value'].sum() 
     )


#---------------------------------------------------------------------------------------------------
# get user input and output the file
#---------------------------------------------------------------------------------------------------

store_list = sorted(df['Store Name'].unique())
store_list_str = '\n  '.join([f'{i+1} - {n}' for i,n in enumerate(store_list)])

while True:
    input_num = input('\nStore list:\n  ' + store_list_str + '\n\n' 
                      + 'Please enter a number (or press Enter to quit): ')
    
    if input_num.isnumeric() and int(input_num) in range(1, len(store_list)+2):
        output_data(df, store_list[int(input_num)-1])
 ",
2022.0,32.0,Other,list/dict comprehension,"months to pay off
df['Monthly Capital'] = df['Monthly Payment'] * df['% of Monthly Repayment going to Capital'] / 100

df['Months Remaining'] = ceil(df['Capital Repayment Remaining'] / df['Monthly Capital'])

df['Last Month'] = [CURRENT_DATE + pd.DateOffset(months=m-1) for m in df['Months Remaining']]",
2022.0,33.0,Other,list/dict comprehension,"sales = ( pd.concat([pd.read_csv(r'.\inputs\PD 2022 Week 33 Input Instore Orders.csv',
                                    parse_dates=['Sales Date'], dayfirst=True)
                          .rename(columns={'Sales Date' : 'Sales Timestamp'}),
                        pd.read_csv(r'.\inputs\PD 2022 Week 33 Input Online Orders.csv', 
                                    parse_dates=['Sales Timestamp'], dayfirst=True)\
                          .assign(Store='Online')])
           )
                                                        
df_lookup = ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - Product Lookup.csv"")
                .assign(Product_Type = lambda df_x: df_x['Product Name'].str.extract('(.*) - .*'))
                .rename(columns=lambda c: c.replace('_', ' '))
            )


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# return the product type for the product ID
df_sales['Product Type'] = df_sales['Product'].replace({k:v for k,v in zip(df_lookup['Product ID'], 
    ",
2022.0,34.0,Other,list/dict comprehension,"ons_str = '\n'.join([f'  {i+1} - {c}' for i,c in enumerate(value_list)])
    
    while True:
        user_input = input(f'\n{value_name.title()} ",
,,Other,list/dict comprehension," week
    renames = {'Volume' : 'Sales Volume', 'Value' : 'Sales Value'}
    df_weekly = pd.concat([pd.read_excel(xl, s)\
                             .assign(Week=int(s.replace('Week ', '')))\
                             .rename(columns=renames)
                           for s in xl.sheet_names if 'Week' in s])\
                  .assign(Type=lambda df_x: df_x['Type'].str.lower())",
,,Other,list/dict comprehension,"t.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expand=True)

    
# turn the Return State field into a binary Returned field
df['Returned'] = where(df['Return State'].notna(), 1, 0)


# create a Sales field
df['Unit Price'] = df['Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df['Sales'] = df['Unit Price'] * df['Quantity']


# add IDs
for c in ['Store', 'Customer', 'Product Name']:
    df.sort_values(by=['Order Date', c], key=sort_ignorecase, inplace=True)
    df[f""{c.replace(' Name', '')}ID""] = df[c].factorize()[0] + 1


# create the Store dimension table
df_store = df.groupby(['StoreID', 'Store'])['Order Date'].min().reset_index()\
             .rename(columns={'Order Date' : 'First Order'})
             

# create the Customer dimension table
df_cust = df.groupby(['CustomerID', 'Customer'])\
            .agg(Returned=('Returned', 'sum'),
                 Order_Lines=('OrderID', 'count'),
                 Number_of_Orders=('OrderID', 'nunique'),
                 First_Order=('Order Date', 'min')).reset_index()
df_cust.columns = [c.replace('_', ' ') for c in df_cust.columns]
df_cust['Return %'] = (df_cu",
,,Other,list/dict comprehension,"t.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expand=True)

    
# turn the Return State field into a binary Returned field
df['Returned'] = where(df['Return State'].notna(), 1, 0)


# create a Sales field
df['Unit Price'] = df['Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df['Sales'] = df['Unit Price'] * df['Quantity']


# create the Store dimension table
df_store = df.groupby('Store')['Order Date'].min().reset_index()\
             .sort_values(by=['Order Date', 'Store'], key=sort_ignorecase)\
             .rename(columns={'Order Date' : 'First Order'})
df_store['StoreID'] = range(1, len(df_store) + 1)


# create the Customer dimension table
df_cust = df.groupby('Customer').agg(Returned=('Returned', 'sum'),
                                     Order_Lines=('OrderID', 'count'),
                                     Number_of_Orders=('OrderID', 'nunique'),
                                     First_Order=('Order Date', 'min')).reset_index()\
            .sort_values(by=['First_Order', 'Customer'], key=sort_ignorecase)
df_cust.columns = [c.replace('_', ' ') for c in df_cust.columns]
df_cust['Return %'] = (df_cu",
,,Other,list/dict comprehension,"   return DataFrame({'Customer' : [''.join(rnd.choices('ABCDEFGHIJKLMNOPQRST', k=2))*10 
                                        for n in range(0, recs)],
                          'Order Date' : rnd.choices(date_range('2000-01-01', '2021-12-31'), k=recs)})

    # low cardinality customer (52 possible choices)
    elif card == 'low':
        return DataFrame({'Customer' : [rnd.ch",
2021.0,12.0,Other,sets,"Breakdown}"")
    ax.set(xlabel=None, ylabel=",
2022.0,28.0,Other,sets,"ales
missing = list(set(pd.date_range(start=",
2020.0,12.0,Other,user input,"'.\inputs\PD week 12 input(1).xlsx')


# Total ",1.0
2022.0,13.0,Other,user input,"------

FILTER_PCT = input('Enter the % of sale",1.0
2022.0,31.0,Other,user input,"rue:
    input_num = input('\nStore list:\n  ' ",1.0
2022.0,34.0,Other,user input,        user_input = input(f'\n{value_name.titl,1.0
2022.0,35.0,Other,user input,"rue:
    input_kph = input('Enter the average p",1.0
2020.0,8.0,Pandas - Aggregation,"Boolean aggregation(```any```, ```all```)","        .dropna(how='all', axis=1)\
         ",1.0
2021.0,26.0,Pandas - Aggregation,"Boolean aggregation(```any```, ```all```)","gg['Destination'] = 'All'


# union the desti",1.0
2021.0,48.0,Pandas - Aggregation,"Boolean aggregation(```any```, ```all```)",".dropna(axis=1, how='all').dropna(axis=0, how",1.0
2022.0,14.0,Pandas - Aggregation,"Boolean aggregation(```any```, ```all```)","'check1'].transform('any')


# compare ranks ",1.0
2022.0,16.0,Pandas - Aggregation,"Boolean aggregation(```any```, ```all```)","        .dropna(how='all', axis=1)
    
    d",1.0
,,Pandas - Aggregation,"Boolean aggregation(```any```, ```all```)","        .dropna(how='all', axis=1)\
         ",1.0
2020.0,3.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)",'Team')['win_flag'].cumsum(skipna=False) # thi,
2020.0,11.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","Number'].transform('cumcount') + 1


#----------",
2021.0,1.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","er'])['Bike Value'].cumsum()


# generate char",
2021.0,8.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","seconds().ge(59*60).cumsum() + 1


# number th",
2021.0,37.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","e')['Monthly Cost'].cumsum()


#--------------",
2021.0,49.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","')['months_worked'].cumsum()

    
# create th",
2021.0,50.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","ar'])['YTD Total2'].cumsum()
df_tot.drop(colum",
2022.0,13.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","f_x['Pct_of_Total'].cumsum().round(2))    


#",
2022.0,15.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)"," Month'].transform('cumsum')
         
       ",
2022.0,32.0,Pandas - Aggregation,"Cumulative aggregation(```cumcount```, ```cumsum```, etc.)","apital'].transform('cumsum') )

df_out['Capita",
2020.0,3.0,Pandas - Aggregation,Named Aggregation,"m'], as_index=False).agg( 
               { '",
2020.0,5.0,Pandas - Aggregation,Named Aggregation,"e'], as_index=False).agg( 
                 {",
2020.0,6.0,Pandas - Aggregation,Named Aggregation,"k'], as_index=False).agg( 
                 {",
2020.0,7.0,Pandas - Aggregation,Named Aggregation,"th', as_index=False).agg( 
        { 'Employe",
2021.0,2.0,Pandas - Aggregation,Named Aggregation,"rand', 'Bike Type']).agg({ 'Quantity' : ['sum",
2021.0,6.0,Pandas - Aggregation,Named Aggregation,= df.groupby('TOUR').agg(Total_Prize_Money = ,
2021.0,7.0,Pandas - Aggregation,Named Aggregation,ption'])['Contains'].agg(Contains=list).reset,
2021.0,17.0,Pandas - Aggregation,Named Aggregation,"df_m.groupby('Name').agg(total_days=('Date', ",
2021.0,18.0,Pandas - Aggregation,Named Aggregation,(id_vars + ['Task']).agg(count=('Scheduled Da,
2021.0,20.0,Pandas - Aggregation,Named Aggregation,"= df.groupby('Week').agg(Mean=('Complaints', ",
2021.0,22.0,Pandas - Aggregation,Named Aggregation,"swer Smash')['Name'].agg(Names=('Name', 'nuni",
2021.0,25.0,Pandas - Aggregation,Named Aggregation,y('Evolution Group').agg(Appearances=('Episod,
2021.0,26.0,Pandas - Aggregation,Named Aggregation,"stination', 'Date']).agg(Rolling_Week_Avg=('R",
2021.0,28.0,Pandas - Aggregation,Named Aggregation,df_m.groupby('Team').agg(Total_Shootouts=('sh,
2021.0,32.0,Pandas - Aggregation,Named Aggregation,"['Flight', 'Class']).agg(avg_gte_7=('sales_gt",
2021.0,34.0,Pandas - Aggregation,Named Aggregation,"Store', 'Employee']).agg(Avg_monthly_Sales=('",
2021.0,36.0,Pandas - Aggregation,Named Aggregation,oupby('Search Term').agg(Avg_index = ('index',
2021.0,38.0,Pandas - Aggregation,Named Aggregation,('Trilogy Grouping').agg(Trilogy_Average=('Ra,
2021.0,46.0,Pandas - Aggregation,Named Aggregation,                    .agg(Number_of_Months_Che,
2021.0,47.0,Pandas - Aggregation,Named Aggregation,"groupby('player_id').agg(wins=('first_place',",
2021.0,49.0,Pandas - Aggregation,Named Aggregation,", 'Reporting Year']).agg(min_date=('Date', 'm",
2021.0,51.0,Pandas - Aggregation,Named Aggregation,.groupby('Customer').agg(Returned=('Returned',
2021.0,52.0,Pandas - Aggregation,Named Aggregation,"s']\
               .agg(lambda x: ', '.join(",
2022.0,3.0,Pandas - Aggregation,Named Aggregation,"dent ID', 'Gender']).agg(Passed_Subjects=('Pa",
2022.0,4.0,Pandas - Aggregation,Named Aggregation,"=False)\
           .agg(Number_of_Trips=('St",
2022.0,6.0,Pandas - Aggregation,Named Aggregation,"e'], as_index=False).agg(Count=('Tile', 'size",
2022.0,9.0,Pandas - Aggregation,Named Aggregation,"e'], as_index=False).agg(First_Purchase=('Yea",
2022.0,19.0,Pandas - Aggregation,Named Aggregation,"_index=False)
      .agg(Sales_with_the_wrong",
2022.0,27.0,Pandas - Aggregation,Named Aggregation,"False)
             .agg(Sale_Value=('Sale Va",
2022.0,35.0,Pandas - Aggregation,Named Aggregation,"])
                 .agg(Total_Mins=('Mins', ",
,,Pandas - Aggregation,Named Aggregation,"mer'])\
            .agg(Returned=('Returned'",
,,Pandas - Aggregation,Named Aggregation,.groupby('Customer').agg(Returned=('Returned',
,,Pandas - Aggregation,Named Aggregation,"e)\
                .agg(First_Order=('Order ",
,,Pandas - Aggregation,Named Aggregation,"ndex=False)\
       .agg(Number_of_Trips=('St",
2021.0,23.0,Pandas - Aggregation,```groupby``` with filter,"t by airline
df = df.groupby('Airline').filter(lambda x: len(x) >= ",
2020.0,3.0,Pandas - Aggregation,```rank```,"upby('Team')['Date'].rank(method='first', asce",1.0
2020.0,5.0,Pandas - Aggregation,```rank```,"'Rank'] = df['Diff'].rank(method='min', ascend",1.0
2020.0,9.0,Pandas - Aggregation,```rank```,"e'])['Poll Results'].rank(method='max', ascend",1.0
2021.0,4.0,Pandas - Aggregation,```rank```,"Variance to Target'].rank(ascending=False)

# ",1.0
2021.0,6.0,Pandas - Aggregation,```rank```,"rank'] = df['MONEY'].rank(method='first', asce",1.0
2021.0,8.0,Pandas - Aggregation,```rank```,"'Session #')['Date'].rank('dense', ascending=T",1.0
2021.0,9.0,Pandas - Aggregation,```rank```,item.groupby('Area').rank(ascending=False)[['R,1.0
2021.0,13.0,Pandas - Aggregation,```rank```,"                    .rank(method='min', ascend",1.0
2021.0,14.0,Pandas - Aggregation,```rank```,q1['Avg per Flight'].rank(ascending=False).ast,1.0
2021.0,16.0,Pandas - Aggregation,```rank```,"                    .rank(method='min', ascend",1.0
2021.0,21.0,Pandas - Aggregation,```rank```,        ['Variance'].rank(ascending=False).ast,1.0
2021.0,25.0,Pandas - Aggregation,```rank```, = df['Appearances'].rank(method='min').astype,1.0
2021.0,28.0,Pandas - Aggregation,```rank```,"t1['Shootout Win %'].rank(method='dense', asce",1.0
2021.0,47.0,Pandas - Aggregation,```rank```,"etric')['raw_value'].rank(method='average', as",1.0
2022.0,5.0,Pandas - Aggregation,```rank```,"('Subject')['Score'].rank(method='first')\
   ",1.0
2022.0,6.0,Pandas - Aggregation,```rank```,"_out['Total Points'].rank(method='dense', asce",1.0
2022.0,14.0,Pandas - Aggregation,```rank```,"oup['Points_no_dpf'].rank('min', ascending=Fal",1.0
2022.0,24.0,Pandas - Aggregation,```rank```,"hts['Distance - km'].rank(method='dense', asce",1.0
2022.0,31.0,Pandas - Aggregation,```rank```,"_store['Sale Value'].rank(method='first', asce",1.0
2022.0,34.0,Pandas - Aggregation,```rank```," Type'])['Calories'].rank(ascending=False, 
  ",1.0
2020.0,4.0,Pandas - Aggregation,```transform```,")['Completion Date'].transform('min'),
            ",
2020.0,9.0,Pandas - Aggregation,```transform```,"ll Results']\
      .transform(lambda x: x.max() - ",
2020.0,11.0,Pandas - Aggregation,```transform```,"mber')['Box Number'].transform('cumcount') + 1


#-",
2021.0,9.0,Pandas - Aggregation,```transform```,"upby('join')['join'].transform('count')

# remove u",
2021.0,17.0,Pandas - Aggregation,```transform```,"pby('Name')['Hours'].transform('sum'))\
           ",
2021.0,21.0,Pandas - Aggregation,```transform```,"('Product')['Price'].transform('mean')

# work out ",
2021.0,33.0,Pandas - Aggregation,```transform```,"')['Reporting Date'].transform('min')
df['last_date",
2021.0,35.0,Pandas - Aggregation,```transform```,"ture'])['Area_diff'].transform('min')]


#---------",
2021.0,42.0,Pandas - Aggregation,```transform```,"lue raised per day'].transform('mean').round(9)


#",
2021.0,49.0,Pandas - Aggregation,```transform```,'Name'])['min_date'].transform('min').dt.strftime(',
2021.0,52.0,Pandas - Aggregation,```transform```,"'Name')['Complaint'].transform('size')


# preproce",
2022.0,2.0,Pandas - Aggregation,```transform```,"On', 'Month'])['id'].transform('size')


#---------",
2022.0,4.0,Pandas - Aggregation,```transform```,")['Number_of_Trips'].transform('sum'),
            ",
2022.0,5.0,Pandas - Aggregation,```transform```,"']\
                .transform(lambda x: pd.qcut(x,",
2022.0,9.0,Pandas - Aggregation,```transform```,"urchase'])['Order?'].transform('sum') \
         - ",
2022.0,11.0,Pandas - Aggregation,```transform```,"ame'])['Attendance'].transform('mean')


#---------",
2022.0,14.0,Pandas - Aggregation,```transform```,"'] = group['check1'].transform('any')


# compare r",
2022.0,15.0,Pandas - Aggregation,```transform```,"')['Rent per Month'].transform('cumsum')
         
",
2022.0,17.0,Pandas - Aggregation,```transform```,                    .transform('min').dt.tz_localiz,
2022.0,20.0,Pandas - Aggregation,```transform```,"('Email')['Session'].transform('count')


# join th",
2022.0,32.0,Pandas - Aggregation,```transform```,")['Monthly Capital'].transform('cumsum') )

df_out[",
,,Pandas - Aggregation,```transform```,")['Number_of_Trips'].transform('sum'),
            ",
2019.0,4.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","] = where(df['DATE'].dt.month > 6, df['DATE'] - D",
2020.0,9.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","where(df['End Date'].dt.month >= 7, 
            ",
2021.0,1.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","arter'] = df['Date'].dt.quarter
df['Day of Month'] ",
2021.0,2.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)"," - df['Order Date']).dt.days
df[['Order Date', '",
2021.0,3.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","arter'] = df['Date'].dt.quarter

# aggregation #1: ",
2021.0,4.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","arter'] = df['Date'].dt.quarter

# sum by store and",
2021.0,8.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)",ices['Date'].diff(1).dt.total_seconds().ge(59*60).cumsum(,
2021.0,12.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","= df_region['Month'].dt.year.astype(int)
df_regi",
2021.0,15.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","ders_f['Order Date'].dt.day_name()
orders_f['Price']",
2021.0,18.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","f_complete['Scope']).dt.days
df_complete['Build ",
2021.0,29.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","ents['UK Date Time'].dt.date


# clean the sport",
2021.0,32.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","ight'] - df['Date']).dt.days

# classify as less",
2021.0,36.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)",ear'] = df_t['Week'].dt.year + where(df_t['Week',
2021.0,42.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","'Date'] = df['Date'].dt.day_name()


# average the a",
2021.0,47.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","_tot['first_event']).dt.days) / 365.25


# pivot",
2021.0,49.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)"," Year'] = df['Date'].dt.year


# summarize by pe",
2021.0,50.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","Year'] = df['Month'].dt.year


# fill in the Sal",
2022.0,1.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)"," df['Date of Birth'].dt.year) 
                 ",
2022.0,2.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","s Year\'s Birthday'].dt.day_name()\
                ",
2022.0,9.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","ear=df['Order Date'].dt.year)


#---------------",
2022.0,15.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)",df_c['Contract End'].dt.to_period('M').view('int64') ,
2022.0,28.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","= df_missing['date'].dt.day_name()


# get counts by",
2022.0,33.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","['Sales Timestamp']).dt.total_seconds() / 60


# summariz",
2022.0,35.0,Pandas - Dates,"Dateparts (```dt.month```, ```dt.quarter```, etc.)","Year'] = -df['Date'].dt.year
    df['Calories'] ",
2019.0,4.0,Pandas - Dates,"```DateOffset```, ```offsets```","
from pandas import DateOffset, ExcelFile, read_ex",1.0
2020.0,9.0,Pandas - Dates,"```DateOffset```, ```offsets```","df['End Date'] + pd.DateOffset(years=-1), 
       ",1.0
2021.0,37.0,Pandas - Dates,"```DateOffset```, ```offsets```","from pandas.tseries.offsets import DateOffset

",1.0
2021.0,50.0,Pandas - Dates,"```DateOffset```, ```offsets```","t, ExcelFile, melt, offsets, read_excel

# for ",1.0
2022.0,15.0,Pandas - Dates,"```DateOffset```, ```offsets```","range(s, e, freq=pd.DateOffset(months=1)).union([e",1.0
2022.0,32.0,Pandas - Dates,"```DateOffset```, ```offsets```", [CURRENT_DATE + pd.DateOffset(months=m-1) for m i,1.0
2021.0,24.0,Pandas - Dates,```date_range```," date
df['Date'] = [date_range(d, periods=p, freq='",
2021.0,37.0,Pandas - Dates,```date_range```,"['Payment Date'] = [date_range(d, periods=p, freq=D",
2021.0,42.0,Pandas - Dates,```date_range```,DataFrame({'Date' : date_range(start=df_in['Date'].,
2021.0,44.0,Pandas - Dates,```date_range```,"21(inclusive)
rng = date_range(start='2021-01-01', ",
2022.0,15.0,Pandas - Dates,```date_range```,"   lambda df_x: [pd.date_range(s, e, freq=pd.DateOf",
2022.0,28.0,Pandas - Dates,```date_range```,Frame({ 'date' : pd.date_range(start=df_sales['Sale,
2022.0,32.0,Pandas - Dates,```date_range```,"_Payment_Date = [pd.date_range(start=CURRENT_DATE, ",
2022.0,36.0,Pandas - Dates,```date_range```,"te range
dates = pd.date_range(start=datetime(df['s",
,,Pandas - Dates,```date_range```,"Date' : rnd.choices(date_range('2000-01-01', '2021-",
2019.0,25.0,Pandas - Dates,```dt.strftime```,"inal['Concert Date'].dt.strftime('%Y-%m-%d') + '|' \
",
2021.0,12.0,Pandas - Dates,```dt.strftime```,"= df_region['Month'].dt.strftime('%b')

# draw a grid",
2021.0,28.0,Pandas - Dates,```dt.strftime```,"[0:4] + df_m['date'].dt.strftime('-%m-%d'))


# add c",
2021.0,49.0,Pandas - Dates,```dt.strftime```,"e'].transform('min').dt.strftime('%b %Y') \
         ",
2020.0,2.0,Pandas - Dates,```dt.strptime```,"'] = [dt.strftime(dt.strptime(d, '%d/%m/%y'), '%m/",
2020.0,3.0,Pandas - Dates,```dt.strptime```,"ate
df['Date'] = [dt.strptime(d, '%a %b %d %Y') fo",
2020.0,4.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\inputs\PD 2020 Wk 4 Input.csv', parse_dates=['DoB'], dayfirst=True)
df_q = read_csv(r'.",
2020.0,10.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'', parse_dates=[], dayfirst=True)


#----------------",
2021.0,1.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"ata-2021-01')

df = read_csv('.\\inputs\\PD 2021 Wk 1 Input - Bike Sales.csv', 
              parse_dates=['Date'], dayfirst=True)

# split the 'Store",
2021.0,2.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"input the data
df = read_csv('.\\inputs\\PD 2021 Wk 2 Input - Bike Model Sales.csv', 
              parse_dates=['Order Date', 'Shipping Date'], dayfirst=True)


# clean up the Mo",
2021.0,5.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"input the data
df = read_csv('.\\inputs\\Joined Dataset.csv', parse_dates=['From Date'], dayfirst=True)

# find the current",
2021.0,16.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"ty', 'Spurs']

df = read_csv(r'.\inputs\PL Fixtures.csv', parse_dates=['Date'], dayfirst=True)\
     .dropna(subse",
2021.0,20.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\inputs\Prep Air Complaints - Complaints per Day.csv',
              parse_dates=['Date'], dayfirst=True)


#----------------",
2021.0,26.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\inputs\PD 2021 Wk 26 Input - Sheet1.csv', parse_dates=['Date'], dayfirst=True)\
             .sort",
2021.0,31.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\\inputs\\PD 2021 Wk 31 Input.csv', parse_dates=['Date'])


#----------------",
2021.0,32.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\\inputs\\PD 2021 Wk 32 Input - Data.csv', parse_dates=['Date', 'Date of Flight'], 
              dayfirst=True)


#----------------",
2021.0,39.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\\inputs\\Bike Painting Process - Painting Process.csv',
              parse_dates={'Datetime' : ['Date', 'Time']}, dayfirst=True)


#----------------",
2021.0,42.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"----------

df_in = read_csv(r'.\\inputs\\Prep Generate Rows datasets - Charity Fundraiser.csv', 
                 parse_dates=['Date'], dayfirst=True)


#----------------",
2021.0,43.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"s xl:
    df_risk = read_excel(xl, 'Risk Level')
    df_a = read_excel(xl, 'Business Unit A ', parse_dates={'Date lodged' : ['Month ', 'Date', 'Year']})
    df_b = read_exc",
2021.0,49.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\\inputs\\PD 2021 Wk 49 Input - Input.csv', parse_dates=['Date'], dayfirst=True)


#----------------",
2021.0,51.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(c",
2022.0,1.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'.\inputs\PD 2022 Wk 1 Input - Input.csv', parse_dates=['Date of Birth'])


#----------------",
2022.0,2.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"----------

df = pd.read_csv(r'.\inputs\PD 2022 Wk 1 Input - Input.csv', parse_dates=['Date of Birth'],
                 usecols=['id', 'pupil first name', 'pupil last name', 'Date of Birth'])


#----------------",
2022.0,17.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"ssion
    df_s = pd.read_excel(xl, sheet_name='Streaming', parse_dates=['t'])\
             .assi",
2022.0,24.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"    df_flights = pd.read_excel(xl, sheet_name='Non-stop flights', parse_dates=['First flight'], 
                               dtype={'Scheduled duration' : str})
    df_cities = pd.",
2022.0,27.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"----------

df = pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"", 
                 parse_dates=['Sale Date'], dayfirst=True)


#----------------",
2022.0,28.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"----

df_sales = pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"", 
                       parse_dates=['Sale Date'], dayfirst=True, usecols=['Sale Date'])


#----------------",
2022.0,33.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"s = ( pd.concat([pd.read_csv(r'.\inputs\PD 2022 Week 33 Input Instore Orders.csv',
                                    parse_dates=['Sales Date'], dayfirst=True)
                   ",
2022.0,34.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates," data
    df = ( pd.read_csv(input_path, parse_dates=['Date'], dayfirst=True)
             .renam",
2022.0,35.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"data
    df =  ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - CEO Cycling.csv"", 
                        parse_dates=['Date'], dayfirst=True)
              .rena",
2022.0,36.0,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"----------

df = pd.read_csv(r'.\inputs\employee_data.csv', parse_dates=['scheduled_date'])


#----------------",
,,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'..\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(c",
,,Pandas - Dates,```read_csv``` or ```read_excel``` with parse_dates,"-------------

df = read_csv(r'..\inputs\2021W51 Input.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(c",
2020.0,4.0,Pandas - Dates,"```timedelta```, ```to_timedelta```, ```relativedelta```","ot_table, read_csv, to_timedelta, to_datetime


def ",
2020.0,7.0,Pandas - Dates,"```timedelta```, ```to_timedelta```, ```relativedelta```","parse
from dateutil.relativedelta import relativedelt",
2021.0,8.0,Pandas - Dates,"```timedelta```, ```to_timedelta```, ```relativedelta```","e_asof, read_excel, Timedelta

# used for answer ",
2021.0,18.0,Pandas - Dates,"```timedelta```, ```to_timedelta```, ```relativedelta```","_table, read_excel, Timedelta


# for solution ch",
2021.0,26.0,Pandas - Dates,"```timedelta```, ```to_timedelta```, ```relativedelta```","e, merge, read_csv, Timedelta


#----------------",
2021.0,33.0,Pandas - Dates,"```timedelta```, ```to_timedelta```, ```relativedelta```","elFile, read_excel, Timedelta, to_datetime

# for",
2020.0,4.0,Pandas - Dates,```to_datetime```,"_csv, to_timedelta, to_datetime


def age(begin, en",
2020.0,9.0,Pandas - Dates,```to_datetime```,df['End Date'] = pd.to_datetime(df['Date'].str.extr,
2021.0,2.0,Pandas - Dates,```to_datetime```,"t.to_period('M').dt.to_timestamp()
palette = ['#ccb2",
2021.0,12.0,Pandas - Dates,```to_datetime```,"t, merge, read_csv, to_datetime

# for the charts
i",
2021.0,21.0,Pandas - Dates,```to_datetime```,"elFile, read_excel, to_datetime

# for solution che",
2021.0,28.0,Pandas - Dates,```to_datetime```,"read_excel, Series, to_datetime


#----------------",
2021.0,29.0,Pandas - Dates,```to_datetime```,"Writer, read_excel, to_datetime
from re import IGNO",
2021.0,30.0,Pandas - Dates,```to_datetime```,"ataFrame, read_csv, to_datetime


#----------------",
2021.0,33.0,Pandas - Dates,```to_datetime```,"d_excel, Timedelta, to_datetime

# for results chec",
2021.0,45.0,Pandas - Dates,```to_datetime```," merge, read_excel, to_datetime

# for results chec",
2022.0,7.0,Pandas - Dates,```to_datetime```,"Month_Start_Date=pd.to_datetime(s + '1, 2021'))\
  ",
2022.0,12.0,Pandas - Dates,```to_datetime```,                 pd.to_datetime(df['DateSubmitted'],
2022.0,15.0,Pandas - Dates,```to_datetime```,"

CURRENT_DATE = pd.to_datetime('2022-04-13')


#--",
2022.0,18.0,Pandas - Dates,```to_datetime```,"me
df['Month'] = pd.to_datetime(df['Month'], format",
2021.0,2.0,Pandas - Dates,```to_period```,df['Order Date'].dt.to_period('M').dt.to_timestam,
2022.0,15.0,Pandas - Dates,```to_period```,['Contract End'].dt.to_period('M').view('int64') ,
2022.0,17.0,Pandas - Dates,"working with timezones (```tz_localize```, etc.)",transform('min').dt.tz_localize(None).astype('datet,
2021.0,41.0,Pandas - File I/O,Formatting numeric output,"---------------

df.to_csv(r'.\outputs\output-2021-41.csv', index=False, float_format='%d',
          columns=['Season', 'Outcome', 'Special Circumstances', 'League', 'P', 'W', 'D', 'L',
                   'F', 'A', 'Pts', 'POS'])",
2019.0,4.0,Pandas - File I/O,Read Excel files,"eOffset, ExcelFile, read_excel

# for results chec",
2019.0,25.0,Pandas - File I/O,Read Excel files,")

df_concerts = pd.read_excel('Wow _ PD data set.",
2020.0,6.0,Pandas - File I/O,Read Excel files,"from pandas import read_excel


# import the data",
2020.0,8.0,Pandas - File I/O,Read Excel files,"kly = pd.concat([pd.read_excel(xl, s)\
           ",
2020.0,11.0,Pandas - File I/O,Read Excel files,"
    df_orders = pd.read_excel(xl, sheet_name='Ord",
2020.0,12.0,Pandas - File I/O,Read Excel files,"xcelFile, read_csv, read_excel
import datetime as ",
2020.0,17.0,Pandas - File I/O,Read Excel files,"s import ExcelFile, read_excel, merge
from numpy i",
2020.0,32.0,Pandas - File I/O,Read Excel files,"rge_asof, read_csv, read_excel


#----------------",
2021.0,7.0,Pandas - File I/O,Read Excel files,"taFrame, ExcelFile, read_excel

# used for answer ",
2021.0,8.0,Pandas - File I/O,Read Excel files," merge, merge_asof, read_excel, Timedelta

# used ",
2021.0,9.0,Pandas - File I/O,Read Excel files,"s import DataFrame, read_excel
from numpy import f",
2021.0,10.0,Pandas - File I/O,Read Excel files,"s import ExcelFile, read_excel
from numpy import n",
2021.0,11.0,Pandas - File I/O,Read Excel files,"s import ExcelFile, read_excel

# used for answer ",
2021.0,14.0,Pandas - File I/O,Read Excel files,"lFile, ExcelWriter, read_excel


# ---------------",
2021.0,15.0,Pandas - File I/O,Read Excel files," melt, pivot_table, read_excel

# for results chec",
2021.0,17.0,Pandas - File I/O,Read Excel files,"rt ExcelFile, melt, read_excel

# for solution che",
2021.0,18.0,Pandas - File I/O,Read Excel files,"lFile, pivot_table, read_excel, Timedelta


# for ",
2021.0,19.0,Pandas - File I/O,Read Excel files,"t ExcelFile, merge, read_excel

# for solution che",
2021.0,20.0,Pandas - File I/O,Read Excel files,"s import ExcelFile, read_excel


#----------------",
2021.0,21.0,Pandas - File I/O,Read Excel files,"taFrame, ExcelFile, read_excel, to_datetime

# for",
2021.0,22.0,Pandas - File I/O,Read Excel files,"taFrame, ExcelFile, read_excel

# for solution che",
2021.0,23.0,Pandas - File I/O,Read Excel files,"taFrame, ExcelFile, read_excel

# for results chec",
2021.0,24.0,Pandas - File I/O,Read Excel files,"e_range, ExcelFile, read_excel

# for solution che",
2021.0,25.0,Pandas - File I/O,Read Excel files,", ExcelFile, merge, read_excel


#----------------",
2021.0,27.0,Pandas - File I/O,Read Excel files,"lFile, melt, merge, read_excel
from random import ",
2021.0,28.0,Pandas - File I/O,Read Excel files,"lFile, ExcelWriter, read_excel, Series, to_datetim",
2021.0,29.0,Pandas - File I/O,Read Excel files,"lFile, ExcelWriter, read_excel, to_datetime
from r",
2021.0,33.0,Pandas - File I/O,Read Excel files," concat, ExcelFile, read_excel, Timedelta, to_date",
2021.0,34.0,Pandas - File I/O,Read Excel files,"lFile, melt, merge, read_excel


def stack_dict(in",
2021.0,35.0,Pandas - File I/O,Read Excel files,"t ExcelFile, merge, read_excel


def parse_sizes(s",
2021.0,36.0,Pandas - File I/O,Read Excel files,"lFile, melt, merge, read_excel


#----------------",
2021.0,37.0,Pandas - File I/O,Read Excel files,"e_range, ExcelFile, read_excel
from pandas.tseries",
2021.0,38.0,Pandas - File I/O,Read Excel files,"t merge, ExcelFile, read_excel

# for results chec",
2021.0,43.0,Pandas - File I/O,Read Excel files,"lFile, pivot_table, read_excel
from pandas import ",
2021.0,44.0,Pandas - File I/O,Read Excel files,"lFile, pivot_table, read_excel

# for results chec",
2021.0,45.0,Pandas - File I/O,Read Excel files,", ExcelFile, merge, read_excel, to_datetime

# for",
2021.0,46.0,Pandas - File I/O,Read Excel files,", ExcelFile, merge, read_excel

# for results chec",
2021.0,47.0,Pandas - File I/O,Read Excel files,"rt ExcelFile, melt, read_excel
from textwrap impor",
2021.0,48.0,Pandas - File I/O,Read Excel files,"rt ExcelFile, melt, read_excel, Series

# for resu",
2021.0,50.0,Pandas - File I/O,Read Excel files,"ile, melt, offsets, read_excel

# for results chec",
2021.0,52.0,Pandas - File I/O,Read Excel files,"s import ExcelFile, read_excel


# for results che",
2022.0,6.0,Pandas - File I/O,Read Excel files,":
    df_words = pd.read_excel(xl, sheet_name='7 l",
2022.0,7.0,Pandas - File I/O,Read Excel files,"ric = pd.concat([pd.read_excel(xl, s)\
           ",
2022.0,8.0,Pandas - File I/O,Read Excel files,"rs
    df_stat = pd.read_excel(xl, 'pkmn_stats', u",
2022.0,9.0,Pandas - File I/O,Read Excel files,"----------

df = pd.read_excel(r'.\inputs\Sample -",
2022.0,10.0,Pandas - File I/O,Read Excel files," as xl:
    df = pd.read_excel(xl, 'Webscraping')
",
2022.0,15.0,Pandas - File I/O,Read Excel files,"--------

df_p = pd.read_excel(r'.\inputs\Office S",
2022.0,16.0,Pandas - File I/O,Read Excel files,"
    df_orders = pd.read_excel(xl, 'Orders')\
    ",
2022.0,17.0,Pandas - File I/O,Read Excel files,"ssion
    df_s = pd.read_excel(xl, sheet_name='Str",
2022.0,19.0,Pandas - File I/O,Read Excel files,":
    df_sales = pd.read_excel(xl, sheet_name='Sal",
2022.0,20.0,Pandas - File I/O,Read Excel files,"xl:
    df_reg = pd.read_excel(xl, sheet_name='Reg",
2022.0,21.0,Pandas - File I/O,Read Excel files,"f = ( pd.concat([pd.read_excel(xl, sheet_name=s, s",
2022.0,22.0,Pandas - File I/O,Read Excel files,"xl:
    df_eps = pd.read_excel(xl, sheet_name='epi",
2022.0,24.0,Pandas - File I/O,Read Excel files,"    df_flights = pd.read_excel(xl, sheet_name='Non",
,,Pandas - File I/O,Read Excel files,"kly = pd.concat([pd.read_excel(xl, s)\
           ",
2021.0,23.0,Pandas - File I/O,Read Excel files (dynamic sheets),"xl:
    df = concat([read_excel(xl, s) for s in xl.sheet_names])   
        

#----",
2021.0,25.0,Pandas - File I/O,Read Excel files (dynamic sheets),"   exclude = concat([read_excel(xl, s) for s in ['Mega Evolutions', 'Alolan', 'Galarian', 
                                                  'Gigantamax']])
    unattainable ",
2021.0,45.0,Pandas - File I/O,Read Excel files (dynamic sheets),"xl:
    df = concat([read_excel(xl, s).assign(date=s) for s in xl.sheet_names if s != 'Attendees'])\
        .rename(c",
2021.0,46.0,Pandas - File I/O,Read Excel files (dynamic sheets),"  df_sales = concat([read_excel(xl, s).assign(sheet_name=s) 
                       for s in xl.sheet_names if 'Sales' in s])


#---------------",
2021.0,50.0,Pandas - File I/O,Read Excel files (dynamic sheets),"xl:
    df = concat([read_excel(xl, s).assign(sheet=s) for s in xl.sheet_names])\
         .rename(",
2020.0,3.0,Pandas - File I/O,Write multiple tabs to Excel file," concat, ExcelFile, ExcelWriter, melt

# import the",
2021.0,14.0,Pandas - File I/O,Write multiple tabs to Excel file,"s import ExcelFile, ExcelWriter, read_excel


# ---",
2021.0,20.0,Pandas - File I/O,Write multiple tabs to Excel file,"s import DataFrame, ExcelWriter, read_csv

# for so",
2021.0,28.0,Pandas - File I/O,Write multiple tabs to Excel file," concat, ExcelFile, ExcelWriter, read_excel, Series",
2021.0,29.0,Pandas - File I/O,Write multiple tabs to Excel file,"s import ExcelFile, ExcelWriter, read_excel, to_dat",
2021.0,27.0,Pandas - Joining,```append```,"ottery)]
    lottery.append(choices(list(prob_su",
2021.0,41.0,Pandas - Joining,```append```,"ing_years)})
df = df.append(df_adds)


#--------",
2019.0,25.0,Pandas - Joining,```concat```," = ''
df_final = pd.concat([df_joined, df_artis",
2020.0,1.0,Pandas - Joining,```concat```,"    dfSubtotal = pd.concat([dfSubtotal, 
      ",
2020.0,3.0,Pandas - Joining,```concat```,"lt_sheets:
    df = concat([df, xl.parse(sheet)",
2020.0,7.0,Pandas - Joining,```concat```," employees
df_all = concat([df_curr, df_left], ",
2020.0,8.0,Pandas - Joining,```concat```,"
    df_weekly = pd.concat([pd.read_excel(xl, s",
2020.0,11.0,Pandas - Joining,```concat```,"ired.
df_boxes = pd.concat([df_orders, 
       ",
2021.0,3.0,Pandas - Joining,```concat```," = sheet
    dfIn = concat([dfIn, dfNew])


# p",
2021.0,4.0,Pandas - Joining,```concat```," = sheet
    dfIn = concat([dfIn, dfNew])

# pi",
2021.0,12.0,Pandas - Joining,```concat```," inplace=True)
df = concat([df[df['Country'].no",
2021.0,13.0,Pandas - Joining,```concat```,"-------------

df = concat([read_csv(r'.\inputs",
2021.0,16.0,Pandas - Joining,```concat```," the big 6
# sorted(concat([df['Home Team'], df",
2021.0,21.0,Pandas - Joining,```concat```,"'] = s
        df = concat([df, df_new])


#---",
2021.0,23.0,Pandas - Joining,```concat```,"x') as xl:
    df = concat([read_excel(xl, s) f",
2021.0,25.0,Pandas - Joining,```concat```,"ons')
    exclude = concat([read_excel(xl, s) f",
2021.0,26.0,Pandas - Joining,```concat```,"all values
df_all = concat([df_date_agg, df_tot",
2021.0,28.0,Pandas - Joining,```concat```,"'] = s
        df = concat([df, df_temp])


#--",
2021.0,33.0,Pandas - Joining,```concat```,"'] = s
        df = concat([df, df_temp])
     ",
2021.0,43.0,Pandas - Joining,```concat```,"A & B together
df = concat([df_a, df_b])
df.col",
2021.0,45.0,Pandas - Joining,```concat```,"x') as xl:
    df = concat([read_excel(xl, s).a",
2021.0,46.0,Pandas - Joining,```concat```,"rame
    df_sales = concat([read_excel(xl, s).a",
2021.0,50.0,Pandas - Joining,```concat```,"x') as xl:
    df = concat([read_excel(xl, s).a",
2022.0,7.0,Pandas - Joining,```concat```,"
    df_metric = pd.concat([pd.read_excel(xl, s",
2022.0,10.0,Pandas - Joining,```concat```,"na()
df_html_m = pd.concat([df_html_m.loc[df_ht",
2022.0,12.0,Pandas - Joining,```concat```,"ubmitted']

df = pd.concat([pd.read_csv(path.jo",
2022.0,20.0,Pandas - Joining,```concat```,"1

df_combined = pd.concat( 
    [df_reg_onl.me",
2022.0,21.0,Pandas - Joining,```concat```,"
    
    df = ( pd.concat([pd.read_excel(xl, s",
2022.0,23.0,Pandas - Joining,```concat```,"history
df_out = pd.concat([df[['Id', 'CreatedD",
2022.0,30.0,Pandas - Joining,```concat```,"csv']

df_top3 = pd.concat([pd.read_csv(f)
    ",
2022.0,33.0,Pandas - Joining,```concat```,"--

df_sales = ( pd.concat([pd.read_csv(r'.\inp",
2022.0,35.0,Pandas - Joining,```concat```,"f
    df_all = ( pd.concat([df_agg.drop(columns",
,,Pandas - Joining,```concat```,"
    df_weekly = pd.concat([pd.read_excel(xl, s",
2020.0,32.0,Pandas - Joining,```merge_asof```," Manager'].notna()]
merge_asof(df, df_managers, on=",
2021.0,8.0,Pandas - Joining,```merge_asof```,"customer_sessions = merge_asof(customers, session_s",
2022.0,22.0,Pandas - Joining,```merge_asof```,"tions
df_out = ( pd.merge_asof(df_dialogue.sort_val",
2022.0,33.0,Pandas - Joining,```merge_asof```,"D'])

df_merge = pd.merge_asof(df_sales, 
         ",
2019.0,25.0,Pandas - Joining,```merge```,"---

df_joined = pd.merge(df_concerts, df_lat_",
2020.0,1.0,Pandas - Joining,```merge```," the Profit
df = pd.merge(df, dfSubtotal, how=",
2020.0,3.0,Pandas - Joining,```merge```,"conferences
df = df.merge(df_team, left_on='wi",
2020.0,4.0,Pandas - Joining,```merge```,"et_index()\
       .merge(df_r, how='inner', o",
2020.0,6.0,Pandas - Joining,```merge```,"s
df_all = df_sales.merge(df_rates_sum, on=['Y",
2020.0,7.0,Pandas - Joining,```merge```,"= 1
df_all = df_all.merge(df_dates, left_on='L",
2020.0,8.0,Pandas - Joining,```merge```,"fit_out = df_profit.merge(df_weekly, on=['Week",
2020.0,12.0,Pandas - Joining,```merge```,"es data
df = df_pct.merge(df_lookup, how='left",
2020.0,17.0,Pandas - Joining,```merge```," 1

dfDeviceCount = merge(dfDeviceCount, dfDev",
2021.0,1.0,Pandas - Joining,```merge```," 1

df_all = df_qtr.merge(df_day, how='outer',",
2021.0,4.0,Pandas - Joining,```merge```,"ata
dfFinal = dfSum.merge(dfTargets, how='left",
2021.0,5.0,Pandas - Joining,```merge```,"dfFinal = dfDeduped.merge(dfCurrentAM, how='le",
2021.0,6.0,Pandas - Joining,```merge```,"rames
dfAll = dfAgg.merge(dfRankAgg, how='left",
2021.0,7.0,Pandas - Joining,```merge```,"= 1
dfAll = dfItems.merge(dfKeywords, on='join",
2021.0,8.0,Pandas - Joining,```merge```,"o song list
final = merge(choices, customer_se",
2021.0,9.0,Pandas - Joining,```merge```,"stomers = customers.merge(areas, on='join', ho",
2021.0,10.0,Pandas - Joining,```merge```,"nulls)
df = pokemon.merge(evolution, left_on='",
2021.0,11.0,Pandas - Joining,```merge```,"cipes.reset_index().merge(src[cols], on='Ingre",
2021.0,12.0,Pandas - Joining,```merge```,"cont_tot = cont_tot.merge(cont_dtl, on=['Break",
2021.0,14.0,Pandas - Joining,```merge```,"mbined = passengers.merge(seats, on='passenger",
2021.0,15.0,Pandas - Joining,```merge```,"s
orders_f = orders.merge(menu_f, left_on='Ord",
2021.0,16.0,Pandas - Joining,```merge```,"n
total_2 = total_2.merge(total_1[['Team', 'Po",
2021.0,17.0,Pandas - Joining,```merge```,"s
df_area = df_area.merge(df_tot, on='Name', h",
2021.0,18.0,Pandas - Joining,```merge```,"l dataframe
df = df.merge(df_complete[id_vars ",
2021.0,19.0,Pandas - Joining,```merge```,"okup tables
df = df.merge(proj, on='Project Co",
2021.0,20.0,Pandas - Joining,```merge```,"] = 1
df_wk = df_wk.merge(DataFrame({'n' : std",
2021.0,22.0,Pandas - Joining,```merge```,"tions
smash = smash.merge(questions, on='Q No'",
2021.0,24.0,Pandas - Joining,```merge```,"df_dates = df_dates.merge(df_count, on='Date',",
2021.0,25.0,Pandas - Joining,```merge```,"gen1[['#', 'Name']].merge(evol_group, on='#', ",
2021.0,26.0,Pandas - Joining,```merge```,"n date
df_join = df.merge(df_dates, on='Date',",
2021.0,27.0,Pandas - Joining,```merge```,"y]})\
             .merge(teams, on='Seed')\
 ",
2021.0,29.0,Pandas - Joining,```merge```,"*)')
final = events.merge(venues[['Venue_lower",
2021.0,34.0,Pandas - Joining,```merge```,"rgets
sales = sales.merge(targets, on=['Store'",
2021.0,35.0,Pandas - Joining,```merge```,"f = p.assign(key=1).merge(f.assign(key=1), how",
2021.0,36.0,Pandas - Joining,```merge```,"et_index()

df = df.merge(df_c, on='Search Ter",
2021.0,38.0,Pandas - Joining,```merge```," fields
df = df_sum.merge(df_t, on='Trilogy Ra",
2021.0,39.0,Pandas - Joining,```merge```,"df_final = df_batch.merge(df_parms, on='Batch ",
2021.0,42.0,Pandas - Joining,```merge```," freq='D')})\
     .merge(df_in, on='Date', ho",
2021.0,45.0,Pandas - Joining,```merge```,"me session)
dc = df.merge(df[['Session ID', 'A",
2021.0,46.0,Pandas - Joining,```merge```,"tail

df = df_sales.merge(d['Edition'], on='IS",
2021.0,47.0,Pandas - Joining,```merge```,"()\
               .merge(df_p[['player_id', '",
2021.0,50.0,Pandas - Joining,```merge```,"_index()\
         .merge(df[df['YTD Total'].n",
2021.0,51.0,Pandas - Joining,```merge```," fact table
df = df.merge(df_store[['StoreID',",
2021.0,52.0,Pandas - Joining,```merge```,")\
                .merge(df_dept.assign(Keywo",
2022.0,3.0,Pandas - Joining,```merge```,"nt
df = df_students.merge(df_grades.melt(id_va",
2022.0,4.0,Pandas - Joining,```merge```," practice!]
df = df.merge(df_students, on='Stu",
2022.0,6.0,Pandas - Joining,```merge```,"etters = df_letters.merge(df_scores, on='Tile'",
2022.0,7.0,Pandas - Joining,```merge```,"
                  .merge(pd.read_excel(xl, 'L",
2022.0,8.0,Pandas - Joining,```merge```,")

df_out = df_evol.merge(df_stat, left_on='St",
2022.0,9.0,Pandas - Joining,```merge```,"))\
               .merge(pd.DataFrame({'Year'",
2022.0,15.0,Pandas - Joining,```merge```,"         
df = df_c.merge(df_p, on=['City', 'O",
2022.0,16.0,Pandas - Joining,```merge```,"x'])\
             .merge(df_lookup, how='left",
2022.0,17.0,Pandas - Joining,```merge```,"ntent
df_out = df_s.merge(df_p.rename(columns=",
2022.0,19.0,Pandas - Joining,```merge```,"e
df_out = df_sales.merge(df_prod, left_on='Pr",
2022.0,20.0,Pandas - Joining,```merge```,"t( 
    [df_reg_onl.merge(df_online, on=['Emai",
2022.0,24.0,Pandas - Joining,```merge```,"_out = ( df_flights.merge(df_cities, left_on='",
2022.0,29.0,Pandas - Joining,```merge```,"aframes
df_out = df.merge(df_target, on=['PROD",
2022.0,30.0,Pandas - Joining,```merge```,"
df_out = ( df_top3.merge(df_sales, on='Store ",
2022.0,36.0,Pandas - Joining,```merge```,"te'])
             .merge(df.drop(columns='sch",
,,Pandas - Joining,```merge```,"fit_out = df_profit.merge(df_weekly, on=['Week",
,,Pandas - Joining,```merge```," fact table
df = df.merge(df_store[['StoreID',",
,,Pandas - Joining,```merge```,"n table
    df = df.merge(df_cust[['CustomerID",
,,Pandas - Joining,```merge```," practice!]
df = df.merge(df_students, on='Stu",
2020.0,4.0,Pandas - Other,"```apply```, ```map```","omer'] = df_p['DoB'].apply(lambda x: age(x, CU",
2021.0,1.0,Pandas - Other,"```apply```, ```map```","size='xx-large')

g.map(sns.lineplot, ""Day o",
2021.0,10.0,Pandas - Other,"```apply```, ```map```",Group'] = df['Name'].apply(get_evolution_group,
2021.0,11.0,Pandas - Other,"```apply```, ```map```",es['Cost'] = recipes.apply(lambda r: round(flo,
2021.0,14.0,Pandas - Other,"```apply```, ```map```","ts['Seat Position'].map(seat_types)

# parse",
2021.0,16.0,Pandas - Other,"```apply```, ```map```","on'] = total_1[cols].apply(tuple, axis=1)\
   ",
2021.0,17.0,Pandas - Other,"```apply```, ```map```","                   .map('{:.0%}'.format)


#",
2021.0,18.0,Pandas - Other,"```apply```, ```map```",erence to Schedule'].apply(lambda x: Timedelta,
2021.0,42.0,Pandas - Other,"```apply```, ```map```","
                 s.map(lambda x: f'{x:.0f}'",
2021.0,47.0,Pandas - Other,"```apply```, ```map```","ayer_data['metric'].map(color_map)
    )    ",
2022.0,2.0,Pandas - Other,"```apply```, ```map```", df['Date of Birth'].apply(lambda x: x.replace,
2022.0,6.0,Pandas - Other,"```apply```, ```map```","cy'].sum()).round(2).apply(d.Decimal)


# coun",
2022.0,10.0,Pandas - Other,"```apply```, ```map```",variable']=='Named'].apply(lambda x: x.str.low,
2020.0,8.0,Pandas - Other,```assign```,                    .assign(Week=int(s.replace(',
2020.0,11.0,Pandas - Other,```assign```,"sizes})\
           .assign(Remainder=lambda df_",
2021.0,35.0,Pandas - Other,```assign```,"es and frames
df = p.assign(key=1).merge(f.assig",
2021.0,45.0,Pandas - Other,```assign```,"t([read_excel(xl, s).assign(date=s) for s in xl.",
2021.0,46.0,Pandas - Other,```assign```,"t([read_excel(xl, s).assign(sheet_name=s) 
     ",
2021.0,50.0,Pandas - Other,```assign```,"t([read_excel(xl, s).assign(sheet=s) for s in xl",
2021.0,52.0,Pandas - Other,```assign```,"())
df_out = df_comp.assign(Keyword=df_comp['Com",
2022.0,3.0,Pandas - Other,```assign```,"')\
                .assign(Pass=lambda df_x: df",
2022.0,4.0,Pandas - Other,```assign```,"ount'))\
           .assign(Trips_per_day = lamb",
2022.0,6.0,Pandas - Other,```assign```,">.*)')\
            .assign(Tile=lambda df_x: df",
2022.0,7.0,Pandas - Other,```assign```,                    .assign(Month_Start_Date=pd.,
2022.0,9.0,Pandas - Other,```assign```,", 'Orders')\
       .assign(Year=df['Order Date'",
2022.0,11.0,Pandas - Other,```assign```," : 'Time'})\
       .assign(Time=lambda df_x: df",
2022.0,12.0,Pandas - Other,```assign```,"\
                  .assign(Report=f[-16:-4],
  ",
2022.0,13.0,Pandas - Other,```assign```,"rue)\
              .assign(Pct_of_Total=lambda ",
2022.0,14.0,Pandas - Other,```assign```,"= Series)"")\
       .assign(F=lambda df_x: df_x[",
2022.0,15.0,Pandas - Other,```assign```,"w='left')\
         .assign(Month_Divider=\
    ",
2022.0,16.0,Pandas - Other,```assign```,"dex')\
             .assign(Guest=lambda df_x: d",
2022.0,17.0,Pandas - Other,```assign```,"'t'])\
             .assign(location=lambda df_x",
2022.0,21.0,Pandas - Other,```assign```,"                    .assign(Shop=s) 
           ",
2022.0,22.0,Pandas - Other,```assign```,"me
df_out = ( df_out.assign(name=df_out['name'].",
2022.0,23.0,Pandas - Other,```assign```,"                    .assign(Stage='Opened', 
   ",
2022.0,29.0,Pandas - Other,```assign```,"nput.csv"")
         .assign(PRODUCT = lambda df_",
2022.0,30.0,Pandas - Other,```assign```,                    .assign(Region=f[f.find('('),
2022.0,32.0,Pandas - Other,```assign```," month
df_out = ( df.assign(Monthly_Payment_Date",
2022.0,33.0,Pandas - Other,```assign```,"                    .assign(Store='Online')])
  ",
2022.0,36.0,Pandas - Other,```assign```,"ed_date', 'emp_id']].assign(scheduled=True), 
  ",
,,Pandas - Other,```assign```,                    .assign(Week=int(s.replace(',
,,Pandas - Other,```assign```,"])\
                .assign(CustomerID=range(1, ",
,,Pandas - Other,```assign```,"of_Travel')\
       .assign(Method_of_Travel = l",
2020.0,3.0,Pandas - Other,```astype```,"loc[:, 2:].fillna(0).astype(int)], axis=1)


# c",
2020.0,4.0,Pandas - Other,```astype```,our'] = df_p['Hour'].astype(int) + where(df_p['A,
2020.0,6.0,Pandas - Other,```astype```,".]+)', expand=False).astype(float)
df_rates['Wee",
2020.0,7.0,Pandas - Other,```astype```," = df_curr['Salary'].astype(int)
df_curr['Leave ",
2020.0,8.0,Pandas - Other,```astype```,"extract('.*\_(\d+)').astype(int),
              ",
2020.0,9.0,Pandas - Other,```astype```,"se)\
               .astype(int)
      
        ",
2020.0,11.0,Pandas - Other,```astype```,"f['Remainder'] / s)).astype(int)
            
  ",
2021.0,6.0,Pandas - Other,```astype```,"per_Event'].round(0).astype(int)

# rank
df['ove",
2021.0,8.0,Pandas - Other,```astype```,"se', ascending=True).astype(int)


# join the cu",
2021.0,9.0,Pandas - Other,```astype```,"customers['ordered'].astype(int)

# clean the pr",
2021.0,10.0,Pandas - Other,```astype```,okemon[(pokemon['#'].astype(float) <= 386) & (po,
2021.0,12.0,Pandas - Other,```astype```,"alue'] = df['value'].astype(int)

# extract the ",
2021.0,13.0,Pandas - Other,```astype```," scored'].fillna(0)).astype(int)

# rename the o",
2021.0,14.0,Pandas - Other,```astype```," flights['FlightID'].astype(int)

# calculate th",
2021.0,15.0,Pandas - Other,```astype```,"'] = orders['Order'].astype(str).str.split('-')
",
2021.0,16.0,Pandas - Other,```astype```,"(' - ', expand=True).astype(int)
df['Winner'] = ",
2021.0,17.0,Pandas - Other,```astype```,"rs'] = df_m['Hours'].astype('float16')


# work ",
2021.0,19.0,Pandas - Other,```astype```,"'Week ' + df['Week'].astype(str)

# split projec",
2021.0,21.0,Pandas - Other,```astype```,"+ df['Day of Month'].astype(str))

# Create 'New",
2021.0,23.0,Pandas - Other,```astype```,"include_lowest=True).astype(str)
    
# calculat",
2021.0,25.0,Pandas - Other,```astype```,"'] = evol_group['#'].astype(int)
evol_group.drop",
2021.0,27.0,Pandas - Other,```astype```,rob'] = prob['prob'].astype(str).str.replace('>',
2021.0,28.0,Pandas - Other,```astype```," + '|' + df_m['no.'].astype(str)
df_m['penalty_s",
2021.0,30.0,Pandas - Other,```astype```,07-12 ' + df['Hour'].astype(str) + ':' + df['Min,
2021.0,32.0,Pandas - Other,```astype```,"           .round(0).astype(int)\
           .re",
2021.0,35.0,Pandas - Other,```astype```,"rs
    df[0] = df[0].astype(float) * where(df[1]",
2021.0,41.0,Pandas - Other,```astype```,"\d+)', expand=False).astype(float)))

# create a",
2021.0,44.0,Pandas - Other,```astype```,"Date')['km'].count().astype('Int64')

# ensure t",
2021.0,45.0,Pandas - Other,```astype```, (df['Session Time'].astype(str) + ':00:00').str,
2021.0,46.0,Pandas - Other,```astype```,"d['Info']['BookID2'].astype(str)).count() == \
 ",
2021.0,48.0,Pandas - Other,```astype```,"                    .astype(int)

# clean the me",
2021.0,50.0,Pandas - Other,```astype```,"         df['Date']).astype('datetime64[M]')
df[",
2021.0,51.0,Pandas - Other,```astype```,"-]', '', regex=True).astype(float) 
df['Sales'] ",
2022.0,6.0,Pandas - Other,```astype```,"extract(r'.*(\d+)').astype(int),
              ",
2022.0,7.0,Pandas - Other,```astype```,"                    .astype(float))\
           ",
2022.0,9.0,Pandas - Other,```astype```,".shift(1))\
        .astype('Int16'))


# add th",
2022.0,12.0,Pandas - Other,```astype```,"_datetime(df['Year'].astype(str) + '-01-01'))


",
2022.0,14.0,Pandas - Other,```astype```,"mbda df_x: df_x['F'].astype('int'),
            ",
2022.0,17.0,Pandas - Other,```astype```,"dt.tz_localize(None).astype('datetime64[M]'), 
 ",
2022.0,19.0,Pandas - Other,```astype```,"                    .astype(str)
               ",
2022.0,21.0,Pandas - Other,```astype```,"df_x: df_x['Target'].astype(str))
             .",
2022.0,22.0,Pandas - Other,```astype```,"                   ).astype(int)


# split (dupl",
2022.0,24.0,Pandas - Other,```astype```,"                    .astype(int)
               ",
2022.0,27.0,Pandas - Other,```astype```,'Original Quantity'].astype(int) * where(df['Uni,
2022.0,28.0,Pandas - Other,```astype```," [datetime64(d, 'D').astype(datetime).strftime('",
2022.0,34.0,Pandas - Other,```astype```,"s'] = df['Calories'].astype(int)
    
    return",
2022.0,35.0,Pandas - Other,```astype```,"s'] = df['Calories'].astype(int)
    df['Distanc",
,,Pandas - Other,```astype```,"extract('.*\_(\d+)').astype(int),
              ",
,,Pandas - Other,```astype```,"-]', '', regex=True).astype(float) 
df['Sales'] ",
,,Pandas - Other,```astype```,"-]', '', regex=True).astype(float) 
df['Sales'] ",
2022.0,5.0,Pandas - Other,"```cut```, ```qcut```","nsform(lambda x: pd.qcut(x, q=6, labels=['F',",
,,Pandas - Other,```factorize```,"e', '')}ID""] = df[c].factorize()[0] + 1


# create ",
,,Pandas - Other,```factorize```,"D'] = df['Customer'].factorize()[0] + 1
           ",
2019.0,25.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","f_joined['Concert'].fillna('')

# create a new",
2020.0,1.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","zeroes
df['Profit'].fillna(0, inplace=True)

#",
2020.0,3.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","'] = df['win_flag'].fillna(0)
df['loss_flag'] ",
2020.0,32.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","-------
# Method 1: ffill
#------------------",
2021.0,1.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","of Month', 'Bike']).fillna(0)
df_all['Cuml Sal",
2021.0,12.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","alue_CountryTotal'].fillna(0)

# remove the co",
2021.0,13.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","'Penalties scored'].fillna(0) \
              ",
2021.0,24.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","'Date', how='left').fillna(0)


#-------------",
2021.0,39.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","Data Value'], nan)).fillna(method='ffill')


#",
2021.0,42.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","al Raised to date'].fillna(method='ffill', inp",
2021.0,44.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```",= df_p.reindex(rng).fillna(0).rename_axis('Dat,
2021.0,47.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","= df_e['prize_usd'].fillna(0)


# summarize ev",
2021.0,48.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","                   .fillna(method='ffill')

# ",
2021.0,50.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","= df['Salesperson'].fillna(method='bfill')


#",
2021.0,52.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","= df_out['Keyword'].fillna('other')
df_out['De",
2022.0,11.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","ing=False)\
       .ffill()

# find the avera",
2022.0,12.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","d')['EmployerName'].ffill())\
       .reset_i",
2022.0,16.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","ce(COURSES), NaN )).ffill())\
             .d",
2022.0,21.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```"," df_x['Department'].ffill(),
                ",
2022.0,36.0,Pandas - Other,"```fillna```, ```ffill```, ```bfill```","eft')
             .fillna(False)
         )

",
2021.0,44.0,Pandas - Other,```reindex```,"-11-01')
df_p = df_p.reindex(rng).fillna(0).renam",
2021.0,8.0,Pandas - Other,"```shift```, ```diff```",'] = choices['Date'].diff(1).dt.total_seconds(,
2021.0,30.0,Pandas - Other,"```shift```, ```diff```","s'] = abs(df['From'].shift(-1) - df['To'])


# ",
2021.0,41.0,Pandas - Other,"```shift```, ```diff```",ere(df['league_nbr'].shift(-1) < df['league_nbr,
2021.0,50.0,Pandas - Other,"```shift```, ```diff```",          df['Date'].shift(1) - offsets.MonthBe,
2022.0,9.0,Pandas - Other,"```shift```, ```diff```","df_cust_yr['Order?'].shift(1) == 0, 'Returning'",
2022.0,12.0,Pandas - Other,"```shift```, ```diff```",e(df_x['EmployerId'].shift(1) != df_x['Employer,
2020.0,3.0,Pandas - Other,```sort_values```,"'loser', 1, nan)

df.sort_values(['Team', 'Date'], as",
2020.0,5.0,Pandas - Other,```sort_values```,"dard Competition)']].sort_values(by=['Venue'])


# ou",
2020.0,9.0,Pandas - Other,```sort_values```," Voter']\
          .sort_values(by=['Candidate', 'En",
2021.0,1.0,Pandas - Other,```sort_values```,"uarter'].unique() }).sort_values(by='Quarter')
df_qtr",
2021.0,5.0,Pandas - Other,```sort_values```,"e']
dfCurrentAM = df.sort_values(by='From Date').grou",
2021.0,8.0,Pandas - Other,```sort_values```,", 'Karaoke Choices').sort_values(by='Date')
    custo",
2021.0,13.0,Pandas - Other,```sort_values```,"size().reset_index().sort_values(by=0, ascending=Fals",
2021.0,14.0,Pandas - Other,```sort_values```,"ex()\
              .sort_values(by='purchase_amount'",
2021.0,15.0,Pandas - Other,```sort_values```,"x()\
               .sort_values(by='Order', ascendin",
2021.0,25.0,Pandas - Other,```sort_values```,"n='Evolution Group').sort_values(by='Appearances')


",
2021.0,26.0,Pandas - Other,```sort_values```,"True)\
             .sort_values(by='Date')


#------",
2021.0,27.0,Pandas - Other,```sort_values```,r t in teams['Seed'].sort_values() if t not in lotter,
2021.0,30.0,Pandas - Other,```sort_values```,"f = df.reset_index().sort_values(by=['trip_dtt', 'ind",
2021.0,36.0,Pandas - Other,```sort_values```,"que())]\
           .sort_values('pct', ascending=Fal",
2021.0,38.0,Pandas - Other,```sort_values```,"ax'))\
             .sort_values(['Trilogy_Average', ",
2021.0,39.0,Pandas - Other,```sort_values```,"process step
df = df.sort_values(['Batch No.', 'Datet",
2021.0,41.0,Pandas - Other,```sort_values```," null value)
df = df.sort_values(by='Season')
df['Out",
2021.0,45.0,Pandas - Other,```sort_values```,", axis=0)\
         .sort_values(by='Contact_Type')\
",
2021.0,50.0,Pandas - Other,```sort_values```,"person'])\
         .sort_values(by=['Salesperson', '",
2021.0,51.0,Pandas - Other,```sort_values```,"dex()\
             .sort_values(by=['Order Date', 'S",
2022.0,9.0,Pandas - Other,```sort_values```,"e')\
               .sort_values(by=['Customer ID', '",
2022.0,10.0,Pandas - Other,```sort_values```,"west ranking
df = df.sort_values(by='Ranking', ascend",
2022.0,11.0,Pandas - Other,```sort_values```,"the same day
df = df.sort_values(by=['Weekday', 'Time",
2022.0,12.0,Pandas - Other,```sort_values```,"loyerName   
df = df.sort_values(by=['EmployerId', 'D",
2022.0,13.0,Pandas - Other,```sort_values```,"um()\
              .sort_values(by='Sales', ascendin",
2022.0,22.0,Pandas - Other,```sort_values```,rge_asof(df_dialogue.sort_values(by=['time_in_secs']),
2022.0,32.0,Pandas - Other,```sort_values```,"Date')
             .sort_values(by=['Store', 'Monthl",
2022.0,33.0,Pandas - Other,```sort_values```,"
df_sales = df_sales.sort_values(by=['ID'])

df_merge",
2022.0,34.0,Pandas - Other,```sort_values```,"k'] <= n]
          .sort_values('Rank', ascending=Tr",
,,Pandas - Other,```sort_values```,"oduct Name']:
    df.sort_values(by=['Order Date', c]",
,,Pandas - Other,```sort_values```,"dex()\
             .sort_values(by=['Order Date', 'S",
,,Pandas - Other,```sort_values```,"ing factorize
    df.sort_values(by=['Order Date', 'C",
2021.0,40.0,Pandas - Reshaping,```crosstab```,"nimal Type
final = (crosstab(df['Animal Type'], d",
2020.0,8.0,Pandas - Reshaping,```explode```,"                    .explode('Week')

# I could h",
2020.0,11.0,Pandas - Reshaping,```explode```,"
                   .explode('Box_Number')\
     ",
2021.0,7.0,Pandas - Reshaping,```explode```,"ck().str.split(', ').explode().reset_index(drop=T",
2021.0,9.0,Pandas - Reshaping,```explode```,"IDs'].str.split(' ').explode()
areas = read_excel",
2021.0,11.0,Pandas - Reshaping,```explode```,l)'].str.split('; ').explode().str.extract(regex_,
2021.0,15.0,Pandas - Reshaping,```explode```,"'-')
orders = orders.explode('Order')
orders['Ord",
2021.0,19.0,Pandas - Reshaping,```explode```,r.split('\s+(?=\[)').explode().str.strip().reset_,
2021.0,22.0,Pandas - Reshaping,```explode```,"ash']]
smash = smash.explode('Name')

# find the ",
2021.0,24.0,Pandas - Reshaping,```explode```,"ff'])]
df_count = df.explode('Date').groupby('Dat",
2021.0,26.0,Pandas - Reshaping,```explode```,"
df_dates = df_dates.explode('join_date')


# joi",
2021.0,29.0,Pandas - Reshaping,```explode```,",')]
events = events.explode('Events Split')


# ",
2021.0,37.0,Pandas - Reshaping,```explode```,"(months)'])]
df = df.explode('Payment Date')


# ",
2021.0,45.0,Pandas - Reshaping,```explode```,".split(', ')
df = df.explode('Attendee ID').astyp",
2021.0,52.0,Pandas - Reshaping,```explode```,"))\
                .explode('Keyword')\
        ",
2022.0,6.0,Pandas - Reshaping,```explode```,"(','))\
            .explode('Tile')\
           ",
2022.0,15.0,Pandas - Reshaping,```explode```," End'])])\
         .explode('Month_Divider')\
  ",
2022.0,22.0,Pandas - Reshaping,```explode```,"))
                 .explode('name')
            ",
2022.0,32.0,Pandas - Reshaping,```explode```,"th']])
             .explode('Monthly_Payment_Dat",
,,Pandas - Reshaping,```explode```,"                    .explode('Week')

# I could h",
2019.0,4.0,Pandas - Reshaping,```extract```,"alue']] = df[f'{c}'].str.extract('(.*?) (\d+)')


# d",
2019.0,25.0,Pandas - Reshaping,```extract```,= df_joined.LongLats.str.extract('(?P<Latitude>[\d\.\,
2020.0,1.0,Pandas - Reshaping,```extract```,"archy'] = df['Item'].str.extract('([\d\.]+?)\.? .*')
",
2020.0,4.0,Pandas - Reshaping,```extract```,"                    .str.extract('(\d{1,2}):?(\d{2})\",
2020.0,6.0,Pandas - Reshaping,```extract```,"Pound to US Dollar'].str.extract('\= ([\d\.]+)', expa",
2020.0,8.0,Pandas - Reshaping,```extract```,a df_x: df_x['Week'].str.extract('.*\_(\d+)').astype(,
2020.0,9.0,Pandas - Reshaping,```extract```,"_datetime(df['Date'].str.extract('.*- (\d+/\d+)', exp",
2021.0,9.0,Pandas - Reshaping,```extract```,"ustomer_info)['IDs'].str.extract(extract_regex)
custo",
2021.0,11.0,Pandas - Reshaping,```extract```,"plit('; ').explode().str.extract(regex_str, expand=Tr",
2021.0,15.0,Pandas - Reshaping,```extract```,] = menu['variable'].str.extract('(.*?)\s?(ID|Price|$,
2021.0,17.0,Pandas - Reshaping,```extract```,"]] = df_m.iloc[:, 0].str.extract('(.*), (\d+): (.*)')",
2021.0,19.0,Pandas - Reshaping,```extract```,    df['Commentary'].str.extract('\[(.*?)\/(.*?)\-(.*,
2021.0,22.0,Pandas - Reshaping,```extract```,Answer'].str.strip().str.extract('(?P<Category>.*?)\:,
2021.0,25.0,Pandas - Reshaping,```extract```,'] = exclude['Name'].str.extract('(?:\w+) (.*?)(?: [X,
2021.0,29.0,Pandas - Reshaping,```extract```,"= venues['Location'].str.extract('(.*), (.*)')
final ",
2021.0,35.0,Pandas - Reshaping,```extract```,"a
    """"""
    df = s.str.extract(r'(\d+)(\S+)(?: x )?",
2021.0,38.0,Pandas - Reshaping,```extract```,"['Number in Series'].str.extract('(.*)/(.*)')


# avg",
2021.0,39.0,Pandas - Reshaping,```extract```,df['Data Parameter'].str.extract('(Actual|Target)? ?(,
2021.0,41.0,Pandas - Reshaping,```extract```,"        df['League'].str.extract('.*-(\d+)', expand=F",
2021.0,48.0,Pandas - Reshaping,```extract```,f_melt['Unnamed: 1'].str.extract('(.*?) ?(\(.*\))?$'),
2021.0,51.0,Pandas - Reshaping,```extract```,"] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expan",
2022.0,6.0,Pandas - Reshaping,```extract```,f_scores['Scrabble'].str.extract('(?P<Points>\d+) poi,
2022.0,7.0,Pandas - Reshaping,```extract```,a df_x:df_x['Goals'].str.extract('.*? (?P<goal_amt>\d,
2022.0,10.0,Pandas - Reshaping,```extract```,"tr.replace('\n', '').str.extract(pattern, expand=True",
2022.0,16.0,Pandas - Reshaping,```extract```,"_x: df_x['variable'].str.extract('(.*?)\_.*'),
      ",
2022.0,18.0,Pandas - Reshaping,```extract```,']] = df['variable'].str.extract('(\w+?)_+(\w{3}_\d+),
2022.0,20.0,Pandas - Reshaping,```extract```,"'] = df_out['Email'].str.extract('.*@(.*?)\..*')


#-",
2022.0,24.0,Pandas - Reshaping,```extract```,"                    .str.extract('([\d\,]+) km \(([\d",
2022.0,27.0,Pandas - Reshaping,```extract```,= df['Product Name'].str.extract('(.+?) - (\d+)(.*)'),
2022.0,29.0,Pandas - Reshaping,```extract```,"df_x['Product Name'].str.extract('(.*) - .*'))
      ",
2022.0,31.0,Pandas - Reshaping,```extract```,"tore['Product Name'].str.extract('(.*) - (.*)')
    
",
2022.0,33.0,Pandas - Reshaping,```extract```,"df_x['Product Name'].str.extract('(.*) - .*'))
      ",
2022.0,34.0,Pandas - Reshaping,```extract```,e']] = df['Unnamed'].str.extract('(.*)\s+-\s+(\d+)\s+,
2022.0,35.0,Pandas - Reshaping,```extract```,e']] = df['Unnamed'].str.extract('(.*)\s+-\s+(\d+)\s+,
,,Pandas - Reshaping,```extract```,a df_x: df_x['Week'].str.extract('.*\_(\d+)').astype(,
,,Pandas - Reshaping,```extract```,"] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expan",
,,Pandas - Reshaping,```extract```,"] = df['OrderID_in'].str.extract('(\D+)-(\d+)', expan",
2020.0,8.0,Pandas - Reshaping,```melt```,"                    .melt(id_vars=['Type', 'Me",
2020.0,9.0,Pandas - Reshaping,```melt```,"alues='--')\
       .melt(id_vars=['Poll', 'Da",
2020.0,11.0,Pandas - Reshaping,```melt```,"
df_soaps = df_boxes.melt(id_vars=['Order Numb",
2021.0,12.0,Pandas - Reshaping,```melt```,"s=['id'])\
         .melt(id_vars=key_cols)\
 ",
2021.0,14.0,Pandas - Reshaping,```melt```,"st
seats = seats_mtx.melt(id_vars='Row', value",
2021.0,15.0,Pandas - Reshaping,```melt```,"= menu.reset_index().melt(id_vars=['index'])

",
2021.0,16.0,Pandas - Reshaping,```melt```,"way Team']
df_m = df.melt(id_vars=[c for c in ",
2021.0,17.0,Pandas - Reshaping,```melt```,"nd 'Hours'
df_m = df.melt(id_vars=['Name, Age,",
2021.0,28.0,Pandas - Reshaping,```melt```," into rows
df_m = df.melt(id_vars=[c for c in ",
2021.0,34.0,Pandas - Reshaping,```melt```,"ales')\
            .melt(id_vars=['Store', 'E",
2021.0,36.0,Pandas - Reshaping,```melt```,"rows=2)\
           .melt(id_vars='Week', var_",
2021.0,43.0,Pandas - Reshaping,```melt```,"to columns
df_p = df.melt(id_vars=['Ticket ID'",
2021.0,47.0,Pandas - Reshaping,```melt```,"']
df_out = df_p_tot.melt(id_vars='name', valu",
2021.0,48.0,Pandas - Reshaping,```melt```,"       
df_melt = df.melt(id_vars=['Unnamed: 1",
2021.0,50.0,Pandas - Reshaping,```melt```,"'sheet'])\
         .melt(id_vars=['Salesperso",
2022.0,3.0,Pandas - Reshaping,```melt```,ents.merge(df_grades.melt(id_vars='Student ID',
2022.0,4.0,Pandas - Reshaping,```melt```,"ow='inner')\
       .melt(id_vars='Student ID'",
2022.0,5.0,Pandas - Reshaping,```melt```,"Input.csv')\
       .melt(id_vars='Student ID'",
2022.0,8.0,Pandas - Reshaping,```melt```,"s)\
                .melt(id_vars=['name', 'po",
2022.0,10.0,Pandas - Reshaping,```melt```,"
df_html_m = df_html.melt(id_vars='Char', valu",
2022.0,16.0,Pandas - Reshaping,```melt```,"dex()\
             .melt(id_vars='index')\
  ",
2022.0,18.0,Pandas - Reshaping,```melt```,"Input.csv')\
       .melt(id_vars='Region')


",
2022.0,21.0,Pandas - Reshaping,```melt```,"o rows
df_out = ( df.melt(id_vars=['Shop', 'De",
2022.0,29.0,Pandas - Reshaping,```melt```,"v"")
                .melt(id_vars='PRODUCT', v",
2022.0,35.0,Pandas - Reshaping,```melt```," data  
    ( df_all.melt(ignore_index=False, ",
,,Pandas - Reshaping,```melt```,"                    .melt(id_vars=['Type', 'Me",
,,Pandas - Reshaping,```melt```,"ow='inner')\
       .melt(id_vars='Student ID'",
2020.0,4.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","ttributes
df_p = df.pivot_table(index=['Response'], ",
2020.0,8.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","                   .pivot_table(index=['Type', 'Star",
2021.0,6.0,Pandas - Reshaping,"```pivot```, ```pivot_table```"," cols
dfAll = dfAll.pivot(index='variable', co",
2021.0,15.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","lumns
menu_f = menu.pivot_table(values='value', inde",
2021.0,18.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","ns
df_complete = df.pivot_table(index=id_vars, colum",
2021.0,23.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","st %)
df_pivot = df.pivot_table(values=['CustomerID'",
2021.0,31.0,Pandas - Reshaping,"```pivot```, ```pivot_table```"," per store'
pivot = pivot_table(df, values='Number o",
2021.0,39.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","ch batch
df_batch = pivot_table(df[df['Data Paramete",
2021.0,43.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","Status')\
         .pivot_table(values='Ticket ID', ",
2021.0,44.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","e values)
df_p = df.pivot_table(values='km', index='",
2022.0,16.0,Pandas - Reshaping,"```pivot```, ```pivot_table```",")'))\
             .pivot_table(index=['Guest', 'ind",
2022.0,18.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","columns
df_out = df.pivot_table(index=['Region', 'Bi",
2022.0,21.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","mns
df_out = df_out.pivot_table(values='value', inde",
2022.0,35.0,Pandas - Reshaping,"```pivot```, ```pivot_table```","sure')
            .pivot_table(index='Measure', val",
,,Pandas - Reshaping,"```pivot```, ```pivot_table```","                   .pivot_table(index=['Type', 'Star",
2019.0,25.0,Pandas - Reshaping,```stack```,"df_artist.ConcertID).stack()

# make ConcertID ",
2021.0,7.0,Pandas - Reshaping,```stack```,"or k in dfKeywordsIn.stack().str.split(', ').ex",
2022.0,7.0,Pandas - Reshaping,transpose ```T```,")\
                 .T


#----------------",
2019.0,25.0,Pandas - Selection/slicing,```drop_duplicates```,"
df_final = df_final.drop_duplicates(subset=['unique_key'",
2020.0,4.0,Pandas - Selection/slicing,```drop_duplicates```,"as the DoB
df_r = df.drop_duplicates(subset='Response')[[",
2020.0,17.0,Pandas - Selection/slicing,```drop_duplicates```,"Timestamp')
dfSurvey.drop_duplicates(subset=columnSubset,",
2021.0,5.0,Pandas - Selection/slicing,```drop_duplicates```,"ent']
dfDeduped = df.drop_duplicates(subset=outCols)[outC",
2021.0,7.0,Pandas - Selection/slicing,```drop_duplicates```,"ds into a list
dfAll.drop_duplicates(subset=['Product', '",
2021.0,10.0,Pandas - Selection/slicing,```drop_duplicates```,"nplace=True)
pokemon.drop_duplicates(inplace=True)

# rem",
2021.0,25.0,Pandas - Selection/slicing,```drop_duplicates```,"'Anime Appearances').drop_duplicates()


#---------------",
2021.0,29.0,Pandas - Selection/slicing,```drop_duplicates```,"tude', 'Longitude']].drop_duplicates(), 
                ",
2021.0,35.0,Pandas - Selection/slicing,```drop_duplicates```,"_excel(xl, 'Frames').drop_duplicates().rename(columns={'S",
2021.0,45.0,Pandas - Selection/slicing,```drop_duplicates```,"l = concat([dc[cols].drop_duplicates(subset=['Subject', '",
2022.0,10.0,Pandas - Selection/slicing,```drop_duplicates```,"))])\
              .drop_duplicates()
            
char_",
2022.0,22.0,Pandas - Selection/slicing,```drop_duplicates```,"])
                 .drop_duplicates()
         )


#----",
2022.0,36.0,Pandas - Selection/slicing,```drop_duplicates```,"ns='scheduled_date').drop_duplicates(), how='cross')
    ",
2019.0,25.0,Pandas - Selection/slicing,```drop```,"f_joined = df_joined.drop(['LongLats'], axis=1",
2020.0,3.0,Pandas - Selection/slicing,```drop```," how='left') \
     .drop(columns=['Team','Div",
2020.0,6.0,Pandas - Selection/slicing,```drop```,"an up columns
df_all.drop(columns=['Year','Sal",
2020.0,11.0,Pandas - Selection/slicing,```drop```,"      
    return df.drop(columns=['Order Size",
2021.0,8.0,Pandas - Selection/slicing,```drop```,"xes=['','_y'])
final.drop(columns=['Date_y'], ",
2021.0,10.0,Pandas - Selection/slicing,```drop```,"erent types 
pokemon.drop(columns=['Type'], in",
2021.0,12.0,Pandas - Selection/slicing,```drop```,"s=['na'])\
         .drop(columns=['id'])\
   ",
2021.0,14.0,Pandas - Selection/slicing,```drop```,").astype(int)
planes.drop(columns=['Business C",
2021.0,15.0,Pandas - Selection/slicing,```drop```,"dex()\
             .drop(columns='index')


#",
2021.0,16.0,Pandas - Selection/slicing,```drop```,"['Position']
total_2.drop(columns=['Position_1",
2021.0,17.0,Pandas - Selection/slicing,```drop```,"ecessary fields
df_m.drop(columns=['Name, Age,",
2021.0,25.0,Pandas - Selection/slicing,```drop```,"utions for gen1
evol.drop(columns=['Evolution ",
2021.0,35.0,Pandas - Selection/slicing,```drop```,"=['', '_f'])\
      .drop('key', 1)


# find v",
2021.0,43.0,Pandas - Selection/slicing,```drop```,"_value=0)\
         .drop(columns=['In Progres",
2021.0,45.0,Pandas - Selection/slicing,```drop```,"00:00').str[0:8])
df.drop(columns=['date', 'Se",
2021.0,46.0,Pandas - Selection/slicing,```drop```,"eft')\
             .drop(columns=['sheet_name",
2021.0,48.0,Pandas - Selection/slicing,```drop```,"xtra columns
df_melt.drop(columns=['Unnamed: 1",
2021.0,50.0,Pandas - Selection/slicing,```drop```,"[df['Date'].notna()].drop(columns=['RowID', 'T",
2022.0,7.0,Pandas - Selection/slicing,```drop```,"\
                  .drop(columns='AgentID')\
",
2022.0,8.0,Pandas - Selection/slicing,```drop```,"')\
                .drop(columns=['name'])\
 ",
2022.0,9.0,Pandas - Selection/slicing,```drop```,"1))\
               .drop(columns='Order_Lines",
2022.0,10.0,Pandas - Selection/slicing,```drop```,"----------------

df.drop(columns='DownloadDat",
2022.0,14.0,Pandas - Selection/slicing,```drop```,"Original Rank'})\
  .drop(columns=['F', 'F_ran",
2022.0,16.0,Pandas - Selection/slicing,```drop```,"on'])\
             .drop(columns=['Selection'",
2022.0,17.0,Pandas - Selection/slicing,```drop```,"eft')\
             .drop(columns='Month')\
  ",
2022.0,22.0,Pandas - Selection/slicing,```drop```,"')
                 .drop(columns=['end_time']",
2022.0,24.0,Pandas - Selection/slicing,```drop```,"                    .drop(columns=['City_x', '",
2022.0,27.0,Pandas - Selection/slicing,```drop```,"ype']==t]
          .drop(columns=['Product Ty",
2022.0,29.0,Pandas - Selection/slicing,```drop```,"00)
                .drop(columns=[""Sales Targ",
2022.0,30.0,Pandas - Selection/slicing,```drop```,")
                  .drop(columns='Store')
   ",
2022.0,35.0,Pandas - Selection/slicing,```drop```,"se c)
              .drop(columns=['Units', 'T",
2022.0,36.0,Pandas - Selection/slicing,```drop```,           .merge(df.drop(columns='scheduled_d,
2020.0,8.0,Pandas - Selection/slicing,```dropna```,"                    .dropna(how='all', axis=1)\
",
2020.0,9.0,Pandas - Selection/slicing,```dropna```,"l Results')\
       .dropna(subset=['Poll Result",
2020.0,11.0,Pandas - Selection/slicing,```dropna```,"
                   .dropna(subset=['Box_Number'",
2021.0,8.0,Pandas - Selection/slicing,```dropna```,"direction='forward').dropna()


# join customer ",
2021.0,15.0,Pandas - Selection/slicing,```dropna```," menu['field'])
menu.dropna(subset=['value'], in",
2021.0,16.0,Pandas - Selection/slicing,```dropna```,"ayfirst=True)\
     .dropna(subset=['Result'])

",
2021.0,27.0,Pandas - Selection/slicing,```dropna```,", value_name='prob').dropna()
prob['prob'] = pro",
2021.0,48.0,Pandas - Selection/slicing,```dropna```," df = read_excel(xl).dropna(axis=1, how='all').d",
2022.0,10.0,Pandas - Selection/slicing,```dropna```,"'Numeric', 'Named']).dropna()
df_html_m = pd.con",
2022.0,16.0,Pandas - Selection/slicing,```dropna```,"\
                  .dropna(how='all', axis=1)
 ",
2022.0,21.0,Pandas - Selection/slicing,```dropna```,"alue')
             .dropna() 
             .ass",
,,Pandas - Selection/slicing,```dropna```,"                    .dropna(how='all', axis=1)\
",
2021.0,24.0,Pandas - Selection/slicing,"```idmax```, ```idmin```","[df_dates.iloc[:, 1].idxmax()]['Date']
print('D",
2021.0,30.0,Pandas - Selection/slicing,"```idmax```, ```idmin```","'From')['To'].size().idxmax()


# if every trip",
2022.0,35.0,Pandas - Selection/slicing,"```idmax```, ```idmin```","upby('Year')['Mins'].idxmax()]
                ",
2020.0,9.0,Pandas - Selection/slicing,```query```," Results'])\
       .query(""~Poll.str.contains(",
2022.0,8.0,Pandas - Selection/slicing,```query```,"')\
                .query('Stage_2==Stage_2 or",
2022.0,9.0,Pandas - Selection/slicing,```query```,"s')\
               .query('Year >= First_Purch",
2022.0,14.0,Pandas - Selection/slicing,```query```,"ns=renames)\
       .query(""(Series.str[0] != '",
2022.0,21.0,Pandas - Selection/slicing,```query```,"ill())
             .query(f""Breakdown == {str(",
2022.0,22.0,Pandas - Selection/slicing,```query```,"False)
             .query(""section == 'Gamepla",
2020.0,1.0,Pandas - Selection/slicing,slicing a DataFrame,"[dfSubtotal, 
      df[df['Level']==maxLevel].",1.0
2020.0,3.0,Pandas - Selection/slicing,slicing a DataFrame,"al', 'PTS.1']]
df = df[df['PTS'].notna()]
df['",1.0
2020.0,4.0,Pandas - Selection/slicing,slicing a DataFrame,"Other'))
df_p = df_p.loc[df_p['Result'] != 'O",1.0
2020.0,8.0,Pandas - Selection/slicing,slicing a DataFrame,"tables
header_row = df_budget_in[df_budget_in['Week']=='Type'].ind",1.0
2020.0,9.0,Pandas - Selection/slicing,slicing a DataFrame,"ed voters
df_rv = df.loc[df['Sample Type']=='",1.0
2020.0,12.0,Pandas - Selection/slicing,slicing a DataFrame,"n properly
df_pct = df_pct[df_pct['Percentage of Sales",1.0
2021.0,1.0,Pandas - Selection/slicing,slicing a DataFrame," test values
df = df.iloc[10:]

# output the d",1.0
2021.0,7.0,Pandas - Selection/slicing,slicing a DataFrame,"------------------

dfItems[dfItems['Contains'] == ''].t",1.0
2021.0,8.0,Pandas - Selection/slicing,slicing a DataFrame,"nce
session_start = choices[choices['Song Order']==1][['",1.0
2021.0,12.0,Pandas - Selection/slicing,slicing a DataFrame," null values
df = df.loc[(df['Series-Measure'",1.0
2021.0,13.0,Pandas - Selection/slicing,slicing a DataFrame," the dataset
df = df.loc[(df['Position'] != '",1.0
2021.0,14.0,Pandas - Selection/slicing,slicing a DataFrame,"lights = flight_list.iloc[:, 0].str.replace('[",1.0
2021.0,15.0,Pandas - Selection/slicing,slicing a DataFrame,"r', ascending=False).iloc[0:1, :]
out2.rename(",1.0
2021.0,16.0,Pandas - Selection/slicing,slicing a DataFrame,"t_index()
total_2 = df_m[df_m['big_6'] == 0].group",1.0
2021.0,17.0,Pandas - Selection/slicing,slicing a DataFrame,"e total rows
df = df.loc[df['Name, Age, Area ",1.0
2021.0,18.0,Pandas - Selection/slicing,slicing a DataFrame,"eset_index()
if len(df_check[df_check['count'] > 1]) > 0:
",1.0
2021.0,19.0,Pandas - Selection/slicing,slicing a DataFrame,"for misjoins
if len(df[df['Project'].isna()]) ",1.0
2021.0,20.0,Pandas - Selection/slicing,slicing a DataFrame,df_outliers = df_all.loc[df_all['Outlier?']==,1.0
2021.0,21.0,Pandas - Selection/slicing,slicing a DataFrame,"ce', 'Destination']
df[df['Variance Rank by De",1.0
2021.0,22.0,Pandas - Selection/slicing,slicing a DataFrame,"eset_index()
if len(check[check['Names'] < 1]) > 0:
",1.0
2021.0,23.0,Pandas - Selection/slicing,slicing a DataFrame,", 'NPS', 'Z-Score']
df_pivot[df_pivot['Airline']=='Prep Ai",1.0
2021.0,24.0,Pandas - Selection/slicing,slicing a DataFrame,"
max_date = df_dates.iloc[df_dates.iloc[:, 1].",1.0
2021.0,25.0,Pandas - Selection/slicing,slicing a DataFrame,"not used
gen1 = gen1.loc[gen1['#'].notna()]

",1.0
2021.0,27.0,Pandas - Selection/slicing,slicing a DataFrame,"
    prob_sub = prob.loc[(prob['pick']==pick+",1.0
2021.0,28.0,Pandas - Selection/slicing,slicing a DataFrame,"ype(int)
out1 = out1.loc[out1['Shootouts'] > ",1.0
2021.0,31.0,Pandas - Selection/slicing,slicing a DataFrame,"rer' records
df = df.loc[df['Status'] != 'Ret",1.0
2021.0,33.0,Pandas - Selection/slicing,slicing a DataFrame," date
fulfilled = df.loc[(df['Reporting Date'",1.0
2021.0,34.0,Pandas - Selection/slicing,slicing a DataFrame,"et
summary = summary.loc[summary['Avg monthly",1.0
2021.0,35.0,Pandas - Selection/slicing,slicing a DataFrame,"a difference
df = df.loc[(df['Max Side'] <= d",1.0
2021.0,36.0,Pandas - Selection/slicing,slicing a DataFrame,'Index Peak'] = df_t.iloc[df['idxmax']]['index,1.0
2021.0,39.0,Pandas - Selection/slicing,slicing a DataFrame,batch = pivot_table(df[df['Data Parameter'].is,1.0
2021.0,40.0,Pandas - Selection/slicing,slicing a DataFrame,"ols=usecols)
df = df.loc[df['Animal Type'].is",1.0
2021.0,44.0,Pandas - Selection/slicing,slicing a DataFrame,"ith no activities: {df_p[df_p[""Activities per day""",1.0
2021.0,45.0,Pandas - Selection/slicing,slicing a DataFrame,"', '_2'])
dc.drop(dc.loc[dc['Attendee ID'] ==",1.0
2021.0,48.0,Pandas - Selection/slicing,slicing a DataFrame," }
df_melt = df_melt.loc[df_melt['True Value'",1.0
2021.0,50.0,Pandas - Selection/slicing,slicing a DataFrame,"ingle column
df_m = df[df['Date'].notna()].dro",1.0
2021.0,51.0,Pandas - Selection/slicing,slicing a DataFrame,"ity']

print_errors(df[df['Sales'].isna()][['O",1.0
2022.0,5.0,Pandas - Selection/slicing,slicing a DataFrame," grade
avg_with_a = df[df['Grade']=='A']['Avg ",1.0
2022.0,8.0,Pandas - Selection/slicing,slicing a DataFrame,"m evolving?')
print(df_out[df_out['combat_power_increa",1.0
2022.0,10.0,Pandas - Selection/slicing,slicing a DataFrame,pd.concat([df_html_m.loc[df_html_m['variable',1.0
2022.0,13.0,Pandas - Selection/slicing,slicing a DataFrame,"glish
filter_rows = df[df['Running_Pct_Total_S",1.0
2022.0,19.0,Pandas - Selection/slicing,slicing a DataFrame," 1: correct sizes
( df_out[df_out['Sold Size'] == df_o",1.0
2022.0,20.0,Pandas - Selection/slicing,slicing a DataFrame,"n data
df_reg_onl = df_reg[df_reg['Online/In Person']=",1.0
2022.0,27.0,Pandas - Selection/slicing,slicing a DataFrame,"'].unique(): 
    ( df_out[df_out['Product Type']==t]
",1.0
2022.0,28.0,Pandas - Selection/slicing,slicing a DataFrame,"
df_missing = df_all.loc[mask, ['date']]


# ",1.0
2022.0,31.0,Pandas - Selection/slicing,slicing a DataFrame,"re
    df_store = df.loc[(df['Store Name']==s",1.0
2022.0,33.0,Pandas - Selection/slicing,slicing a DataFrame,"nd store
df_out = ( df_merge[df_merge['time_diff_min'] >= ",1.0
2022.0,34.0,Pandas - Selection/slicing,slicing a DataFrame,"rned
    df_out = df.loc[(df['Coach'] == coac",1.0
2022.0,35.0,Pandas - Selection/slicing,slicing a DataFrame,            df_coach.iloc[df_coach.groupby('Ye,1.0
,,Pandas - Selection/slicing,slicing a DataFrame,"tables
header_row = df_budget_in[df_budget_in['Week']=='Type'].ind",1.0
2020.0,3.0,Rounding,Basic,"+ df_summary['L'])).round(3)
df_summary['Conf'",
2020.0,6.0,Rounding,Basic," (GBP)'] =    \
    round(df_all['Sales Value'",
2020.0,12.0,Rounding,Basic,"table
df['Sales'] = round(df['Total Scent Sale",
2021.0,2.0,Rounding,Basic,"d per Brand, Type'].round(1)


# calculate Day",
2021.0,6.0,Rounding,Basic,"g_Money_per_Event'].round(0).astype(int)

# ra",
2021.0,8.0,Rounding,Basic," = final['Date'].dt.round('1s')
final.to_csv('",
2021.0,11.0,Rounding,Basic,pes.apply(lambda r: round(float(r['ml']) * r[',
2021.0,13.0,Rounding,Basic,Play Goals/Game'] = round(df_sum['Open Play Go,
2021.0,14.0,Rounding,Basic,"1['Avg per Flight'].round(2)
q1['Rank'] = q1['",
2021.0,28.0,Rounding,Basic,'Shootout Win %'] = round(out1['Shootouts'] / ,
2021.0,30.0,Rounding,Basic,efault position' : [round(df['floors_from_dp'],
2021.0,32.0,Rounding,Basic,"sum'))\
           .round(0).astype(int)\
    ",
2021.0,34.0,Rounding,Basic,nths target met'] = round(summary['% of months,
2021.0,36.0,Rounding,Basic,"dex', 'Avg_index']].round(1)

df['Index Peak']",
2021.0,38.0,Rounding,Basic,"['Trilogy Average'].round(1)


#--------------",
2021.0,40.0,Rounding,Basic,"alize='index')*100).round(1)\
        .reset_i",
2021.0,42.0,Rounding,Basic,"nto fund raising']).round(9)


# workout the w",
2021.0,49.0,Rounding,Basic,"t['months_worked']).round(2)
df_out['Yearly Bo",
2021.0,51.0,Rounding,Basic,"ust['Order Lines']).round(2)
df_cust['Customer",
2022.0,3.0,Rounding,Basic,": df_x['Avg_Score'].round(1))\
           .ren",
2022.0,4.0,Rounding,Basic,"x['Trips_per_day']).round(2))\
           .ren",
2022.0,6.0,Rounding,Basic,"'Frequency'].sum()).round(2).apply(d.Decimal)
",
2022.0,7.0,Rounding,Basic,"l['Calls Offered']).round(3),
                ",
2022.0,12.0,Rounding,Basic,"dianHourlyPercent'].round(2).abs()\
          ",
2022.0,13.0,Rounding,Basic,_Total=lambda df_x: round(df_x['Sales'] / df_x,
2022.0,20.0,Rounding,Basic,"ered_Count'] * 100).round(2)


# from the Emai",
2022.0,31.0,Rounding,Basic,"f_out['Sale Value'].round(-1)

    # output th",
2022.0,33.0,Rounding,Basic,"e_diff_min'].mean().round(1)
               .r",
2022.0,35.0,Rounding,Basic,"agg['Total_Rides']).round(1)
    df_agg['Avg. ",
,,Rounding,Basic,"ust['Order Lines']).round(2)


# create the Pr",
,,Rounding,Basic,"ust['Order Lines']).round(2)
df_cust['Customer",
,,Rounding,Basic,"x['Trips_per_day']).round(2))


#-------------",
2021.0,9.0,Rounding,Round Half Up,"port read_csv


def round_half_up(n, decimals=0):
   ",
2019.0,25.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","l['unique_key'].str.lower()
#df_final.head()

",
2020.0,4.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)",new_dict.update({ k.lower() : k.title() for k ,
2020.0,8.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","x: df_x['Type'].str.lower())\
                ",
2020.0,9.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","time')
p.add_layout(Title(text='Data from: rea",
2020.0,12.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","lookup['Scent'].str.upper().replace(' +', '', ",
2021.0,7.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","s'] = [', '.join([k.lower() for k in dfKeyword",
2021.0,19.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","-Project Code'].str.lower().replace('ops', 'op",
2021.0,22.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)", names['Name'] if s.lower().startswith(n.lower,
2021.0,28.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)",f_temp.columns = [c.lower().strip() for c in d,
2021.0,29.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","place('\.', '').str.title().replace(sports_map",
2021.0,41.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)",f c == 'POS' else c.title() for c in df.column,
2021.0,44.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","e(df['Measure'].str.lower() == 'min', df['Valu",
2021.0,51.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","       return x.str.lower()
    else:
        ",
2021.0,52.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","t'].str.strip().str.lower()


# find keyword m",
2022.0,6.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)",x: df_x['Tile'].str.lower().str.extract(r'\s*(,
2022.0,10.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","ply(lambda x: x.str.lower()),
                ",
2022.0,20.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","ine/In Person'].str.lower().str[0]=='o',
     ",
2022.0,29.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","rget['PRODUCT'].str.title()
df_target['Store N",
2022.0,34.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)",put(f'\n{value_name.title()} list:\n{options_s,
2022.0,35.0,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","e'].str.strip().str.title()
    df['Year'] = -",
,,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","x: df_x['Type'].str.lower())\
                ",
,,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","       return x.str.lower()
    else:
        ",
,,String Functions,"Changing case (```upper```, ```lower```, ```title```, etc.)","       return x.str.lower()
    else:
        ",
2020.0,1.0,String Functions,Count matches,"'] = df['Hierarchy'].str.count('\.')
maxLevel = df[",
2019.0,4.0,String Functions,String slicing,"8 instead of 2019
df['True Date'] = where(df['DATE'].dt.month > 6, df['DATE'] - DateOffset(years=1), df['DATE'])


# split the ""Hi-"" categories up so player and value is separate
for c in [c for c in df.columns if c.startswith('HI ')]:
    df[[f'{c} - Player', f'{c} - Value']] = df[f'{c}'].str.e",1.0
2019.0,25.0,String Functions,String slicing,"tract('(?P<Latitude>[\d\.\-]+), (?P<Longitude>.*$)')
#df_latlon_split.head()

# add the split columns to the joined table, and drop the old column
df_joined = pd.merge(df_joined, df_latlon_split, left_index=True, right_index=True)

df_joined = df_joined.drop(['LongLats'], axis=1)
#df_joined.count()


#------------------------------------------------------------------------------
# Split the concert field
#------------------------------------------------------------------------------

# replace null values in the concert field with empty string
df_joined['Concert'] = df_joined['Concert'].fillna('')

# create a new dataframe with the concert ID and concert name
#    only including concert names with a slash
df_artist = df_joined[['ConcertID', 'Concert']][df_joined['Concert'].str.contains('/')]
8#df_artist.head()
#df_artist.count()

# create a new dataframe with the artists split into rows, using ConcertID
# as an index
df_artist = pd.DataFrame(df_artist.Concert.str.split(' / ').tolist(), \
                                index=df_artist.ConcertID).stack()

# make ConcertID a column and rename the fields
df_artist = df_artist.reset_index([0, 'ConcertID'])
df_artist.columns = ['ConcertID', 'Fellow Artists']
#df_artist.head()
#df_artist.count()

# remove Ben Howard, Ed Sheeran, and blanks
df_artist = df_artist[~df_artist['Fellow Artists'].isin(['Ben Howard', 'Ed Sheeran', ''])]
#df_artist.count()

# merge the new table with the main table into a new table
# the solution expects at least one row for Ben Howard or Ed Sheeran, plus
# rows for any Fellow Artists
df_artist = pd.merge(df_joined, df_artist, on='ConcertID')
#df_artist.count()

# union the new table with the primary table
df_joined['Fellow Artists'] = ''
df_final = pd.concat([df_joined, df_artist], ignore_index=True)
#df_final.count()


#------------------------------------------------------------------------------
# Remove duplicates
#------------------------------------------------------------------------------

# create a key field to identify unique records
df_final['unique_key'] = df_final['Artist'].fillna('') + '|' \
                          + df_final['Concert Date'].dt.strftime('%Y-%m-%d') + '|' \
                          + df_final['Concert'].fillna('') + '|' \
                          + df_final['Location'].fillna('') + '|' \
                          + df_final['Venue'].fillna('') + '|' \
                          + df_final['Fellow Artists'].fillna('')

df_final['unique_key'] = df_final['unique_key'].str.lower()
#df_final.head()

# delete duplicates based on the unique key
df_final = df_final.drop_duplicates(subset=['unique_key'], keep='first')
#df_final.count()

# remove the key
df_final = df_final.drop(columns=['unique_key', 'ConcertID'])
#df_final.count()


#------------------------------------------------------------------------------
# Add the home long/lat for each artist
#------------------------------------------------------------------------------

# create a dataframe of the home locations
df_hometowns = pd.DataFrame({
    'Artist':['Ed Sheeran', 'Ben Howard'],
    'Hometown':['F",1.0
2020.0,1.0,String Functions,String slicing,"nulls with zeroes
df['Profit'].fillna(0, inplace=True)

# extract the hierarchy from the Item and copy it
df['Hierarchy'] = df['Item'].str.extract('([\d\.]+?)\.? .*')
df['Hierarchy2'] = df['Hierarchy']
df['Level'] = df['Hierarchy'].str.count('\.')
maxLevel = df['Level'].max()

# iterate through the hierarchy levels, creating subtotals at each level
dfSubtotal = None
for i in range(maxLevel, 0, -1):
    # remove a layer of hierarchy
    df['Hierarchy2'] = df['Hierarchy2'].",1.0
2020.0,2.0,String Functions,String slicing,"om d/m/y to m/d/y
df['Date_out'] = [dt.strftime(dt.strptime(d, '%d/%m/%y'), '%m/%d/%Y') for d in df['Date']]


# clean time field:

# keep numeric characters and pad with zero
df['time_part'] = df['Time'].str.re",1.0
2020.0,3.0,String Functions,String slicing,"ets
result_sheets = [s for s in xl.sheet_names if 'Results' in s]
df = None
for sheet in result_sheets:
    df = concat([df, xl.parse(sheet)])


# keep necessary",1.0
2020.0,4.0,String Functions,String slicing,"t.csv', parse_dates=['DoB'], dayfirst=True)
df_q = read_csv(r'.\inputs\Store Survey Results - Question Sheet.csv')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean the Country and Store names
#df['Country'].unique()
#df['Store'].unique()

country_dict = { 'England' : ['3ngland', 'egland', 'eggland', 'ingland'],
                 '",1.0
2020.0,5.0,String Functions,String slicing," at halftime
df = df[(df['HTf'] != '-') & (df['HTa'] != '-')]


# determine standard competition rank based on Diff (larger = better)
df['Rank'] = df['Diff'].rank(method='min', ascending=False)


# summarize by venue (count of games, best rank, worst rank, avg rank)
df_sum = df.groupby(['Venue'], as_index=False).agg( 
                 { 'Rank' : [('Count', 'count'),
                             ('Best','min'),
                             ('Worst', 'max'),
                             ('Average', 'mean')]
                 }
",1.0
2020.0,6.0,String Functions,String slicing,"e year/week
df_rates['Rate'] =    \
    df_rates['British Pound to US Dollar'].str.extract('\= ([\d\.]+)', expand=False).astype(float)
df_rates['Week'] = [int(d.strftime('%U')) + 1 for d in df_rates['Date']]
df_rates['Year'] = [d.year for d in df_rates['Date']]


# find the min and max rate for each week
df_rates_sum = df_rates.groupby(['Year','Week'], as_index=False).agg( 
                 { 'Rate' : [('Worst','min'),
                             ('Best', 'max')] }
               )
",1.0
2020.0,7.0,String Functions,String slicing," leave month
df_curr['Salary'] = df_curr['Salary'].str.replace(r""[\D]"",'')  
df_curr['Salary'] = df_curr['Salary'].astype(int)
df_curr['Leave Date'] = '3/1/2020'


# concatenate (union) the current and left employees
df_all = concat([df_curr, df_left], sort=False)


# convert the join/leave dates to the first of the month for joining
df_all['Join Month'] = [parse(d) for d in df_all['Join Date'].str.replace('/\d+/', '/1/')]
df_all['Leave Month'] = [parse(d) for d in df_all['Leave Date'].str.replace('/\d+/', '/1/')]


# convert the date inputs to the first of the month for joining
df_dates['Month'] = [parse(d) + relativedelta(day=1) for d in df_dates['Month']]


# add fields for join, then merge the dataframes to create a cross join
df_all['Link'] = 1
df_dates['Link'] = 1
df_all = df_all.merge(df_dates, left_on='Link', right_on='Link')


# remove report dates outside of the employee's range
date_in_range = (df_all['Month'] >= df_all['Join Month']) \
                 & (df_all['Month'] < df_all['Leave Month'])
df_all = df_all[date_in_range]


# summarize
df_summary = df_all.groupby('Month', as_index=False).agg( 
        { 'Employee ID' : 'count' , 
         'Salary': 'sum' }
        )

df_summary.columns = ['Month', 'Current Employees', 'Total Monthly Salary']
df_summary['Avg Sal",1.0
2020.0,8.0,String Functions,String slicing,"_weekly = pd.concat([pd.read_excel(xl, s)\
                             .assign(Week=int(s.replace('Week ', '')))\
                             .rename(columns=renames)
                           for s in xl.sheet_names if 'Week' in s])\
                  .assign(Type=lambda df_x: df_x['Type'].str.lower())\
     ",1.0
2020.0,9.0,String Functions,String slicing,"      .melt(id_vars=['Poll', 'Date', 'Sample'], var_name='Candidate', value_name='Poll Results')\
       .dropna(subset=['Poll Results'])\
       .query(""~Poll.str.contains('Average')"", engine='python')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean up end date
df['End Date'] = pd.to_datetime(df['Date'].str.extract('.*- (\d+/\d+)', expand=False) + '/2020')
df['End Date'] = where(df['End Date'].dt.month >= 7, 
                       df['End Date'] + pd.DateOffset(years=-1), 
                       df['End Date'])


# form a Rank (modified competition) of the candidates per Poll based on their results
df['Rank'] = df.groupby(['Poll', 'End Date', 'Sample'])['Poll Results'].rank(method='max', ascending=False)\
               .astype(int)
      
        
# difference in poll results between first and second
df['Spread from 1st to 2nd Place'] = \
    df.groupby(['Poll', 'End Date', 'Sample'], as_index=False)['Poll Results']\
      .transform(lambda x: x.max() - x.nlargest(2).min())
               

# rename sample types
sample_map = {'.*RV' : 'Registered Voter', '.*LV' : 'Likely Voter', nan : 'Unknown'}
df['Sample Type'] = df['Sample'].repl",1.0
2020.0,11.0,String Functions,String slicing,"er=lambda df_x: df_x['Order Size'])
    size_list.sort(reverse=True)
    
    for i, s in enumerate(size_list):
        if i < len(size_list) - 1:
            df[f'Boxes of {s}'] = df['Remainder'] /",1.0
2020.0,12.0,String Functions,String slicing,"inplace=True)
df_tot['Scent_join'] = df_tot['Scent_orig'].str.replace(' ', '')


# Percentage of Sales:
# get the year/week
# concatenate the Product ID and Size for joining to the lookup table
df_pct = read_excel(in_file, sheet_name='Percentage of Sales')
#df_pct.dtypes    # make sure numbers read in properly
df_pct = df_pct[df_pct['Percentage of Sales'] != 0]
df_pct['Produ",1.0
2020.0,17.0,String Functions,String slicing,"viceCount = dfSurvey[['Timestamp', dfSurvey.columns[2]]].copy()
dfDeviceCount.rename(columns={dfSurvey.columns[2] : 'survey_devices'}, inplace=True)
dfDeviceCount['join_field'] = 1
dfDevices['join",1.0
2020.0,32.0,String Functions,String slicing," 
          columns=['Store Manager', 'Store', 'Sales Target'])



#------------------------------------------------------------------------------
# Method 2: merge_asof (non-equijoin)
#------------------------------------------------------------------------------

# import and fill the data
df = read_excel('.\\inputs\\Copy Down Data Challenge.xlsx')

df_managers = df[['Row ID', 'Store Manager']][df['Store Manager'",1.0
2021.0,1.0,String Functions,String slicing,"        parse_dates=['Date'], dayfirst=True)

# split the 'Store-Bike' field into 'Store' and 'Bike'
df[['Store','Bike']] = df['Store - Bike'].str.split(pat=' - ', expand=True)

# clean up the 'Bike' field to leave just three values in the 'Bike' field 
# (Mountain, Gravel, Road)
remap = { 'Graval' : 'Gravel',
          'Gravle' : 'Gravel',
          'Mountaen' : 'Mountain',
          'Rood' : 'Road',
          'Rowd' : 'Road' }
df['Bike'].replace(remap, inpl",1.0
2021.0,2.0,String Functions,String slicing,"        parse_dates=['Order Date', 'Shipping Date'], dayfirst=True)


# clean up the Model field to leave only the letters to represent the 
# Brand of the bike
df['Brand'] = df['Model'].str.replace('[^A-Z]+', '')


# work out the Order Value using Value per Bike and Quantity
df['Order Value'] = df['Value per Bike'] * df['Quantity']


# aggregate Value per Bike, Order Value and Quantity by Brand and Bike Type 
# note, the avg value sold is the straight average and not weighted
df_brand = df.groupby(['Brand', 'Bike Type']).agg({ 'Quantity' : ['sum'],
                  ",1.0
2021.0,3.0,String Functions,String slicing,"rse(sheet)
    dfNew['Store'] = sheet
    dfIn = concat([dfIn, dfNew])


# pivot the columns and rename the new value Products Sold
df = melt(dfIn, id_vars=['Date','Store'], var_name='Customer-Product', value_name='Products Sold')

# split the customer type and product
df[['Customer Type','Product']] = df['Customer-Product'].str.split(pat=' - ', expand=True)

# convert date to quarter
df['Quarter'] = df['Date'].dt.quarter

# aggregation #1: products sold by product, quarter
groupFields = ['Product', 'Quarter']
agg1 = df.groupby(g",1.0
2021.0,4.0,String Functions,String slicing,"= None
for sheet in [s for s in xl.sheet_names if s != 'Targets']:
    dfNew = xl.parse(sheet)
    dfNew['Store'] = sheet
    dfIn = ",1.0
2021.0,6.0,String Functions,String slicing," weighted average
df['avg_money_by_person'] = df['MONEY'] / df['EVENTS']

# aggregate by tour
dfAgg = df.groupby('TOUR').agg(Total_Prize_Money = ('MONEY', 'sum'),
                                 Number_of_Players = ('PLAYER NAME', 'count'),
                                 Number_of_Events = ('EVENTS', 'sum'),
                                 Avg_Money_per_Event = ('avg_money_by_person', 'mean')).reset_index()
dfAgg['Avg_Money_per_Event'] = dfAgg['Avg_Money_per_Event'].round(0).astype(int)

# rank
df['overall_rank'] = df['MONEY'].rank(method='first', ascending=False)
df['tour_rank'] = df.groupby('TOUR')['MONEY'].rank(method='first', ascending=False)
df['rank_diff'] = df['overall_rank'] - df['tour_rank']

dfRankAgg = df.groupby('TOUR').agg(Avg_Difference_in_Ranking = ('rank_diff', 'mean')).reset_index()

# join the dataframes
dfAll = dfAgg.merge(dfRankAgg, how='left', on='TOUR')

# melt fieldnames into rows
dfAll = melt(dfAll, id_vars = ['TOUR'])

# pivot tour into cols
dfAll = dfAll.pivot(index='variable', columns='TOUR', values='value').reset_index()
dfAll['Difference between tours'] = dfAll['LPGA'] - dfAll['PGA']

# rename cols and remove underscores from measure names
dfAll.rename( columns = { 'variable' : 'Measure'}, inplace=True)
dfAll['Measure'] = dfAll['Measure'].",1.0
2021.0,7.0,String Functions,String slicing,"mbers
keywordList = ['E' + k if k.isnumeric() else k 
               for k in dfKeywordsIn.stack().str.split(', ').explode().reset_index(drop=True)]

    
    
#--------------------------------------------------------------------------------
# method 1: list comprehension
#    The challenge specifically said to append the keywords (i.e. a cartesian
#    product, method #2 below). In practice, I would accomplish this using a
#    list comprehension.
#--------------------------------------------------------------------------------

# parse the keywords and numbers, stack the names and numbers, add E to the numbers
dfKeywords = ['E' + k if k.isnumeric() else k 
              for k in dfKeywordsIn.stack().str.split(', ').explode().reset_index(drop=True)]
    
dfItems['Conta",1.0
2021.0,8.0,String Functions,String slicing,"tomer IDs 
customers['Customer ID'] = [convert_id(c) for c in customers['Customer ID']]


# determine the session (if the time between songs is >= 59 minutes, increment the session)
choices['Session #'] = choices['Date'].diff(1).dt.total_seconds().ge(59*60).cumsum() + 1


# number the songs in order for each session
choices['Song Order'] = choices.groupby('Session #')['Date'].rank('dense', ascending=True).astype(int)


# join the customer to the closest session after their entry time, 10 min tolerance
session_start = choices[choices['Song Order']==1][['Date','Session #']]

customer_sessions = merge_asof(customers, session_start, left_on='Entry Time', right_on='Date', 
                               tolerance=Timedelta(minutes=10), direction='forward').dropna()


# join customer sessions to song list
final = merge(choices, customer_sessions, how='left', on=['Session #'], suffixes=['','_y'])
final.drop(columns=['Date_y'], inplace=True)


#--------------------------------------------------------------------------------
# output the file
#--------------------------------------------------------------------------------

final['Date'] = final['Date'].dt.round('1s')
final.to_csv('.\\outputs\\output-2021-08.csv', index=False, date_format='%d/%m/%Y %H:%M:%S',
             columns=['Session #', 'Customer ID', 'Song Order', 'Date', 'Artist', 'Song'])",1.0
2021.0,10.0,String Functions,String slicing,"me == evolution_dict[p_name]: 
        return p_name
    else:
        return get_evolution_group(evolution_dict[p_name])
    

#-----------",1.0
2021.0,11.0,String Functions,String slicing,"
recipes = cocktails['Recipe (ml)'].str.split('; ').explode().str.extract(regex_str, expand=True)

# convert ingredient prices to pounds
src['price_pounds'] = [p / conv_dict[c] for c,p in zip(src['Currency'], src['Price'])] 

# calculate the cocktail cost in pounds
cols = ['Ingredient','ml per Bottle', 'price_pounds']
recipes = recipes.reset_index().merge(src[cols], on='Ingredient', how='left')

recipes['Cost'] = recipes.apply(lambda r: round(float(r['ml']) * r['price_pounds'",1.0
2021.0,12.0,String Functions,String slicing,"stack it
key_cols = ['Series-Measure', 'Hierarchy-Breakdown', 'Unit-Detail']

df = read_csv(r'.\inputs\Tourism Input.csv', na_values=['na'])\
         .drop(columns=['id'])\
         .melt(id_vars=key_cols)\
         .rename(columns={'variable':'Month'})

df['Month'] = to_datetime(df['M",1.0
2021.0,13.0,String Functions,String slicing,"------

df = concat([read_csv(r'.\inputs\\' + f) for f in listdir(r'.\inputs')])
#df.info(verbose=True)    # investigate the list of fields


# --------------------------------------------------------------------------------------------------
# prep data
# --------------------------------------------------------------------------------------------------

# remove the space after the player name
df['Name'] = df['Name'].str.strip()

# remove all goalkeepers and appearances = 0
#df['Position'].unique()    # review the positions in the dataset
df = df.loc[(df['Position'] != 'Goalkeeper') & (df['Appearances'] != 0)]

# calculate Open Play Goals
df['Open Play Goals'] = (df['Goals'] - df['Penalties scored'].fillna(0) \
                                     - df['Freekicks scored'].fillna(0)).astype(int)

# rename the original Goals field to Total Goals Scored
df.rename(columns={'Goals' : 'Total Goals'}, inplace=True)

# check for players with multiple positions
df.groupby(['Name', 'Position']).size().reset_index",1.0
2021.0,14.0,String Functions,String slicing,"sengers = passengers[[c for c in passengers.columns if 'Unnamed' not in c]]


# --------------------------------------------------------------------------------------------------
# prep / calculations
# --------------------------------------------------------------------------------------------------

# transpose the seat list
seats = seats_mtx.melt(id_vars='Row', value_vars=seats_mtx.columns[1:])
seats.rename(colum",1.0
2021.0,15.0,String Functions,String slicing,"ndex().melt(id_vars=['index'])

# parse the fieldnames and remove nulls
menu[['Type', 'field']] = menu['variable'].str.extract('(.*?)\s?(ID|Price|$).*', expand=True)
menu['field'] = where(menu['field'].str.strip() == '', 'Item', menu['field'])
menu.dropna(subset=['value'], inplace=True)

# pivot fieldnames into columns
menu_f = menu.pivot_table(values='value', index=['index', 'Type'], columns=['field'],
                          aggfunc='first')\
             .reset_index()\
             .drop(columns='index')


# put each item ID from orders into a separate row
orders['Order'] = orders['Order'].astype(str).str.split('-')
orders = orders.explode('Order')
orders['Order'] = orders['Order'].astype(int)

# join the tables
orders_f = orders.merge(menu_f, left_on='Order', right_on='ID', how='inner')

# apply 50% discount on Mondays
orders_f['Weekday'] = orders_f['Order Date'].dt.day_name()
orders_f['Price'] = where(orders_f['Weekday'] == 'Monday', orders_f['Price'] * 0.5,
                          orders_f['Price'])


#---------------------------------------------------------------------------------------------------
# output the files
#---------------------------------------------------------------------------------------------------

# output 1: calculate total spend by day of week
out1 = orders_f.groupby('Weekday')['Price'].sum().reset_index()",1.0
2021.0,17.0,String Functions,String slicing,"tal rows
df = df.loc[df['Name, Age, Area of Work'].notna()]

# pivot dates to rows and rename fields 'Date' and 'Hours'
df_m = df.melt(id_vars=['Name, Age, Area of Work', 'Project'], var_name='Date', value_name='Hours')

# split the Name, Age, Area of Work field into 3 Fields and Rename
df_m[['Name', 'Age', 'Area of Work']] = df_m.iloc[:, 0].str.extract('(.*), ",1.0
2021.0,18.0,String Functions,String slicing," 'Completed Date'
df['Completed Date'] = df['Scheduled Date'] \
                       + df['Days Difference to Schedule'].apply(lambda x: Timedelta(x, unit='D'))

# check for multiple rows
id_vars = ['Project', 'Sub-project', 'Owner']
df_check = df.group",1.0
2021.0,19.0,String Functions,String slicing,"ject details in the [ ]
- Output the file

Author: Kelly Gilbert
Created: 2021-05-28
Requirements:
  - input dataset:
      - PD 2021 Wk 19 Input.xlsx
  - output dataset (for results check):
      - PD 2021 Wk 19 Output.csv
""""""


from pandas import ExcelFile, merge, read_excel

# for solution check only
from pandas import read_csv


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\\inputs\\PD 2021 Week 19 Input.xlsx') as xl:
    df = read_excel(xl, 'Project Schedule Updates')
    proj = read_excel(xl, 'Project Lookup Table')
    subproj = read_excel(xl, 'Sub-Project Lookup Table')
    task = read_excel(xl, 'Task Lookup Table')
    owner = read_excel(xl, 'Owner Lookup Table')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# 'Week' with the word week and week number together 'Week x'
df.index = 'Week ' + df['Week'].astype(str)

# spli",1.0
2021.0,20.0,String Functions,String slicing,"        parse_dates=['Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create the mean and standard deviation for each Week
df_wk = df.groupby('Week').agg(Mean=('Complaints', 'mean'),
                               stdev=('Complaints', 'std')).reset_index()

# duplicate the weekly dataframe for each number of standard deviations
stdev_values = [1, 2, 3]

df_wk['join'] = 1
df_wk = df_wk.merge(DataFrame({'n' : stdev_values, 'join' : [1]*len(stdev_values)})",1.0
2021.0,22.0,String Functions,String slicing,"egory data
cat = cat['Category: Answer'].str.strip().str.ext",1.0
2021.0,23.0,String Functions,String slicing,"xl:
    df = concat([read_excel(xl, s) for s in xl.sheet_names])   
        

#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# response count by airline
df = df.groupby('Airline').filter(lambda x: len(x) >= 50) 

# classify customer responses to the question in the following way
#     0-6 = Detractors, 7-8 = Passive, 9-10 = Promoters
df['nps_type'] = cut(df['How likel",1.0
2021.0,24.0,String Functions,String slicing," absences by date
df['Date'] = [date_range(d, periods=p, freq='D') for d, p in zip(df['Start Date'], df['Days Off'])]
df_count = df.explode('Date').groupby('Date')['Name'].count().reset_index()
df_count.rename(columns={'Name' : 'Number of people off each day'}, inplace=True)


# generate a list of days in the range of interest
df_dates = DataFrame({'Date' : date_range(start=start_date, end=end_date)})

# join to dates of interest and fill in zeroes
df_dates = df_dates.merge(df_count, on='Date', how='left').fillna(0)


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_dates.to_csv(r'.\outputs\output-2021-24.csv', index=False, date_format='%d/%m/%Y %H:%M:%S')


#---------------------------------------------------------------------------------------------------
# questions
#---------------------------------------------------------------------------------------------------

# What date had the most people off?
max_date = df_dates.iloc[df_dates.iloc[:, 1].idxmax()]['Date']
p",1.0
2021.0,25.0,String Functions,String slicing,"   exclude = concat([read_excel(xl, s) for s in ['Mega Evolutions', 'Alolan', 'Galarian', 
                                                  'Gigantamax']])
    unattainable = read_excel(xl, 'Unattainable in Sword & Shield')
    appearances = read_excel(xl, 'Anime Appearances').drop_duplicates()


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# clean up the list of Gen 1 Pokmon so we have 1 row per Pokmon
# the type (which wraps to the next row) is not used
gen1 = gen1.loc[gen1['#'].notna()]

# clean the evolution group #
evol_group['#'] = evol_group['#'].astype(int)
evol_group.drop_duplicates(subset=['#', 'Evolution Group'], inplace=True)

# get the evolution group for gen1
gen1 = gen1[['#', 'Name']].merge(evol_group, on='#', how='inner')


# get the to/from evolutions for gen1
evol.drop(columns=['Evolution Type', 'Condition', 'Level'], inplace=True)
gen1 = gen1.merge(evol, left_on='Name', right_on='Evolving to', how='left')\
           .drop(columns=['Evolving to'])
gen1 = gen1.merge(evol, left_on='Name', right_on='Evolving from', suffixes=['', '_r'], how='left')\
           .drop(columns=['Evolving from_r'])


# get the list of Pokemon with a mega evolution, Alolan, Galarian or Gigantamax form
exclude['Name_clean'] = exclude['Name'].str.extract('(?:\w+) (.*?)(?: [XY]|$)')
exclude_list =",1.0
2021.0,26.0,String Functions,String slicing,"1.csv', parse_dates=['Date'], dayfirst=True)\
             .sort_values(by='Date')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create 7 rows per original date
df_dates = DataFrame( {'Date' : df['Date'].unique()} )
df_date",1.0
2021.0,27.0,String Functions,String slicing," melt(prob, id_vars=['Seed'], var_name='pick', value_name='prob').dropna()
prob['prob'] = prob['prob'].astype(str).str.replace('>', '').astype(float)


# select the first four picks by lottery
lottery = []
#pick = 0
for pick in range(lottery_count):
    
    # subset the probability table and randomly select a seed based on the weights
    prob_sub = prob.loc[(prob['pick']==pick+1) & ~prob['S",1.0
2021.0,28.0,String Functions,String slicing,"  df_temp.columns = [c.lower().strip() for c in df_temp.columns]
        df_temp['sheet'] = s
        df = concat([df, df_temp])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# melt winner/loser into rows
df_m = df.melt(id_vars=[c for c in df.columns if c not in (['winner', 'loser'])],
               value_vars=['winner', 'loser'], value_name='Team')


# clean team name
df_m['Team'] = df_m['Team'].str.replace('.*Germany.*', 'Germany').str.strip()


# update date with year (note, this field isn't used in the output, but this in the requirements)
df_m['date'] = to_datetime(df_m['event year'].str[0:4] + df_m['date'].dt.s",1.0
2021.0,29.0,String Functions,String slicing,"ateTime field
events['UK Date Time'] = to_datetime(events['Date'].str.replace('(?<=\d)[a-z]+', '') + ' ' 
                                         + events['Time'].str.replace('xx', '0:00'), 
                                     format='%d_%B_%Y %H:%M')
events['Date'] = events['UK Date T",1.0
2021.0,30.0,String Functions,String slicing,"der is maintained
df['trip_dtt'] = to_datetime('2021-07-12 ' + df['Hour'].astype(str) + ':' + df['Minute'].astype(str), 
     ",1.0
2021.0,31.0,String Functions,String slicing,"t.csv', parse_dates=['Date'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# remove the 'Return to Manufacturer' records
df = df.loc[df['Status'] != 'Return to Manufacturer']

# pivot the data by store and item
total_name = 'Items sold per store'
pivot = pivot_table(df, values='Number of Items', index='Store', columns='Item', aggfunc='sum', 
                    fill_value=None, margins=True, margins_name=total_name)\
                   .reset_index()


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

cols = list(pivot.columns)
cols.reverse()

pivot.iloc[0:len(pivot)-1].to_csv(r'.\outputs\",1.0
2021.0,33.0,String Functions,String slicing,", s)
        df_temp['sheet'] = s
        df = concat([df, df_temp])
        
        
#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create the Reporting Date
df['Reporting Date'] = to_datetime(df['sheet'].str[0:4] + '-' + df['sheet']",1.0
2021.0,34.0,String Functions,String slicing,"map = { 'Bristol' : ['Bristal', 'Bristole', 'Bristoll'],
              'Stratford' : ['Statford' , 'Stratfod', 'Straford', 'Stratfodd'],
              'Wim",1.0
2021.0,35.0,String Functions,String slicing,"o centimeters
    df[0] = df[0].astype(float) * where(df[1] == '""', 2.54, 1)
    df[2] = where(df[2].isna(), df[0], df[2]).astype(float) * where(df[1] == '""', 2.54, 1)
    
    # find largest size
    df['Max Side'] = where(df[0] >= df[2], df[0], df[2])
    df['Min Side'] = where(df[0] < df[2], df[0], df[2])
    df['Area'] = df['Max Side'] * df['Min Side']
    
    return df[['Max Side', 'Min Side', 'Area']]


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with ExcelFile(r'.\inputs\Pictures Input.xlsx') as xl:
    p = read_excel(xl, 'Pictures')
    f = read_excel(xl, 'Frames').drop_duplicates().rename(columns={'Size' : 'Frame'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split up the sizes of the pictures and the frames into lengths and widths
p[['Max Side', 'Min Side', 'Area']] = parse_sizes(p['S",1.0
2021.0,36.0,String Functions,String slicing,"earch term name
df_t['Search Term'] = df_t['Search Term'].str.replace(':.*', '')
df_c['Search Term'] = df_c['Search Term",1.0
2021.0,41.0,String Functions,String slicing,"Pts'})
df.columns = [c if c == 'POS' else c.title() for c in df.columns]


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create a Special Circumstances field with the following categories
df['Special Circumstances'] = where(df['Season'] == df['Season'].max(), 'Incomplete',
                                where(df['Season'] == '1939-40', 'Abandoned due to WW2', 'N/A'))

# ensure the POS field only has values for full seasons
df['POS'] = where(df['Special Circumstances'] == 'N/A', df['POS'], nan)

# extract the numeric values from the leagues
df['league_nbr'] = where(df['League'] == 'FL-CH', 0,
                     where(df['League'] == 'NAT-P', 5,
                           df['League'].str.extract('.*-(\d+)', expand=False).astype(float)))

# create an Outcome field with 3 potential values. (Note: this should apply to all seasons in the
# data order regardless of any gaps. The current season will have a null value)
df = df.sort_values(by='Season')
df['Outcome'] = where(df['league_",1.0
2021.0,42.0,String Functions,String slicing,"        parse_dates=['Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create new rows for any date missing between the first and last date in the data set provided
# calculate how many days of fundraising there has been by the date in each row (1st Jan would be 0)
df = DataFrame({'Date' : date_range(start=df_in['Date'].min(), end=df_in['D",1.0
2021.0,43.0,String Functions,String slicing,"es={'Date lodged' : ['Month ', 'Date', 'Year']})
    df_b = read_excel(xl, 'Business Unit B ', skiprows=5, parse_dates=['Date lodged'], dayfirst=True)\
           .rename(columns={'Unit' : 'Business Unit '})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# use the lookup table to update the risk rating for business unit A
risk_dict = dict(zip(df_risk['Risk level'], df_risk['Risk rati",1.0
2021.0,44.0,String Functions,String slicing,"ssuming 30 km/hr)
df['km'] = where(df['Measure'].str.lower() == 'min', df['Value'] * 30 / 60, df['Value'])    
    
# classify as 'Outdoors' if measure is km, and 'Turbo Trainer' otherwise
df['type'] = where(df['Measure'].str.lower() == 'km', 'Outdoors', 'Turbo Trainer')  
  
# create a separate column for Outdoors and Turbo Trainer (indoor static bike values)
df_p = df.pivot_table(values='km', index='Date', columns='type', aggfunc='sum')

# count the number of activities per day
df_p['Activities per day'] = df.groupby('Date')['km'].count().astype('Int64')

# ensure there is a row for each date between 1st Jan 2021 and 1st Nov 2021(inclusive)
rng = date_range(start='2021-01-01', end='2021-11-01')
df_p = df_p.reindex(rng).fillna(0).rename_axis('Date').reset_index()


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

# work out how many days I did no activities
print(f'Days with no activities: {df_p[df_p[""Activities per day""] == 0][""Date""].count",1.0
2021.0,45.0,String Functions,String slicing,"xl:
    df = concat([read_excel(xl, s).assign(date=s) for s in xl.sheet_names if s != 'Attendees'])\
        .rename(columns={'Attendee IDs' : 'Attendee ID'})
    df_att = read_excel(xl, 'Attendees', )


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# create a DateTime field for each Session
df['DateTime'] = to_datetime(df['d",1.0
2021.0,46.0,String Functions,String slicing,"d = {}
    for s in [s for s in xl.sheet_names if 'Sales' not in s]:
        d[s] = read_excel(xl, s)",1.0
2021.0,47.0,String Functions,String slicing,"e_usd with zero
df_e['prize_usd'] = df_e['prize_usd'].fillna(0)


# summarize event metrics for each player and add the player name
df_e['first_place'] = where(df_e['player_place']=='1st', 1, 0)
df_p_tot = df_e.groupby('player_id').agg(wins=('first_place', 'sum'),
                                         number_of_events=('event_date', 'count'),
                                         first_event=('event_date', 'min'),
                                         last_event=('event_date', 'max'),
                                         biggest_win=('prize_usd', 'max'),
                                         countries_visited=('event_country', 'nunique'))\
               .reset_index()\
               .merge(df_p[['player_id', 'name', 'all_time_money_usd']], on='player_id', how='left')\
               .rename(columns={'all_time_money_usd' : 'total_prize_money'})


# calculate win % and career length
df_p_tot['percent_won'] = df_p_tot['wins'] ",1.0
2021.0,48.0,String Functions,String slicing,"t = df.melt(id_vars=['Unnamed: 1'],
                  ",1.0
2021.0,49.0,String Functions,String slicing,"t.csv', parse_dates=['Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# add the reporting year
df['Reporting Year'] = df['Date'].dt.year


# summarize by person and year
df_out = df.groupby(['Name', 'Reporting Year']).agg(min_date=('Date', 'min'),
                                                    max_date=('Date', 'max'),
                                                    months_worked=('Date', 'count'),
                                                    annual_salary=('Annual Salary', 'mean'),
                                                    total_sales=('Sales', 'sum')).reset_index()


# calculate salary and bonus
df_out['Salary Paid'] = (df_out['annual_salary'] / 12 * df_out['months_worked']).round(2)
df_out['Yearly Bonus'] = (df_out['total_sales'] * 0.05).round(2)
df_out['Total Paid'] = (df_out['Salary Paid'] + df_out['Yearly Bonus']).round(0)


# calculate cumulative tenure
df_out['Tenure by End of Reporting Year'] = df_out.groupby('Name')['months_worked'].cumsum()

    
# create the Employment Range field which captures the employees full tenure at the company                         
df_out['Employment Range'] = df_out.groupby(['Name'])['min_date'].transform('min').dt.strftime('%b %Y') \
                             + ' to ' \
                             + df_out.groupby(['Name'])['max_date'].transform('max').dt.strftime('%b %Y')


#---------------------------------------------------------------------------------------------------
# output the file
#---------------------------------------------------------------------------------------------------

df_out.rename(columns={'annual_salary' : 'Annual Salary'})

cols = ['Name', 'Employment Range', 'Reporting Year', 'Tenure by End of Reporting Year', 
        'Salary Paid', 'Yearly Bonus', 'Total Paid']
df_out.to_csv(r'.\o",1.0
2021.0,50.0,String Functions,String slicing,"xl:
    df = concat([read_excel(xl, s).assign(sheet=s) for s in xl.sheet_names])\
         .rename(columns={'Unnamed: 7' : 'YTD Total'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# get the month (for total rows, use the date from the prior row) -- use for YTD calc
df['Month'] = where(df['Salespe",1.0
2021.0,51.0,String Functions,String slicing,"t.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in']",1.0
2022.0,1.0,String Functions,String slicing,"t.csv', parse_dates=['Date of Birth'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# form the pupil's name correctly for the records in the format Last Name, First Name
df[""Pupil's Name""] = df['pupil last name'] + ', ' + df['pupil first name']

# form the parental contact's name in the same format as the pupil's 
df['Parental Contact Use'] = where(df['Parental Contact']==1, df['Parental Contact Name_1'],
                                   df['Parental Contact Name_2'])
df['Parental Contact Full Name'] = df['pupil last name'] + ', ' + df['Parental Contact Use']


# create the email address to contact the parent: Parent First Name.Parent Last Name@Employer.com
df['Parental Contact Email Address'] = df['Parental Cont",1.0
2022.0,2.0,String Functions,String slicing,"t.csv', parse_dates=['Date of Birth'],
                 usecols=['id', 'pupil first name', 'pupil last name', 'Date of Birth'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# format the pupil's name in First Name Last Name format (ie Carl Allchin)
df['Pupil Name'] = df['pupil first name'] + ' ' + df['pupil last name']


# create the date for the pupil's birthday in calendar year 2022 (not academic year)
df['This Year\'s Birthday'] = df['Date of Birth'].apply(lambda x: x.replace(year=datetime.now().year))


# birthday weekday and month
df['Cake Needed On'] = df['This Year\'s ",1.0
2022.0,3.0,String Functions,String slicing,"Input.csv', usecols=['id', 'gender'])\
                .rename(columns={'id' : 'Student ID', 'gender' : 'Gender'})
       
df_grades = pd.read_csv(r'.\inputs\PD 2022 WK 3 Grades.csv')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# join the data sets together to give us the grades per student
df = df_students.merge(df_grades.melt(id_vars='Student ID', var_name='Subject', value_name='Score'),
                       on='Student ID', how='left')\
                .assign(Pass=lambda df_x: df_x['Score'] >= PASSING_SCORE)

",1.0
2022.0,4.0,String Functions,String slicing,"stainable_methods = ['Bicycle', ""Dad's Shoulders"", 'Hopped', 'Jumped', ""Mum's Shoulders"", 
                       'Scooter', 'Skipped', 'Walk']


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = pd.read_csv(r'.\inputs\PD 2021 WK 1 to 4 ideas - Preferences of Travel.csv')
df_students = pd.read_csv(r'.\inputs\PD 2022 Wk 1 Input - Input.csv', usecols=['id'])\
                .rename(columns={'id' : 'Student ID'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# join the data sets together based on their common field, then melt weekdays into rows
# [KLG: the join doesn't seem necessary for the result. I think it's just for practice!]
df = df.merge(df_st",1.0
2022.0,5.0,String Functions,String slicing,"letter and points
df['Grade'] = df.groupby('Subject')['Score']\
                .transform(lambda x: pd.qcut(x, q=6, labels=['F', 'E', 'D', 'C', 'B', 'A']))

df['Points'] = d",1.0
2022.0,6.0,String Functions,String slicing,"f_scores = df_scores['Scrabble'].str.extract('(?P<Points>\d+) points?:\s+(?P<Tile>.*)')\
            .assign(Tile=lambda df_x: df_x['Tile'].str.split(','))\
  ",1.0
2022.0,7.0,String Functions,String slicing,"_metric = pd.concat([pd.read_excel(xl, s)\
                               .assign(Month_Start_Date=pd.to_datetime(s + '1, 2021'))\
                               .rename(columns=lambda x: 'Calls ' + x 
                                                         if x in ['Offered', 'Not Answered', 'Answered']
                   ",1.0
2022.0,8.0,String Functions,String slicing,"------

drop_cols = ['weight', 'height', 'evolves_from']

with pd.ExcelFile(r'.\inputs\input_pkmn_stats_and_evolutions.xlsx') as xl:
  
    # read in the stats tab, pivot combat factors to rows, then sum the combat factors
    df_stat = pd.read_excel(xl, 'pkmn_stats', usecols=lambda c: c not in drop_cols)\
                .melt(id_vars=['name', 'pokedex_number', 'gen_introduced'])\
                .",1.0
2022.0,9.0,String Functions,String slicing,"     .assign(Year=df['Order Date'].dt.year)


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# generate a list of all possible customer/year combos
# join to the actual cust/year combos
# set Order flag
# remove years before the first purchase
df_cust_yr = df.groupby(['Customer ID', 'Customer Name'], as_index=False).agg(First_Purchase=('Year', 'min'))\
               .merge(pd.DataFrame({'Year':df['Year'].unique()}), how='cr",1.0
2022.0,10.0,String Functions,String slicing,"?)\.png.*? title=\""\[(.*?)\].*? href.*?>(.*?)<.*'
df[['Pass/Fail', 'Categorisation', 'Movie']] = \
    df['DownloadData'].str.replace('\n', '').str.extract(pattern, expand=True)


# replace html codes 
df_html_m = df_html.melt(id_vars='Char', value_vars=['Numeric', 'Named']).dropna()
df_html_m = pd.concat([df_html_m.loc[df_html_m['variable']=='Numeric'],
                       df_html_m.loc[df_html_m['variable']=='Named'].apply(lambda x: x.str.lower()),
                       df_html_m.loc[df_html_m['variable']=='Named'].apply(lam",1.0
2022.0,11.0,String Functions,String slicing,"me=lambda df_x: df_x['Time'] + ':00')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# fill in the blanks (each lesson is taught at the same time, on the same day
df = df.sort_values(by=['Weekday', 'Time', 'Subject', 'Lesson Name'], ascending=False)\
",1.0
2022.0,12.0,String Functions,String slicing,"--------

usecols = ['EmployerName', 'EmployerId', 'EmployerSize', 'DiffMedianHourlyPercent', 'DateSubmitted']

df = pd.concat([pd.read_csv(path.join(IN_DIR, f), encoding='utf-8', usecols=usecols)\
                  .assign(Report=f[-16:-4],
                  ",1.0
2022.0,13.0,String Functions,String slicing,"= df_orders.groupby(['Customer ID', 'First Name', 'Surname'], as_index=False)['Sales'].sum()\
              .sort_values(by='Sales', ascending=False)\
              .reset_index(drop=True)\
              .assign(Pct_of_Total=lambda df_x: round(df_x['Sales'] / df_x['Sales'].sum",1.0
2022.0,14.0,String Functions,String slicing,"nd N Series
usecols=['Player', 'Ser.', 'Wk.', 'Week', 'Total', 'Week.1', 'F', 'F.1']
renames = {'Ser.' : 'Series', 'Wk.' : 'Week', 'Total' : 'Score', 'Week' : 'Points', 
           'Week.1' : 'Original_Rank', 'F.1' : 'F_rank'}

df = pd.read_csv(r"".\inputs\Richard Osman's House of Games - Episode Guide - Players.csv"",
                 usecols=usecols)\
       .rename(columns=renames)\
       .query(""(Series.str[0] != 'N') and not (Se",1.0
2022.0,15.0,String Functions,String slicing,"--------------

df_c['Current Date'] = CURRENT_DATE

# work out the length of each contract in months 
df_c['Contract Length'] = df_c['Contract End'].dt.to_period('M').view('int64') \
                          - df_c['Contract Start'].dt.to_period('M').view('int64')


# work out the number of months until each contract expires (imagine today is 13th April 2022)
df_c['Months Until Expiry'] = df_c['Contract End'].dt.to_period('M').view('int64') \
                              - df_c['Current Date'].dt.to_period('M').view('int64') 


# join to the pricing table and create one row per month begin date                              
df = df_c.merge(df_p, on=['City', 'Office Size'], how='left')\
         .assign(Month_Divider=\
                 lambda df_x: [pd.date_range(s, e, freq=pd.DateOffset(months=1)).union([e]) 
                 ",1.0
2022.0,16.0,String Functions,String slicing,"df_orders.columns = [f'{df_orders.columns[i-1]}_Selection' if 'Unnamed' in c else f'{c}_Dish'
                     for i, c in enumerate(df_orders.columns)]


# melt original columns into rows, split the column names into guest name and dish or selection,
# pivot dish/selection into columns
# fill the course name down
# join to lookup to get the recipe id
df_out = \
    df_orders.reset_index()\
             .melt(id_vars='index')\
             .assign(Guest=lambda df_x: df_x['variable'].str.extract('(.*?)\",1.0
2022.0,17.0,String Functions,String slicing,"YPES = {'Primary' : ['Cardiff', 'Edinburgh', 'London']}


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

with pd.ExcelFile(r'.\inputs\2022W17 Input.xlsx') as xl:
    
    # input the streaming data, fix location spelling, sum the duration by session
    df_s = pd.read_excel(xl, sheet_name='Streaming', parse_dates=['t'])\
             .ass",1.0
2022.0,19.0,String Functions,String slicing,"oducts table
df_prod['Product Code'] = df_prod['Product Code'].str.replace('S', '')


# get the actual size for each size ID
df_sales['Sold Size'] = df_sales['Size'].replace(dict(zip(df_sizes['Size ID'], df_sizes['Size'])))
                 

# join to the product table to get the scent and correct size
df_out = df_sales.merge(df_prod, left_on='Product', right_on='Product Code', how='left')


# remove decimals for output
df_out[['Sold Size', 'Product Size']] = ( df_out[['Sold Size', 'Product Size']]
                                             .astype(str)
                                             .replace('\.0', '', regex=True) )


#---------------------------------------------------------------------------------------------------
# output the files
#---------------------------------------------------------------------------------------------------

# output 1: correct sizes
( df_out[df_out['Sold Size'] == df_out['Product ",1.0
2022.0,20.0,String Functions,String slicing,"person field
#df_reg['Online/In Person'].unique()
df_reg['Online/In Person'] = where(df_reg['Online/In Person'].str.lower().str[0]=='o',
                                   'Online', 'In Person')
    

# look up the session name
df_reg['Session'] = df_reg['Session ID'].replace(dict(zip(df_ses['Session ID'], df_ses['Session'])))


# total registered sessions by person
df_reg['Registered_Count'] = df_reg.groupby('Email')['Session'].transform('count')


# join the attendance data to the registration data
df_reg_onl = df_reg[df_reg['Online/In Person']=='Online']
df_reg_inp = df_reg[df_reg['Online/In Person']=='In Person']
df_online['Attended'] = 1
df_inp['Attended'] = 1

df_combined = pd.concat( 
    [df_reg_onl.merge(df_online, on=['Email', 'Session'], how='left'),
     df_reg_inp.merge(df_inp, on=['First Name', 'Last Name', 'Session'], how='left')]
)


# calculate % not attended and output the sessions not attended
df_out = ( df_combined.loc[df_combined['Attended'].isna()]
               .rename(columns={'Session' : 'Session not attended'}) )
df_out['Not Attended %'] = (df_out.groupby('",1.0
2022.0,21.0,String Functions,String slicing,"eturn
METRIC_LIST = ['% Shipped in 3 days', '% Shipped in 5 days', 
               '% Processed in 3 days', '% Processed in 5 days', 
               '# Received']


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# read in all of the sheets, copy the depts and targets down, remove unnecessary cols/rows
with pd.ExcelFile(r'.\inputs\2022W21 Input.xlsx') as xl:
    
    df = ( pd.concat([pd.read_excel(xl, sheet_name=s, skiprows=3, 
                                    usecols=lambda c: 'FY' not in str(c) and c != 'Comment')
                        .assign(Shop=s) 
                      for s in xl.sheet_names]) 
             .ass",1.0
2022.0,22.0,String Functions,String slicing,"ogue.sort_values(by=['time_in_secs'])
                             .rename(columns={'time_in_secs' : 'start_time'}), 
                         df_dialogue[['Episode', 'time_in_secs']].sort_values(by=['t",1.0
2022.0,24.0,String Functions,String slicing,"ights', parse_dates=['First flight'], 
                               dtype={'Scheduled duration' : str})
    df_cities = pd.read_excel(xl, sheet_name='World Cities')


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# remove the airport names from the From and To fields
df_flights[['From', 'To']] = df_flights[['Fro",1.0
2022.0,27.0,String Functions,String slicing,"        parse_dates=['Sale Date'], dayfirst=True)


#---------------------------------------------------------------------------------------------------
# process the data (no split)
#---------------------------------------------------------------------------------------------------

# separate out the Product Name field to form Product Type and Quantity
df[['Product Type', 'Original Quantity', 'Unit']] = df['Product Name'].str.extract('(.+?) - (\d+)(.*)')


# for liquid, ensure every value is in milliliters
df['Quantity'] =  df['Original Quantity'].astype(int) * where(df['Unit'] == 'L', 1000, 1)


# sum sales and count orders by store, region, and quantity
df_out = ( df.groupby(['Product Type', 'Store Name', 'Region', 'Quantity'], as_index=False)
             .agg(Sale_Value=('Sale Value', 'sum'),
                  Present_in_N_orders=('Order ID', 'nunique'))
             .rename(columns=lambda x: x.replace('_', ' '))             
         )


#---------------------------------------------------------------------------------------------------
# output the file (no split)
#---------------------------------------------------------------------------------------------------

for t in df_out['Product Type'].unique(): 
    ( df",1.0
2022.0,28.0,String Functions,String slicing,"        parse_dates=['Sale Date'], dayfirst=True, usecols=['Sale Date'])


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# get a list of all possible dates
df_all = pd.DataFrame({ 'date' : pd.date_range(start=df_sales['Sale Date'].min(), 
           ",1.0
2022.0,29.0,String Functions,String slicing," = lambda df_x: df_x['Product Name'].str.extract('(.*) - .*'))
         .groupby(['PRODUCT', 'Store Name', 'Region'], as_index=False)['Sale Value'].sum()
     )


# read in the target data, melt store names into rows, multiply targets by 1000
df_target = ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - Targets (k's).csv"")
                .melt(id_vars='PRODUCT', var_name='Store Name', value_name=""Sales Target (k's)"")
                .assign(Target = lambda df_x: df_x[""Sales Target (k's)""] * 1000)
           ",1.0
2022.0,30.0,String Functions,String slicing,"ge
top3_filepaths = [r'.\inputs\Preppin_ Summer 2022 - Top 3 Sales People per Store (East).csv',
                  r'.\inputs\Preppin_ Summer 2022 - Top 3 Sales People per Store (West).csv']

df_top3 = pd.concat([pd.read_csv(f)
                       .assign(Region=f[f.find('(') + 1 : f.find(')')])
                  ",1.0
2022.0,31.0,String Functions,String slicing,"   df_store = df.loc[(df['Store Name']==store_name) & (df['Product Name'].str.contains('Liquid')),
                      df.columns]
    
    # split the Product Name field into Product Type and Size
    df_store[['Product Type', 'Size']] = df_store['Product Name'].str.extract('(.*) - (.*)')
    
    # rank based on sales and keep the top 10
    df_store['Rank of Product & Scent by Store'] = \
        df_store['Sale Value'].rank(method='first', ascending=False)
    df_out = df_store.loc[df_store['Rank of Product & Scent by Store'] <= 10, df_store.columns]
    
    # round the Sales Values to the nearest 10 value (ie 1913 becomes 1910)
    df_out['Sale Value'] = df_out['Sale Value'].round(-1)

    # output the file    
    df_out.to_csv(f'.\\outputs\\output-2022-31-{store_name}.csv', index=False,
                  columns=['Store Name', 'Rank of Product & Scent by Store', 'Scent Name', 
                           'Size', 'Sale Value'])
    
    print(f'\n*** SUCCESS: the file for {store_name} has been created.\n')


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

# input the file and sum sales by product, scent, and store
df = ( pd.read_csv(r"".\inputs\Preppin' Summer 2022 - PD 2022 Wk 27 Input.csv"")
         .groupby(['Product Name', 'Scent Name', 'Store Name'], as_index=False)['S",1.0
2022.0,32.0,String Functions,String slicing,"months to pay off
df['Monthly Capital'] = df['Monthly Payment'] * df['% of Monthly Repayment going to Capital'] / 100

df['Months Remaining'] = ceil(df['Capital Repayment Remaining'] / df['Monthly Capital'])

df['Last Month'] = [CURRENT_DATE + pd.DateOffset(months=m-1) for m in df['Months Remaining']]


# create one row per month
df_out = ( df.assign(Monthly_Payment_Date = [pd.date_range(start=CURRENT_DATE, end=e, 
                                                           freq=pd.DateOffset(months=1)) 
                                             for e in df['Last Month']])
             .explode('Monthly_Payment_Date')
             .sort_values(by=['Store', 'Monthly_Payment_Date'])
             .rename(columns=lambda c: c.replace('_', ' '))
         )


# remaining capital by store and for all stores
df_out['Remaining Capital to Repay'] = ( df_out['Capital",1.0
2022.0,33.0,String Functions,String slicing,"sales = ( pd.concat([pd.read_csv(r'.\inputs\PD 2022 Week 33 Input Instore Orders.csv',
                                    parse_dates=['Sales Date'], dayfirst=True)
                          .rename(columns={'Sales Date' : 'Sales Timestamp'}),
                        pd.read_csv(r'.\inputs\PD 2022 Week 33 Input Online Orders.csv', 
                                    parse_dates=['Sales Timestamp'], dayfirst=True)\
  ",1.0
2022.0,34.0,String Functions,String slicing,"ons_str = '\n'.join([f'  {i+1} - {c}' for i,c in enumerate(value_list)])
    
    while True:
        user_input = input(f'\n{value_name.title()} list:\n{options_str}\n\n'
                           + f'Select an option (1 - {len(value_list)}) or press Enter to cancel:')
        
        if user_input.isnumeric() and int(user_input) in range(1, len(value_list)+1):
            return value_list[int(user_input)-1]
        elif user_i",1.0
2022.0,35.0,String Functions,String slicing,"        parse_dates=['Date'], dayfirst=True)
              .rename(columns={'Value' : 'Mins'})
              .rename(columns=lambda c: 'Unnamed' if 'Unnamed' in c else c)
              .drop(columns=['Units', 'Type'])
          )

    d",1.0
,,String Functions,String slicing,"_weekly = pd.concat([pd.read_excel(xl, s)\
                             .assign(Week=int(s.replace('Week ', '')))\
                             .rename(columns=renames)
                           for s in xl.sheet_names if 'Week' in s])\
                  .assign(Type=lambda df_x: df_x['Type'].str.lower())\
     ",1.0
,,String Functions,String slicing,"t.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in']",1.0
,,String Functions,String slicing,"t.csv', parse_dates=['Order Date'], dayfirst=True)\
         .rename(columns={'OrderID' : 'OrderID_in', 'Unit Price' : 'Unit Price_in'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# split out the store name from the OrderID
df[['Store', 'OrderID']] = df['OrderID_in']",1.0
,,String Functions,String slicing,"Frame({'Customer' : [''.join(rnd.choices('ABCDEFGHIJKLMNOPQRST', k=2))*10 
                                        for n in range(0, recs)],
                          'Order Date' : rnd.choices(date_range('2000-01-01', '2021-12-31'), k=recs)})

    # low cardinality customer (52 possible choices)
    elif card == 'low':
        return DataFrame({'Customer' : [rnd.choice(string.ascii_letters)*20 for n in range(0, recs)],
                  ",1.0
,,String Functions,String slicing,"stainable_methods = ['Bicycle', ""Dad's Shoulders"", 'Hopped', 'Jumped', ""Mum's Shoulders"", 
                       'Scooter', 'Skipped', 'Walk']


#---------------------------------------------------------------------------------------------------
# input the data
#---------------------------------------------------------------------------------------------------

df = pd.read_csv(r'.\inputs\PD 2021 WK 1 to 4 ideas - Preferences of Travel.csv')
df_students = pd.read_csv(r'.\inputs\PD 2022 Wk 1 Input - Input.csv', usecols=['id'])\
                .rename(columns={'id' : 'Student ID'})


#---------------------------------------------------------------------------------------------------
# process the data
#---------------------------------------------------------------------------------------------------

# join the data sets together based on their common field, then melt weekdays into rows
# [KLG: the join doesn't seem necessary for the result. I think it's just for practice!]
df = df.merge(df_st",1.0
2019.0,4.0,String Functions,"```contains```, ```startswith```, ```endswith```"," in df.columns if c.startswith('HI ')]:
    df[[f'{",
2019.0,25.0,String Functions,"```contains```, ```startswith```, ```endswith```","ined['Concert'].str.contains('/')]
8#df_artist.he",
2020.0,4.0,String Functions,"```contains```, ```startswith```, ```endswith```","M'].str.lower().str.contains('pm'), 12, 0)
df_p['",
2020.0,9.0,String Functions,"```contains```, ```startswith```, ```endswith```","  .query(""~Poll.str.contains('Average')"", engine=",
2021.0,12.0,String Functions,"```contains```, ```startswith```, ```endswith```",eries-Measure'].str.contains('Tourist arrivals')),
2021.0,22.0,String Functions,"```contains```, ```startswith```, ```endswith```","Name'] if s.lower().startswith(n.lower())] 
       ",
2021.0,28.0,String Functions,"```contains```, ```startswith```, ```endswith```","   .str.lower().str.contains('penalty scored')
df",
2021.0,29.0,String Functions,"```contains```, ```startswith```, ```endswith```",'Events Split'].str.contains('Gold Medal|Victory ,
2021.0,48.0,String Functions,"```contains```, ```startswith```, ```endswith```","t['True Value'].str.contains('Year')==True,
     ",
2022.0,21.0,String Functions,"```contains```, ```startswith```, ```endswith```","(df_x['Target'].str.contains('%'), 1/100, 1) )
  ",
2022.0,23.0,String Functions,"```contains```, ```startswith```, ```endswith```","df['StageName'].str.contains('Closed')][['Id', 'C",
2022.0,31.0,String Functions,"```contains```, ```startswith```, ```endswith```","'Product Name'].str.contains('Liquid')),
        ",
2021.0,52.0,String Functions,```findall```,"omp['Complaint'].str.findall(keywords))\
        ",
2022.0,6.0,String Functions,```findall```,"rd'].str.lower().str.findall('(.)'))\
           ",
2020.0,6.0,String Functions,```join```,s_sum.columns = ['_'.join(t) if t[1] else t[0],
2021.0,7.0,String Functions,```join```,"['Contains'] = [', '.join([k.lower() for k in ",
2021.0,34.0,String Functions,```join```,"he lookup:\n' + '\n'.join(missing_stores))


#",
2021.0,47.0,String Functions,```join```,"   ax.set_title(""\n"".join(wrap(player_name, 15",
2021.0,52.0,String Functions,```join```,"yword
keywords = '|'.join(df_dept['Keyword'].s",
2022.0,12.0,String Functions,```join```,"at([pd.read_csv(path.join(IN_DIR, f), encoding",
2022.0,16.0,String Functions,```join```,Dish'].str.match('|'.join(list(COURSES.keys()),
2022.0,31.0,String Functions,```join```,re_list_str = '\n  '.join([f'{i+1} - {n}' for ,
2022.0,34.0,String Functions,```join```,  options_str = '\n'.join([f'  {i+1} - {c}' fo,
,,String Functions,```join```,me({'Customer' : [''.join(rnd.choices('ABCDEFG,
2020.0,2.0,String Functions,"```ljust```, ```rjust```","se)
df['am_pm'] = [t.ljust(2, 'm') if t != '' e",
2019.0,4.0,String Functions,```match```,"e(df['OPPONENT'].str.match('^vs'), 'Home', 'Awa",
2020.0,17.0,String Functions,```match```,t['flag'] = where(re.match('^|\W' + dfDeviceCou,
2021.0,12.0,String Functions,```match```,rchy-Breakdown'].str.match('.*Tourist arrivals ,
2022.0,16.0,String Functions,```match```,ere(df_x['Dish'].str.match('|'.join(list(COURSE,
2020.0,2.0,String Functions,```replace```,"t'] = df['Time'].str.replace('\D','', regex=True)

# separate daypa",1.0
2020.0,4.0,String Functions,```replace```,"                .str.replace('.', ':', regex=False)\
                ",1.0
2020.0,7.0,String Functions,```replace```,"f_curr['Salary'].str.replace(r""[\D]"",'')  
df_curr['Salary",1.0
2020.0,8.0,String Functions,```replace```,"  .assign(Week=int(s.replace('Week ', '')))\
              ",1.0
2020.0,9.0,String Functions,```replace```,"ype'] = df['Sample'].replace(sample_map, regex=True)


#--------------",1.0
2020.0,11.0,String Functions,```replace```,"df_x['Box_Size'].str.replace('Boxes of ', '').astype(int),
    ",1.0
2020.0,12.0,String Functions,```replace```,"ot['Scent_orig'].str.replace(' ', '')


# Percentage of",1.0
2021.0,1.0,String Functions,```replace```," 'Road' }
df['Bike'].replace(remap, inplace=True)

# create two dif",1.0
2021.0,2.0,String Functions,```replace```,"'] = df['Model'].str.replace('[^A-Z]+', '')


# work out the ",1.0
2021.0,6.0,String Functions,```replace```,"dfAll['Measure'].str.replace('_', ' ')

# output
dfAll.t",1.0
2021.0,12.0,String Functions,```replace```,"rchy-Breakdown'].str.replace('.*Tourist arrivals / ', ''),
                ",1.0
2021.0,14.0,String Functions,```replace```,"_list.iloc[:, 0].str.replace('[\[\]]', '').str.split('|', ex",1.0
2021.0,19.0,String Functions,```replace```,"t Code'].str.lower().replace('ops', 'op')
df['Abbreviation'",1.0
2021.0,21.0,String Functions,```replace```,"df['sheet_name'].str.replace('Month ', '')
                 ",1.0
2021.0,26.0,String Functions,```replace```," df_all.columns = [c.replace('_', ' ') for c in df_all.c",1.0
2021.0,27.0,String Functions,```replace```,"ob'].astype(str).str.replace('>', '').astype(float)


#",1.0
2021.0,28.0,String Functions,```replace```,"] = df_m['Team'].str.replace('.*Germany.*', 'Germany').str.strip()


# u",1.0
2021.0,29.0,String Functions,```replace```,"+ events['Time'].str.replace('xx', '0:00'), 
               ",1.0
2021.0,34.0,String Functions,```replace```,"summary.columns = [c.replace('pct_', '%_').replace('_', ' ')",1.0
2021.0,36.0,String Functions,```replace```,"t['Search Term'].str.replace(':.*', '')
df_c['Search Term",1.0
2021.0,38.0,String Functions,```replace```,"
df_sum.columns = [c.replace('_', ' ') for c in df_sum.c",1.0
2021.0,46.0,String Functions,```replace```,"ames
df.columns = [c.replace('_', ' ') for c in df.colum",1.0
2021.0,48.0,String Functions,```replace```,"lt['True Value'].str.replace('Year ', ''), nan))\
         ",1.0
2021.0,51.0,String Functions,```replace```,"'Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df",1.0
2022.0,4.0,String Functions,```replace```,"(columns=lambda c: c.replace('_', ' ').replace('Pct', '%",1.0
2022.0,7.0,String Functions,```replace```,"(columns=lambda x: x.replace('_', ' '))\
      .to_csv(r",1.0
2022.0,10.0,String Functions,```replace```,"['DownloadData'].str.replace('\n', '').str.extract(patte",1.0
2022.0,12.0,String Functions,```replace```,"    .astype(str).str.replace('\.?0$', '', regex=True)
df['Pay Gap'] = p",1.0
2022.0,13.0,String Functions,```replace```,"(columns=lambda x: x.replace('_', ' ').replace('Pct', '%",1.0
2022.0,15.0,String Functions,```replace```,"(columns=lambda x: x.replace('_', ' '))
         

# cal",1.0
2022.0,19.0,String Functions,```replace```,"['Product Code'].str.replace('S', '')


# get the actua",1.0
2022.0,20.0,String Functions,```replace```,"df_reg['Session ID'].replace(dict(zip(df_ses['Session ID'], df_ses['Session'])))


# total regis",1.0
2022.0,21.0,String Functions,```replace```,": df_x['Target'].str.replace('[^0-9\.\-]', '', regex=True)
                 ",1.0
2022.0,22.0,String Functions,```replace```,"   df_out['Episode'].replace(dict(zip(df_eps['Episode'], 
                                                                 df_eps['runtime_in_secs']))), 
             ",1.0
2022.0,24.0,String Functions,```replace```,"ghts[['From', 'To']].replace('[-/].*', '', regex=True)


# create a Rout",1.0
2022.0,27.0,String Functions,```replace```,"(columns=lambda x: x.replace('_', ' '))             
   ",1.0
2022.0,32.0,String Functions,```replace```,"(columns=lambda c: c.replace('_', ' '))
         )


# r",1.0
2022.0,33.0,String Functions,```replace```,"(columns=lambda c: c.replace('_', ' '))
            )


",1.0
2022.0,35.0,String Functions,```replace```,"(columns=lambda c: c.replace('_', ' '))
             )  ",1.0
,,String Functions,```replace```,"  .assign(Week=int(s.replace('Week ', '')))\
              ",1.0
,,String Functions,```replace```,"'Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df",1.0
,,String Functions,```replace```,"'Unit Price_in'].str.replace('[^\d\.\-]', '', regex=True).astype(float) 
df",1.0
,,String Functions,```replace```,"(columns=lambda c: c.replace('_', ' ').replace('Pct', '%",1.0
2019.0,25.0,String Functions,```split```,"f_artist.Concert.str.split(' / ').tolist(), \
 ",
2021.0,1.0,String Functions,```split```,"['Store - Bike'].str.split(pat=' - ', expand=Tr",
2021.0,3.0,String Functions,```split```,"stomer-Product'].str.split(pat=' - ', expand=Tr",
2021.0,4.0,String Functions,```split```,"stomer-Product'].str.split(pat=' - ', expand=Tr",
2021.0,7.0,String Functions,```split```,"ywordsIn.stack().str.split(', ').explode().rese",
2021.0,9.0,String Functions,```split```,"on.xlsx')['IDs'].str.split(' ').explode()
areas",
2021.0,11.0,String Functions,```split```,s['Recipe (ml)'].str.split('; ').explode().str.,
2021.0,14.0,String Functions,```split```,"ce('[\[\]]', '').str.split('|', expand=True)
fl",
2021.0,15.0,String Functions,```split```,"er'].astype(str).str.split('-')
orders = orders",
2021.0,16.0,String Functions,```split```,"] = df['Result'].str.split(' - ', expand=True).",
2021.0,19.0,String Functions,```split```,df['Commentary'].str.split('\s+(?=\[)').explode,
2021.0,21.0,String Functions,```split```," = df['Product'].str.split(' -').str[0]

# make",
2021.0,29.0,String Functions,```split```,"events['Events'].str.split(',')]
events = event",
2021.0,45.0,String Functions,```split```,"f['Attendee ID'].str.split(', ')
df = df.explod",
2022.0,6.0,String Functions,```split```,"_x: df_x['Tile'].str.split(','))\
            .",
2022.0,22.0,String Functions,```split```,"replace(' ', '').str.split(','),
              ",
2021.0,9.0,String Functions,```strip```,"roducts['Price'].str.strip('').astype(float)

",
2021.0,13.0,String Functions,```strip```,"e'] = df['Name'].str.strip()

# remove all goal",
2021.0,15.0,String Functions,```strip```,"re(menu['field'].str.strip() == '', 'Item', men",
2021.0,19.0,String Functions,```strip```,"=\[)').explode().str.strip().reset_index()

# p",
2021.0,21.0,String Functions,```strip```,"f['Destination'].str.strip()

# Use the Day of ",
2021.0,22.0,String Functions,```strip```,tegory: Answer'].str.strip().str.extract('(?P<C,
2021.0,28.0,String Functions,```strip```,columns = [c.lower().strip() for c in df_temp.c,
2021.0,29.0,String Functions,```strip```,Events Split'] = [[e.strip() for e in es] for e,
2021.0,43.0,String Functions,```strip```,"_b])
df.columns = [c.strip() for c in df.column",
2021.0,46.0,String Functions,```strip```,"'Staff Comment'].str.strip()),
                ",
2021.0,52.0,String Functions,```strip```,"omp['Complaint'].str.strip().str.lower()


# fi",
2022.0,22.0,String Functions,```strip```,"ue'].astype(str).str.strip())
                 ",
2022.0,34.0,String Functions,```strip```,"df['Music Type'].str.strip().str.title()
    df",
2022.0,35.0,String Functions,```strip```,"df['Music Type'].str.strip().str.title()
    df",
2020.0,11.0,numpy,"```ceil```, ```floor```",f'Boxes of {s}'] = (np.ceil(df['Remainder'] / s),
2021.0,9.0,numpy,"```ceil```, ```floor```","taFrame, read_excel
from numpy import floor, vectorize

# used ",
2021.0,23.0,numpy,"```ceil```, ```floor```","NPS Input.xlsx
""""""

from numpy import floor
from pandas import ",
2021.0,47.0,numpy,"```ceil```, ```floor```","patches as mpatches
from numpy import ceil, pi, where
from pan",
2022.0,32.0,numpy,"```ceil```, ```floor```","ime import datetime
from numpy import ceil
import pandas as pd",
2019.0,4.0,numpy,```where```,"from numpy import where
from pandas import ",
2020.0,3.0,numpy,```where```,"port datetime as dt
from numpy import where, nan
from os import",
2020.0,4.0,numpy,```where```,"port datetime as dt
from numpy import nan, where
from pandas import ",
2020.0,9.0,numpy,```where```," 9 Output.csv
""""""


from numpy import nan, where
import pandas as pd",
2020.0,11.0,numpy,```where```,s['Soaps in Box'] = np.where((df_soaps['Box Size',
2020.0,17.0,numpy,```where```,", read_excel, merge
from numpy import where
import re

chdir('C",
2021.0,12.0,numpy,```where```,"9 Output.csv

""""""


from numpy import nan, where
from pandas import ",
2021.0,14.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,15.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,16.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,20.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,28.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,32.0,numpy,```where```,"from numpy import nan, where
from pandas import ",
2021.0,33.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,34.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,35.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,36.0,numpy,```where```,"from numpy import nan, where
from pandas import ",
2021.0,39.0,numpy,```where```,"from numpy import nan, where
from pandas import ",
2021.0,40.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,41.0,numpy,```where```,"41 Output.csv
""""""


from numpy import nan, where
from pandas import ",
2021.0,42.0,numpy,```where```,from numpy import where    # only used for ,
2021.0,43.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,44.0,numpy,```where```,"from numpy import where
from pandas import ",
2021.0,47.0,numpy,```where```,"patches as mpatches
from numpy import ceil, pi, where
from pandas import ",
2021.0,48.0,numpy,```where```,"48 Output.csv
""""""


from numpy import nan, where
from pandas import ",
2021.0,50.0,numpy,```where```,"ent Output.csv
""""""

from numpy import where
from pandas import ",
2021.0,51.0,numpy,```where```,"ion Table.csv
""""""


from numpy import where
from pandas import ",
2022.0,1.0,numpy,```where```,"from numpy import where
from pandas import ",
2022.0,4.0,numpy,```where```,['Sustainable?']  = np.where(df['Method of Travel,
2022.0,6.0,numpy,```where```,"import decimal as d
from numpy import where
import pandas as pd",
2022.0,7.0,numpy,```where```,"from numpy import where, NaN
import pandas ",
2022.0,8.0,numpy,```where```,"a_gnv_feb.csv
""""""


from numpy import where
import pandas as pd",
2022.0,9.0,numpy,```where```,"ion Output.csv
""""""

from numpy import nan, where
import pandas as pd",
2022.0,12.0,numpy,```where```,"Gap Output.csv
""""""

from numpy import nan, where
from os import list",
2022.0,15.0,numpy,```where```, value=lambda df_x: np.where(df_x['Month Divider',
2022.0,16.0,numpy,```where```,"s check only)

""""""

from numpy import NaN, where
import pandas as pd",
2022.0,17.0,numpy,```where```,"from numpy import where
import pandas as pd",
2022.0,20.0,numpy,```where```,"import pandas as pd
from numpy import where
import output_check",
2022.0,21.0,numpy,```where```,"21 Output.csv
""""""


from numpy import where
import pandas as pd",
2022.0,22.0,numpy,```where```,"used for chart only
from numpy import where
import pandas as pd",
2022.0,23.0,numpy,```where```,"orce Data.csv
""""""


from numpy import where
import pandas as pd",
2022.0,27.0,numpy,```where```,"id Output.csv
""""""


from numpy import where
import pandas as pd",
,,numpy,```where```,"ion Table.csv
""""""


from numpy import where
from pandas import ",
,,numpy,```where```,"ion Table.csv
""""""


from numpy import where
from pandas import ",
,,numpy,```where```,able = lambda df_x: np.where(df_x['Method_of_Trav,
2020.0,8.0,numpy,"aggregate functions (e.g. ```sum```, ```max```)","ime import datetime
from numpy import sum
import pandas as pd",
,,numpy,"aggregate functions (e.g. ```sum```, ```max```)","ime import datetime
from numpy import sum
import pandas as pd",
